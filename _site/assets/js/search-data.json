{"0": {
    "doc": "August 27",
    "title": "August 27",
    "content": " ",
    "url": "/blog/class-notes/00-Geometric-Mechanics/Lectures/08-27.html",
    
    "relUrl": "/class-notes/00-Geometric-Mechanics/Lectures/08-27.html"
  },"1": {
    "doc": "August 27",
    "title": "1. Get into Lagrangian and minimal coordinates.",
    "content": " ",
    "url": "/blog/class-notes/00-Geometric-Mechanics/Lectures/08-27.html#1-get-into-lagrangian-and-minimal-coordinates",
    
    "relUrl": "/class-notes/00-Geometric-Mechanics/Lectures/08-27.html#1-get-into-lagrangian-and-minimal-coordinates"
  },"2": {
    "doc": "August 27",
    "title": "Newton’s laws",
    "content": ". | Every object in a state of uniform motion will remain in that state of motion unless acted upon by a net external force. | The acceleration of an object is directly proportional to the net force acting on the object and inversely proportional to the object’s mass. | For every action, there is an equal and opposite reaction. | . Look at Principia Chandrasekar . Lagrange Equations . | Generalized coordinates | minimal set of variables needed to describe configuration of system | . Double pendulum example: . | we can describe with \\((x^1, y^1, x^2, y^2)\\) | we can also use angles \\((\\theta^1, \\theta^2)\\) | angle representation is the minimal representation | . Question: systematic way to determine a minimal set of coordinates? . ",
    "url": "/blog/class-notes/00-Geometric-Mechanics/Lectures/08-27.html#newtons-laws",
    
    "relUrl": "/class-notes/00-Geometric-Mechanics/Lectures/08-27.html#newtons-laws"
  },"3": {
    "doc": "August 27",
    "title": "2. Lagrangian and Hamiltonian",
    "content": ". | related Principle of Least Action | Maupertuis (google: known for principle of least action) . | theory for motion of particles | analogous to Fermat’s principle of least time Best example to think about is light (snell’s law). | . | . formalized by Euler Later Hamilton’s principle 1834 - 1835 . Notes about snell’s law: . | slower in water \\(n_1 &gt; n_2\\) | where V is velocity \\(n_1 = \\frac{1}{V_c}\\) | equation: \\(n_1 \\sin \\theta_1 = n_2 \\sin \\theta_2\\) | . how do we know this minimizes time? how to verify? . | maximize time in easier medium | minimize time spent in tougher medium | this now minimizes total time spent from point A to point B | . \\[\\begin{align*} T = \\text{total time} \\\\ c_1 = \\frac{1}{n_1} \\\\ a = \\text{height of easy medium} \\\\ b = \\text{height of tough medium} \\\\ \\ell = \\text{total horizontal length} \\\\ x = \\text{horizontal length of easy medium} \\\\ T = \\frac{\\sqrt{a^2 + x^2}}{c_1} + \\frac{\\sqrt{b^2 + (\\ell - x)^2}}{c_2} \\end{align*}\\] We can minimize \\(T\\) by taking critical points (calculus). Can’t prove physics laws. One way to come up with them is to use foundational principles. ",
    "url": "/blog/class-notes/00-Geometric-Mechanics/Lectures/08-27.html#2-lagrangian-and-hamiltonian",
    
    "relUrl": "/class-notes/00-Geometric-Mechanics/Lectures/08-27.html#2-lagrangian-and-hamiltonian"
  },"4": {
    "doc": "August 27",
    "title": "2 principles for particles",
    "content": ". | least action | hamilton’s principle | . ",
    "url": "/blog/class-notes/00-Geometric-Mechanics/Lectures/08-27.html#2-principles-for-particles",
    
    "relUrl": "/class-notes/00-Geometric-Mechanics/Lectures/08-27.html#2-principles-for-particles"
  },"5": {
    "doc": "August 27",
    "title": "Lagrangian",
    "content": ". | derived by force-balance | Newton’s laws in generalized coordinates | Lagrangian smooth function \\(\\mathcal{L}(q^1, ..., q^i, t)\\) | . ",
    "url": "/blog/class-notes/00-Geometric-Mechanics/Lectures/08-27.html#lagrangian",
    
    "relUrl": "/class-notes/00-Geometric-Mechanics/Lectures/08-27.html#lagrangian"
  },"6": {
    "doc": "August 27",
    "title": "Hamiltonian",
    "content": ". | derived by variational principle | config space | variational princple: \\(\\mathcal{L} = T - V\\) | \\(\\delta \\int_0^T \\mathcal{L}(q^i, \\dot{q}^i, t) dt = 0\\) over all curves \\(q^i(t)\\) | . Let \\(\\delta q^i\\) be variation of the curve. \\[\\begin{align} \\int_a^b (\\frac{\\partial \\mathcal{L}}{\\partial q^i} \\delta q^i + \\frac{\\partial \\mathcal{L}}{\\partial \\dot{q}^i} \\delta \\dot{q}^i) dt \\quad \\forall \\delta q^i \\\\ \\delta \\dot{q}^i = \\frac{d}{dt} \\delta q^i \\\\ \\delta q^i = 0 \\quad \\text{at} \\quad t = a \\quad t=b \\\\ \\int_a^b (\\frac{\\partial \\mathcal{L}}{\\partial q^i} - \\frac{d}{dt}(\\frac{\\partial \\mathcal{L}}{\\partial \\dot{q}^i})) \\partial q^i dt \\quad \\forall \\delta q^i \\\\ \\end{align}\\] integral of 0 is 0, so just solve the term within the integral. this is nice because we don’t care about the value of \\(\\partial q^i\\) . \\[\\begin{equation} \\frac{d}{dt} \\frac{\\partial \\mathcal{L}}{\\partial \\dot{q}^i} - \\frac{\\partial \\mathcal{L}}{\\partial q^i} = 0 \\end{equation}\\] Now we can get . \\[\\begin{align*} \\mathcal{L} &amp;= \\frac{1}{2} \\sum_{i=1}^n m_i \\|\\dot{q}_i\\|^2 - V(q,i) \\\\ &amp;=&gt; \\frac{d}{dt} (m,q) - \\frac{\\partial V}{\\partial q^i} = 0 \\end{align*}\\] if we take \\(\\frac{d}{dt}\\) operator on lagrangian above we get that nice equation below… . \\[L = T - V\\] where T is kinetic and V is potential. So one way of thinking about lagrangian EoM is a way of minimizing \\(T-V\\) (difference between potential and kinetic energy). Say you’re a particle on a sphere shell. You want to go from point A to point B. There are two arcs if your lagrangian is only kinetic energy. The lagrangian can represent multiple paths while the lower-level dynamics might only suggest one. ",
    "url": "/blog/class-notes/00-Geometric-Mechanics/Lectures/08-27.html#hamiltonian",
    
    "relUrl": "/class-notes/00-Geometric-Mechanics/Lectures/08-27.html#hamiltonian"
  },"7": {
    "doc": "August 27",
    "title": "Hamilton’s Equations",
    "content": "\\[\\begin{align} \\frac{d}{dt} \\frac{\\partial \\mathcal{L}}{\\partial \\dot{q}^i} - \\frac{\\partial \\mathcal{L}}{\\partial q^i} &amp;= 0 \\\\ p_i &amp;= \\frac{\\partial \\mathcal{L}}{\\partial \\dot{q}^i} \\\\ H &amp;= \\sum_{i=1}^n \\dot{q}^i p_i - \\mathcal{L} \\\\ \\frac{\\partial H}{\\partial p_i} &amp;= \\dot{q}^i \\\\ \\frac{\\partial H}{\\partial q^i} &amp;= -\\frac{\\partial \\mathcal{L}}{\\partial q^i} - \\frac{d}{dt} (\\frac{\\partial \\mathcal{L}}{\\partial \\dot{q}^i}) \\\\ &amp;= \\frac{d}{dt} p_i \\end{align}\\] In total . \\[\\begin{align} \\frac{d}{dt} p_i &amp;= -\\frac{\\partial H}{\\partial q^i} \\\\ \\frac{d}{dt} q^i &amp;= \\frac{\\partial H}{\\partial p_i} \\end{align}\\] If \\(\\mathcal{L} = T - V\\) then \\(H = T + V\\). | generalize to manifold | symplectic, poisson manifolds | . Why do hamiltonian? . | Promise that it works better on geometry (preserves volume) | Lagrangian is better when there is dissipation | . Next time: . | Flows on a manifold (flow from dynamical systems) | Existence and Uniqueness of ODE | . \\[\\begin{align*} \\dot x = x \\\\ \\dot x = \\sqrt{x} \\\\ \\dot x = x^2 \\end{align*}\\] ",
    "url": "/blog/class-notes/00-Geometric-Mechanics/Lectures/08-27.html#hamiltons-equations",
    
    "relUrl": "/class-notes/00-Geometric-Mechanics/Lectures/08-27.html#hamiltons-equations"
  },"8": {
    "doc": "September 3",
    "title": "September 3",
    "content": " ",
    "url": "/blog/class-notes/00-Geometric-Mechanics/Lectures/09-03.html",
    
    "relUrl": "/class-notes/00-Geometric-Mechanics/Lectures/09-03.html"
  },"9": {
    "doc": "September 3",
    "title": "Recap",
    "content": ". | Problem set 1 is out. | Should start doing it. | Due in about 2 weeks from August 28… which is September 11. | Recall Feynman Lectures | . Recall that mean square of something is always greater than square of the mean. \\[v = \\bar v\\] Or… . \\[\\begin{align} \\int_0^T \\frac{1}{2} m v^2 dt &amp;\\geq \\int_0^T \\frac{1}{2} m \\bar v^2 \\space dt \\\\ \\bar v &amp;= \\frac{1}{T} \\int_0^T v \\space dt \\end{align}\\] I can prove this haha… . \\[\\begin{align*} \\text{LHS} &amp;= \\frac{1}{2} m \\int_0^T v^2 dt \\\\ \\text{RHS} &amp;= \\int_0^T \\frac{1}{2} m (\\frac{1}{T} \\int_0^T v \\space dt)^2 \\space dt \\\\ &amp;= \\int_0^T \\frac{1}{2} m \\frac{1}{T^2} (\\int_0^T v \\space dt)^2 \\space dt \\\\ &amp;= \\frac{1}{2} m \\int_0^T \\frac{1}{T^2} (\\int_0^T v \\space dt)^2 \\space dt \\\\ &amp;= \\frac{1}{2} m \\frac{1}{T} (\\int_0^T v \\space dt)^2\\\\ \\end{align*}\\] Least Action \\(\\begin{align} T = \\frac{1}{2} \\dot q^T G \\dot q \\\\ p = G \\dot q \\\\ \\dot q = G^{-1} p \\end{align}\\) . \\[\\begin{align*} \\delta \\int_q L(q,\\dot q) \\space dt = 0 \\\\ q(b) = q_b \\\\ q(a) = q_a \\\\ q(t,0) = q(t), \\space t \\in [a,b] \\\\ q(t,a) = q_a \\\\ q(t,b) = q_b \\\\ \\delta q (t) = \\frac{\\partial}{\\partial t} q(t, \\epsilon) |_{\\epsilon = 0} ... \\text{- virtual displacement} \\end{align*}\\] \\[\\begin{align*} \\frac{d}{dt} \\int_a^b L(q(t,\\epsilon), \\dot q(t,\\epsilon)) \\space dt |_{\\epsilon = 0} &amp;= 0 \\\\ q(t) \\text{ is a critical curve} \\end{align*}\\] Proposition: \\(\\frac{d}{dt} (\\frac{\\partial L}{\\partial q^i}) - \\frac{\\partial L}{\\partial q^i} = 0\\) . \\[\\begin{align*} q(t,\\epsilon) = q(t,0) + \\epsilon \\\\ \\text{see Feynman or Goldstein} \\end{align*}\\] Feynman thesis moves least action principle to quantum mechanics. Now we extend to Lagrange d’Alembert Principle… . \\[\\begin{align*} \\frac{d}{dt} (\\frac{\\partial L}{\\partial q^i}) - \\frac{\\partial L}{\\partial q^i} = F_i \\\\ F_i \\text{ is generalized force external to system.} \\end{align*}\\] One example of \\(F_i\\) is friction. Another example is $$ non-holonomic constraint forces. Nonholonomic example: . \\[\\begin{align} \\dot x = R \\cos \\rho \\dot \\theta \\\\ \\dot y = R \\sin \\rho \\dot \\theta \\\\ \\sum_{k=1}^n a^i_k(a), \\dot q^k = 0, j=1,...,m \\\\ \\exists b^j(q) \\text{s.t.} \\\\ \\sum_{k=1}^n \\frac{\\partial b^i}{\\partial q^k} \\dot q^k = 0 \\text{ - holonomic} \\end{align}\\] The force of constraint will keep a non-holonomic system (like a penny rolling on a table) that keeps the penny upright on the table. \\(F_i \\delta q^1 + ... + F_m \\delta q^m = 0\\). Forces of constraint do no work. Now plug into EoM . \\[\\begin{align*} \\frac{d}{dt} (\\frac{\\partial L}{\\partial q^i}) - \\frac{\\partial L}{\\partial q^i} = \\sum_{j=1}^m \\lambda_j a_i^j \\end{align*}\\] where \\(\\lambda_j\\) are the Lagrange multipliers associated with the constraints and \\(a_i^j\\) are the constraints. And thus . \\[\\begin{align*} F_i = a^i_1 + ... + \\lambda_m a_i^m \\\\ F_i \\perp ker(A) \\end{align*}\\] The last statement is just another way of saying that \\(F_i\\) should be in the range of constraint matrix. It cannot lie in the nullspace (thus has to be perpendicular). Now lets look at an example problem: . \\[\\begin{align*} L = \\frac{1}{2}m(\\dot x^2 + \\dot y^2) + \\frac{1}{2} I \\dot \\theta^2 + \\frac{1}{2}J \\rho ^2 \\\\ \\dot x = R \\cos \\rho \\dot \\theta \\\\ \\dot y = R \\sin \\rho \\dot \\theta \\end{align*}\\] Constraints: . \\[\\begin{align*} \\delta x = R \\cos \\rho \\delta \\theta \\\\ \\delta y = R \\sin \\rho \\delta \\theta \\end{align*}\\] Substitute into the Lagrangian: . \\[\\begin{align*} L = \\frac{1}{2}(\\dot x^2 + \\dot y^2) + \\frac{1}{2} I \\dot \\theta^2 + \\frac{1}{2}J \\rho ^2 \\\\ \\dot x = R \\cos \\rho \\dot \\theta \\\\ \\dot y = R \\sin \\rho \\dot \\theta \\end{align*}\\] Constraints: . \\[\\begin{align*} \\delta x = R \\cos \\rho \\delta \\theta \\\\ \\delta y = R \\sin \\rho \\delta \\theta \\end{align*}\\] Now our EoM is this when throwing in \\(\\frac{d}{dt}\\). \\[\\begin{align} m \\ddot x \\delta x + m \\ddot y \\delta y + I \\ddot \\theta \\delta \\theta + J \\ddot \\rho \\delta \\rho = 0 \\end{align}\\] Now replace \\(\\delta\\). \\[\\begin{align} m \\ddot x (R \\cos \\rho \\delta \\theta) + m \\ddot y (R \\sin \\rho \\delta \\theta) + I \\ddot \\theta \\delta \\theta + J \\ddot \\rho \\delta \\rho = 0 \\end{align}\\] Set coeffs of \\(\\delta \\theta\\) and \\(\\delta \\rho\\) to 0. \\[\\begin{align} J \\ddot \\rho = 0 \\\\ m \\ddot x (R \\cos \\rho) + m \\ddot y (R \\sin \\rho) + I \\ddot \\theta = 0 \\end{align}\\] TAKEAWAY: can think of external forces as ones that come from constraints…. ",
    "url": "/blog/class-notes/00-Geometric-Mechanics/Lectures/09-03.html#recap",
    
    "relUrl": "/class-notes/00-Geometric-Mechanics/Lectures/09-03.html#recap"
  },"10": {
    "doc": "September 3",
    "title": "ODE’s",
    "content": "Def’n: A dynamical system is a \\(C^1\\) map \\(\\mathbb{R} \\times S \\mapsto \\phi\\) where \\(S\\) is an open set of euclidian space (eventually we’ll do manifold). \\(\\phi(t,x) = \\phi_t(x)\\). s.t. | flow \\(\\phi_0: S \\mapsto S\\) | flow composition \\(\\phi_t \\circ \\phi_s = \\phi_{t+s}\\) | . For a linear system, \\(e^{At}\\) is the flow of \\(\\dot x = Ax\\). Fundamental theorem of ODE’s. ",
    "url": "/blog/class-notes/00-Geometric-Mechanics/Lectures/09-03.html#odes",
    
    "relUrl": "/class-notes/00-Geometric-Mechanics/Lectures/09-03.html#odes"
  },"11": {
    "doc": "1. Autoregressive",
    "title": "1. Autoregressive Models",
    "content": "Given a dataset \\(x^{(1)},...,x^{(n)} \\sim p_{\\text{data}}(x)\\), we would like to learn a distribution \\(p_\\theta(x)\\) such that sampling from this distribution is like sampling from \\(p_{\\text{data}}(x)\\). ",
    "url": "/blog/class-notes/05-Unsupervised-Learning/1_autoregressive.html#1-autoregressive-models",
    
    "relUrl": "/class-notes/05-Unsupervised-Learning/1_autoregressive.html#1-autoregressive-models"
  },"12": {
    "doc": "1. Autoregressive",
    "title": "1.1 Naive Frequentist Approach",
    "content": "Suppose the support of our dataset \\(p_\\text{data}(x)\\) is \\(\\mathcal{X} = \\{1,...,k\\}\\). To train this model, we count how frequently \\(i \\in \\mathcal{X}\\) appears in the dataset and normalize it: . \\[\\begin{equation} p_\\theta(x = i) = \\frac{1}{\\mid \\mathcal{D} \\mid}\\sum_{d = 1}^{\\mid \\mathcal{D} \\mid} \\mathbb{I}(x^{(d)} = i) \\end{equation}\\] where \\(\\mathcal{D}\\) is the dataset and \\(\\mathbb{I}\\) is the indicator function. Inference simply a lookup of \\(p_\\theta\\). Sampling we can use inverse cumulative distributions to sample any distribution from a standard uniform distribution \\(U(0,1)\\). | Get the inverse CDF of \\(p_\\theta\\), which is a piecewise function. | Sample \\(u \\sim U(0,1)\\). | Find the smallest \\(i\\) such that \\(p_\\theta(x \\leq i) \\geq u\\). | . Limitation The biggest issue with this approach is that it generalizes poorly. Maximum Likelihood Estimation (MLE) What we just did was MLE for a very simple model. MLE is formulated by taking a dataset \\(x^{(1)},...,x^{(n)}\\) and finding the parameters \\(\\theta\\) that maximize the likelihood of the data given the model: . \\[\\begin{align} \\theta^* &amp;= \\arg \\min_\\theta \\mathcal{L}(x^{(1)},...,x^{(n)}; \\theta) \\\\ &amp;= \\arg \\min_\\theta \\sum_{d=1}^{\\mid \\mathcal{D} \\mid} -\\log p_\\theta(x^{(d)}) \\end{align}\\] This is the generalized Negative Log-Likelihod (NLL) loss function. For our discrete distribution, the analytical solution to this problem is: . \\[\\begin{equation} \\hat p_\\text{data}(x) = \\frac{1}{\\mid \\mathcal{D} \\mid}\\sum_{d=1}^{\\mid \\mathcal{D} \\mid} \\mathbb{I}(x^{(d)} = x) \\end{equation}\\] Stochastic Gradient Descent (SGD) We already know about SGD, but its effects on the loss function can be viewed as taking the expected value of the loss: . \\[\\begin{equation} \\arg \\min_\\theta \\mathbb{E}_{x \\sim p_\\text{data}(x)} \\left[ -\\log p_\\theta(x) \\right] \\end{equation}\\] Of course, SGD is used because it allows us to train our models for large datasets. Extension We can also extend our model to more complex distributions like Mixture of Gaussians (MoG) or Mixture of Logistics (MoL). The idea is to use a mixture of distributions to model the data. For example, we can use a mixture of Logistics to model the data as: . \\[\\begin{equation} v \\sim \\sum_{i=1}^K \\pi_i \\text{logistic}(\\mu_i, s_i) \\\\ p(x \\mid \\pi, \\mu, s) = \\sum_{i=1}^K \\pi_i \\left[\\sigma(\\frac{x+0.5 - \\mu_i}{s_i}) - \\sigma( \\frac{x - 0.5 - \\mu_i}{s_i} ) \\right] \\end{equation}\\] People have used these as output heads for neural networks (instead of just frequentist appraoch) and learn the parameters such that we can sample from the mixture of distributions. With just the naive frequentist approach, there are just way too many parameters to learn and it requires way too much data to learn. We wish to tackle the high-dimensional data challenge . ",
    "url": "/blog/class-notes/05-Unsupervised-Learning/1_autoregressive.html#11-naive-frequentist-approach",
    
    "relUrl": "/class-notes/05-Unsupervised-Learning/1_autoregressive.html#11-naive-frequentist-approach"
  },"13": {
    "doc": "1. Autoregressive",
    "title": "1.2 Autoregressive Models",
    "content": "Chain Rule Any multi-variable distribution can be written as a product of conditional distributions (think Bayesian Networks). One such example is when the current variable is conditioned on all the previous variables: . \\[\\begin{equation*} p_\\theta(x) = \\prod_{i=1}^d p_\\theta (x_i \\mid x_{1:i-1}) \\end{equation*}\\] this is called an autoregressive model . 1.2.1 Bayes Net . Instead of conditioning on all the previous variables, we can figure out which variables to condition on through the Bayesian Network structure. | Gives great inductive bias for the underyling structure | Sparsification introduces strong asumptions (limits expressivity) | Limited parameter sharing | . 1.2.2 Masked Autoencoder for Distribution Estimation (MADE) . For MADE, we parametrize conditionals with neural network (learning the bayes net structure). The simplest way to think about this is as follows. Let us consider a 3-layer MLP to generate a distribution: . \\[\\begin{equation} p_\\theta(x) = \\mathbf{V}\\sigma(\\mathbf{W}_2\\sigma(\\mathbf{W}_1\\mathbf{x})) \\end{equation}\\] where \\(\\mathbf{W}_1\\) is the first layer, \\(\\mathbf{W}_2\\) is the second layer, and \\(\\mathbf{V}\\) is the output layer. Instead, we actually add masks to filter which neuron in the network affects the output of the next layer: . \\[\\begin{equation} p_\\theta(x) = (\\mathbf{V} \\odot \\mathbf{M}_3) \\sigma((\\mathbf{W}_2 \\odot \\mathbf{M}_2)\\sigma((\\mathbf{W}_1 \\odot \\mathbf{M}_1)\\mathbf{x})) \\end{equation}\\] where \\(\\odot\\) is the element-wise product and \\(\\mathbf{M}_i\\) is the mask for layer \\(i\\). The masks are 0 or 1. While this approach is expressive, it is not very efficient. It needs more parameter sharing in order to be efficient. 1.2.3 Causal Masked Neural Models . This is like MADE but enables parameter sharing and adds coordinate coding to individualize conditionals. Coordinate coding is appending the index of the variable to its features. The parameter sharing comes from one timestep to another. An example of this is WaveNet which uses Masked Temporal 1D Convolution. Pros: . | parameter sharing across conditionals | add coordinate coding to individualize conditionals Cons: | finite context window | . Casual masked neural models seem promising. 1.2.4 RNN . In this autoregressive context, RNNs do parameter sharing + “infinite-loop-back” . \\[\\begin{equation} \\log p(\\mathbf{x}) = \\sum_{i=1}^d \\log p_\\theta(x_i \\mid x_{1:i-1}) \\end{equation}\\] which the RNN parses through. This is done serially. Pros: . | Expressive Cons: | not as amenable to parallelization | backprop has vanishing/exploding gradient issues. | hard to have signal propagate from long history. | expressive but not expressive enough? compared to next models. | . RNN can actually be seen as a very deep special, highly restrictive causal masked model. ",
    "url": "/blog/class-notes/05-Unsupervised-Learning/1_autoregressive.html#12-autoregressive-models",
    
    "relUrl": "/class-notes/05-Unsupervised-Learning/1_autoregressive.html#12-autoregressive-models"
  },"14": {
    "doc": "1. Autoregressive",
    "title": "1.3 More about Casual Masked Neural Models",
    "content": "1.3.1 Convolutional . So we would like to take the lessons we learned from MADE and apply them to convolutional networks. The idea is to use a convolutional network with a causal mask. just like in the MADE example, we mask the convolutional kernel in an autoregressive fashion (where the kernel is ordered row-major). Then we can autoregressively sample the pixels in the image going from left to right and top to bottom. Doing the PixelCNN-style masking has one issue where it has a blind spot in the receptive field (doesn’t see pixels to right or below). In terms of parameter sharing, convolutional layers are already shared when computing across an image. 1.3.2 Attention . Scaled Dot Product Attention : . \\[\\begin{align} A(q,K,V) = \\sum_i \\frac{\\exp \\{ s(q,k_i) \\}}{ \\sum_j \\exp \\{ s(q,k_j) \\} } v_i \\\\ s(q,k) = \\frac{q \\cdot k}{\\sqrt{d}} \\\\ A(Q,K,V) = \\text{softmax}(QK^T)V \\\\ Q = YW^Q_i \\\\ K = XW^K_i \\\\ V = XW^V_i \\end{align}\\] for self-attention, \\(X = Y\\) and they are inputs into the attention layer. For cross-attention, $X$$ is output of last encoder block and Y is output of previous decoder block. Now we can interpret attention as learning a weighted average of the values of \\(V\\). The weights are given by the inner product similarity between the query \\(q\\) and the keys \\(k_i\\). The weights are normalized by the softmax function. Masked Attention we can now apply the autoregressive masking as inspired by MADE and PixelCNN. \\[\\begin{align} A(q,K,V) = \\sum_i \\frac{\\exp \\{ s(q,k_i) \\}}{ \\sum_{j &lt; i} \\exp \\{ s(q,k_j) \\} } v_i \\\\ s(q,k) = \\frac{q \\cdot k}{\\sqrt{d}} - (1 - M(q,k)) * 10^{10} \\\\ A(Q,K,V) = \\text{softmax}(QK^T - (1 - M)* 10^{10}) V \\end{align}\\] . As we can see, the masked attention is weird. We don’t have to stick with this autoregressive order, but it is the most popular to do it row-major. 1.3.3 Tokenization . When you think of tokens, think of discrete items. Tokenization is the process of converting a complex modality into discrete data. For example, in NLP, we can convert a sentence into a sequence of tokens. We can tokenize using a variety of methods. The most common ones are: . | Byte-Pair Encoding (BPE) for words. | (0-255) tokens per pixel for images. | discrete autoencoder where bottleneck embedding is the token. | . 1.3.4 Caching . This relates mostly to transformers. You cache the \\(K\\) and \\(V\\) matrices for the attention layer. ",
    "url": "/blog/class-notes/05-Unsupervised-Learning/1_autoregressive.html#13-more-about-casual-masked-neural-models",
    
    "relUrl": "/class-notes/05-Unsupervised-Learning/1_autoregressive.html#13-more-about-casual-masked-neural-models"
  },"15": {
    "doc": "1. Autoregressive",
    "title": "1. Autoregressive",
    "content": " ",
    "url": "/blog/class-notes/05-Unsupervised-Learning/1_autoregressive.html",
    
    "relUrl": "/class-notes/05-Unsupervised-Learning/1_autoregressive.html"
  },"16": {
    "doc": "2. Flow",
    "title": "2. Likelihood Models: Flow Models",
    "content": "We stil want to learn a distribution \\(p_\\theta(x)\\) and sample from it. Recall Autoregressive models issues: . | Sampling is serial (hence slow) | Lacks latent representation / embedding space | Limited to discrete data (at least in terms of experimental success) | . Flow models will address these issues, but its performance not as good as other models. ",
    "url": "/blog/class-notes/05-Unsupervised-Learning/2_flow.html#2-likelihood-models-flow-models",
    
    "relUrl": "/class-notes/05-Unsupervised-Learning/2_flow.html#2-likelihood-models-flow-models"
  },"17": {
    "doc": "2. Flow",
    "title": "2.1 Foundation of Flows",
    "content": "Probability Density Function (PDF) : \\(P( x \\in [a,b] ) = \\int_a^b p(x) dx\\) this is probability that x lands in an interval. Review: Fitting density model : We formulate max likelihood problem: . \\[\\begin{equation} \\max_\\theta \\sum_i \\log p_\\theta(x^{(i)}) \\end{equation}\\] where \\(x^{(i)}\\) is the i-th data point. We can reformulate this with SGD and minimization: . \\[\\begin{equation} \\min_\\theta \\mathbb{E}_{x \\sim p_\\theta} \\left[ -\\log p_\\theta(x) \\right] \\end{equation}\\] Another way to quantize (discretize) the data to a discrete distribution instead of continuous. Example Density Model : Mixtures of Gaussians (MoG): . \\[\\begin{equation} p_\\theta (x) = \\sum_{i=1}^k \\pi_i \\mathcal{N}(x;\\mu_i, \\sigma_i^2) \\end{equation}\\] where \\(\\theta = \\{ \\pi_1, ..., \\pi_k, \\mu_1, ..., \\mu_k, \\sigma_1^2, ..., \\sigma_k^2 \\}\\). Our goal is to learn the parameters with NLL loss. Gaussian Perturbation of an image will lead to an unlikely image. Nearby an image, there is not many correct images. | What if we tried other parametrized distributions? Not that many choices. We admit defeat and want to find a better fit. | . Flow Models refer to us going from embedding space back to data space. Say \\(z = f_\\theta (x)\\). We wish to sample by getting \\(x = f_\\theta^{-1}(z)\\). This assumes that \\(f\\) has an inverse. This is a strong assumption. VAE is where we learn the inverse instead. \\[\\begin{align*} z = f_\\theta (x) \\\\ p_\\theta (x) dx = p(z) dz \\\\ p_\\theta (x) = p(f_\\theta (x)) \\left| \\frac{\\partial f_\\theta (x)}{\\partial x} \\right| \\end{align*}\\] \\(f\\) needs to be differentiable and invertible. Now, we can view this in terms of datapoint \\(x^{(i)}\\). \\[\\begin{align*} p_\\theta (x^{(i)}) &amp;= p_Z(z^{(i)}) \\left| \\frac{\\partial z}{\\partial x}(x^{(i)}) \\right| \\\\ &amp;= p_Z(f_\\theta (x^{(i)})) \\left| \\frac{\\partial f_\\theta }{\\partial x}(x^{(i)}) \\right| \\end{align*}\\] Our maximum likelihood objective is: . \\[\\begin{equation} \\max_\\theta \\sum_i \\log p_\\theta(x^{(i)}) = \\max_\\theta \\sum_i \\log p_Z(f_\\theta (x^{(i)})) + \\log \\left| \\frac{\\partial f_\\theta }{\\partial x}(x^{(i)}) \\right| \\\\ \\end{equation}\\] Choices to make: . | Invertible function class. | exp | polynomials | cumulative distribution functions (CDFs)… mixture of gaussians | compose flows | . | Embedding space density . | ideally easy distribution to sample from | unit gaussian (normalizing flows) | mixture of gaussians | uniform distribution | . | . How to sample from \\(p_\\theta(x)\\)? First, sample from \\(p_Z(z)\\). Then, apply the inverse function \\(f_\\theta^{-1}\\) to get back to data space. Remember that we can use inverse CDF to sample from any distribution. We start from uniform distribution (\\(p_Z\\)) and apply the inverse CDF to sample distribution that we want. Really how we have been doing inverse CDF is a special case of flow. ",
    "url": "/blog/class-notes/05-Unsupervised-Learning/2_flow.html#21-foundation-of-flows",
    
    "relUrl": "/class-notes/05-Unsupervised-Learning/2_flow.html#21-foundation-of-flows"
  },"18": {
    "doc": "2. Flow",
    "title": "2.2 2D Flows",
    "content": "We have the following: . \\[x_1 \\mapsto z_1 = f_{\\theta_1}(x_1)\\] \\[z_1 \\mapsto x_1 = f_{\\theta_1}^{-1}(z_1)\\] \\[x_2 \\mapsto z_2 = f_{\\theta_2}(x_1,x_2)\\] \\[z_2,x_1 \\mapsto x_2 = f_{\\theta_2}^{-1}(x_1,z_2)\\] If we do: . \\[z_2,x_1 \\mapsto x_2 = f_{\\theta_2}^{-1}(z_1,z_2)\\] then this is inverse autoregressive flow. Remember, this is for 2-D flow which is just a 2-dimensional vector that we are sampling. 2.2.1 Training Objective . \\[\\begin{align*} \\log p_\\theta(x_1, x_2) &amp;= \\log p_{Z_1}(z_1) + \\log \\left| \\frac{\\partial z_1 (x_1)}{\\partial x_1} \\right| \\\\ &amp;+ \\log p_{Z_2}(z_2) + \\log \\left| \\frac{\\partial z_2 (x_1, x_2)}{\\partial x_2} \\right| \\\\ &amp;= \\log p_{Z_1}(f_{\\theta_1}(x_1)) + \\log \\left| \\frac{\\partial f_{\\theta_1}(x_1)}{\\partial x_1} \\right| \\\\ &amp;+ \\log p_{Z_2}(f_{\\theta_2}(x_1,x_2)) + \\log \\left| \\frac{\\partial f_{\\theta_2}(x_1,x_2)}{\\partial x_2} \\right| \\end{align*}\\] Then use this with NLL to train the model. Data distribution will be a set of 2d vectors. We try to learn the distribution of 2d vectors using this format. ",
    "url": "/blog/class-notes/05-Unsupervised-Learning/2_flow.html#22-2d-flows",
    
    "relUrl": "/class-notes/05-Unsupervised-Learning/2_flow.html#22-2d-flows"
  },"19": {
    "doc": "2. Flow",
    "title": "2.3 N-D Flows",
    "content": "We can see from the previous section the obvious autoregressive extension for n-dimensional flows. \\[\\begin{align*} x_1 \\sim p_\\theta(x_1) \\\\ x_1 = f_{\\theta}^{-1}(z_1) \\\\ x_2 \\sum p_\\theta(x_2|x_1) \\\\ x_2 = f_{\\theta}^{-1}(z_2;x_1) \\\\ x_3 \\sim p_\\theta(x_3|x_1,x_2) \\\\ x_3 = f_{\\theta}^{-1}(z_3;x_1,x_2) \\end{align*}\\] Sampling is invertible mapping from \\(z\\) to \\(x\\). We train as before. Inverse Autoregressive Flow : . ",
    "url": "/blog/class-notes/05-Unsupervised-Learning/2_flow.html#23-n-d-flows",
    
    "relUrl": "/class-notes/05-Unsupervised-Learning/2_flow.html#23-n-d-flows"
  },"20": {
    "doc": "2. Flow",
    "title": "2.4 Dequantization",
    "content": " ",
    "url": "/blog/class-notes/05-Unsupervised-Learning/2_flow.html#24-dequantization",
    
    "relUrl": "/class-notes/05-Unsupervised-Learning/2_flow.html#24-dequantization"
  },"21": {
    "doc": "2. Flow",
    "title": "2. Flow",
    "content": " ",
    "url": "/blog/class-notes/05-Unsupervised-Learning/2_flow.html",
    
    "relUrl": "/class-notes/05-Unsupervised-Learning/2_flow.html"
  },"22": {
    "doc": "3. Latent Variable Models",
    "title": "3. Latent Variable Models",
    "content": " ",
    "url": "/blog/class-notes/05-Unsupervised-Learning/3_lvm.html",
    
    "relUrl": "/class-notes/05-Unsupervised-Learning/3_lvm.html"
  },"23": {
    "doc": "3a. LVM Papers",
    "title": "3a. LVM Papers",
    "content": " ",
    "url": "/blog/class-notes/05-Unsupervised-Learning/3a_lvm.html",
    
    "relUrl": "/class-notes/05-Unsupervised-Learning/3a_lvm.html"
  },"24": {
    "doc": "4. Implicit Models",
    "title": "4. Implicit Models",
    "content": " ",
    "url": "/blog/class-notes/05-Unsupervised-Learning/4_implicit_sample.html",
    
    "relUrl": "/class-notes/05-Unsupervised-Learning/4_implicit_sample.html"
  },"25": {
    "doc": "5. Diffusion",
    "title": "5. Diffusion",
    "content": " ",
    "url": "/blog/class-notes/05-Unsupervised-Learning/5_diffusion.html",
    
    "relUrl": "/class-notes/05-Unsupervised-Learning/5_diffusion.html"
  },"26": {
    "doc": "5a. Diffusion Papers",
    "title": "5a. Diffusion Papers",
    "content": " ",
    "url": "/blog/class-notes/05-Unsupervised-Learning/5a_diffusion.html",
    
    "relUrl": "/class-notes/05-Unsupervised-Learning/5a_diffusion.html"
  },"27": {
    "doc": "6. Self-Supervised Learning",
    "title": "6. Self-Supervised Learning",
    "content": " ",
    "url": "/blog/class-notes/05-Unsupervised-Learning/6_self_supervised.html",
    
    "relUrl": "/class-notes/05-Unsupervised-Learning/6_self_supervised.html"
  },"28": {
    "doc": "Advective Diffusion",
    "title": "Topological Generalisation with Advective Diffusion Transformers",
    "content": "This is a continuation of my notes from the graph diffusion perspective. Advective diffusion transformers (diffusion through message-passing) combines message-passing with transformers for better topological generalisation. GNNS are often reported to show poor performance when train/test datasets are generated from different distributions (ditribution shift… or topological shift). Neural Graph Diffusion . Neural graph diffusion (NGD) formulates a continuous-time differential equation of the GNN layers using the message-passing connection with heat diffusion equation. \\[\\begin{align} \\dot{\\mathbf{X}}(t) = \\text{div}(\\mathbf{S}(t) \\nabla \\mathbf{X}(t)) \\end{align}\\] We initialize this equation with . \\[\\begin{align} \\mathbf{X}(0) = \\text{ENC}(\\mathbf{X}_i) \\end{align}\\] Then we run the differential equation and integrate it to get the following. \\[\\begin{align} \\mathbf{X}_o = \\text{DEC}(\\mathbf{X}(T)) \\end{align}\\] which is then the output of the architecture. We can now interpret most message-passing layers as some form of the stated diffusion equation. For example, the linear version of this equation is the GCN layer. The nonlinear version is the GAT layer. The generalisation ability of the NGD is up to the sensitivity of the solution \\(\\mathbf{X}(T) = f(\\mathbf{X}(0), \\mathbf{A})\\) to a perturbation of the adjacency matrix \\(\\tilde{\\mathbf{A}} = \\mathbf{A} + \\delta \\mathbf{A}\\). For both nonlinear and linear graph diffusion, the solution is still highly sensitive to adjacency perturbations. Graph Transformers . So the graph transformer processes the graph using a computational method much closer to the actual QKV attention mechanism described: . \\[\\begin{equation} \\text{Att}(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}) = \\text{softmax} \\left( \\frac{\\mathbf{Q} \\mathbf{K}^T}{\\sqrt{d_k}} \\right) \\mathbf{V} \\end{equation}\\] For the graph transformer, we calculate the following: \\(\\begin{align} \\mathbf{Q} &amp;= \\mathbf{X} \\mathbf{W}_Q \\\\ \\mathbf{K} &amp;= \\mathbf{X} \\mathbf{W}_K \\\\ \\mathbf{V} &amp;= \\mathbf{X} \\mathbf{W}_V \\end{align}\\) . We then re-use the attention equation to calculate the attention scores for every node in the graph. If there are edge features, we can use those to modify the attention scores. However, it is important to note that this is done GLOBALLY on a graph instead of GAT which is done locally on a single node’s neighbors for every node. This can also be seen as non-local diffusion in our context. This non-local diffusion gives stronger topological generalisation. However this asssumes that node labels and graph topology are statistically independent. However, we want to analyze this where they are statistically dependent. Advective Graph Diffusion . We now modify our diffusion equation to contain an advective term: . \\[\\begin{equation} \\dot{\\mathbf{X}}(t) = \\text{div}(\\mathbf{S}(t) \\nabla \\mathbf{X}(t)) + \\beta\\text{div}(\\mathbf{V}(t) \\nabla \\mathbf{X}(t)) \\end{equation}\\] this is known as advective diffusion which arises in fluid dynamics. The advective term relates to the movement of the water. The diffusion term comes from the graph transformer term (global attention) and the advection term comes from local operations. We will use the equation above similarly to how NGD uses its diffusion equation. This was shown to be superior to the previous models (GCN, GAT, MPNN, NGD) in terms of topological generalisation for the task of graph generation. ",
    "url": "/blog/graph-notes/advective-diff.html#topological-generalisation-with-advective-diffusion-transformers",
    
    "relUrl": "/graph-notes/advective-diff.html#topological-generalisation-with-advective-diffusion-transformers"
  },"29": {
    "doc": "Advective Diffusion",
    "title": "Advective Diffusion",
    "content": " ",
    "url": "/blog/graph-notes/advective-diff.html",
    
    "relUrl": "/graph-notes/advective-diff.html"
  },"30": {
    "doc": "Beyond Pick and Place Workshop",
    "title": "Beyond Pick and Place Workshop",
    "content": " ",
    "url": "/blog/conference-notes/icra-2025/workshop/beyond-pick-place.html",
    
    "relUrl": "/conference-notes/icra-2025/workshop/beyond-pick-place.html"
  },"31": {
    "doc": "Beyond Pick and Place Workshop",
    "title": "Katerina Fragkiadaki - 3D Generative Manipulation and Object Dynamics",
    "content": "Autonomous vehicles technology to robot manipulation. | Foundation Models | 2D diffusion policy or \\(\\pi_0\\) | 3D diffuser Actor | . TO do 3d diffusion, predict gripper position . | 3d point tracks | model-based control with generative models (guided diffusion) | . ",
    "url": "/blog/conference-notes/icra-2025/workshop/beyond-pick-place.html#katerina-fragkiadaki---3d-generative-manipulation-and-object-dynamics",
    
    "relUrl": "/conference-notes/icra-2025/workshop/beyond-pick-place.html#katerina-fragkiadaki---3d-generative-manipulation-and-object-dynamics"
  },"32": {
    "doc": "Beyond Pick and Place Workshop",
    "title": "David Held - Spatially-aware Robot Learning",
    "content": ". | generalize dexterous manipulation to unseen objects? | contact point network creates critic map (Q-value) …. gets contact locaiton through softmax | motion network for actions (hybrid discrete-continuous RL) . | learning hybrid actor-critic maps | . | Visual Manip w/ legs | HacMan++ | ArticuBot . | generate lots of demos in simulation | . | Point Cloud Diffusion Transformer (delta location of points) . | this is SE(3) equivariant | 3D diffusion model | . | TAPIP3D | Discrete Diffusion | . This is very similar to what I thought of (actor-critic maps) . ",
    "url": "/blog/conference-notes/icra-2025/workshop/beyond-pick-place.html#david-held---spatially-aware-robot-learning",
    
    "relUrl": "/conference-notes/icra-2025/workshop/beyond-pick-place.html#david-held---spatially-aware-robot-learning"
  },"33": {
    "doc": "Beyond Pick and Place Workshop",
    "title": "Rachel Holladay - Modelling Mechanics for Dexterous Decision-Making",
    "content": "Works on TAMP (Task And Motion Planning) . I think this is the same stuff as before. New work (beyond pick and place) . | guarded planning | GUARDED stands for: guiding uncertainty accounting for risk and dynamics | . This is just revamped safety-based robotics. ",
    "url": "/blog/conference-notes/icra-2025/workshop/beyond-pick-place.html#rachel-holladay---modelling-mechanics-for-dexterous-decision-making",
    
    "relUrl": "/conference-notes/icra-2025/workshop/beyond-pick-place.html#rachel-holladay---modelling-mechanics-for-dexterous-decision-making"
  },"34": {
    "doc": "Beyond Pick and Place Workshop",
    "title": "Micheal Posa - Dexterity and generalization?",
    "content": ". | knows that robots will be first deployed doing tasks w/ BC or RL. | pitching his stuff as forward-thinking | . Model-based hierarchies struggle a lot with robotics. Micheal Posa thinks contact rich dynamics are a big problem. Goes into Vysics which reconstructs objects using vision and contact-rich physics. Reconstructs objects with partial view and contact. Russ said before -&gt; Goal: “LQR” but for multi-contact systems . Can’t solve the contact-rich MPC (QCQP or MIP). Using linearization and ADMM you can split this into 2 parts: . | QP dynamic constraints | complementarity constraint projection (non-convex) | it works ok but is inherently local | . Now he wants to do sampling with CI-MPC . | bi-level hybrid reasoning | . Insight: Find an idea that you have unique insight into doing. If no one else did it, it wouldn’t be solved. Don’t chase the newest thing. ",
    "url": "/blog/conference-notes/icra-2025/workshop/beyond-pick-place.html#micheal-posa---dexterity-and-generalization",
    
    "relUrl": "/conference-notes/icra-2025/workshop/beyond-pick-place.html#micheal-posa---dexterity-and-generalization"
  },"35": {
    "doc": "Beyond Pick and Place Workshop",
    "title": "Spotlight",
    "content": "List of papers that are interesitng: . | GET-ZERO: Graph Embodiment Transformer for Zero-shot Embodiment Generalization | Metric Semantic Manipulation-enhanced Mapping | AugInsert: Learning robust force-visual policy via data augmentation | FoAR: Force-Aware Reactive Policy for Contact-Rich Robotic Manipulation | Streaming Flow Policy: Simplifying diffusion/flow policies | DyWA: Dynamics-adaptive World Action Model for Generalizable Non-prehensile Manipulation | SAIL: Faster-than-Demonstration Execution of Imitation Learning Policies | Reactive Diffusion Policy: Slow-Fast Visual-Tactile Policy Learning for Contact-Rich Manipulation | . ",
    "url": "/blog/conference-notes/icra-2025/workshop/beyond-pick-place.html#spotlight",
    
    "relUrl": "/conference-notes/icra-2025/workshop/beyond-pick-place.html#spotlight"
  },"36": {
    "doc": "Beyond Pick and Place Workshop",
    "title": "Russ Tedrake - Multitask pretraining",
    "content": "We got ourselves an LBM? He wants to make it sound more like a biology talk. diffusion policy (DP) as single-task. LBMs are multi-task version of DP. | language conditioned visuomotor policy | VLA is one way to make an LBM. | . Shows evidence of a long-horizon task (no decomposition): . | fine-tune LBM on a single task | caveat: teleoperators are really good | takes one-day of data to make it work. | . My Question: Is LBM trained on same environment as his evidence video? . Does LBM pretraining help on new single tasks? . | the first principle is that you must not fool yourself and you are the easiest person to fool. (Richard Feynman) | Evidence that pretraining makes dexterous manipulation better? | . Hypothesis 1: Multitask scaling laws. By training on many tasks, we can add new tasks more quickly. | less data for the same performance. | . Hypothesis 2: Improved Robustness. Due to more diverse training data; skill transfer. | More data is more robustness. | . Diffusion policy is scaled up for multitask: . | Resnet 18 -&gt; CLIP-ViT/B-16 for RGB | new CLIP-ViT/B-32 for language encoding (mostly frozen) | UNet -&gt; DiT | ~150M -&gt; ~560M parameters | . Why focus on this? . | sota for dexterous tasks | simpler to study than vlas | . Evaluation: Real-world hardware testing: . | A/B testing -&gt; study relative scores (do everything in one day) | always blind, rnadomized trials | eval GUI for repeatable trials | rich reporting on “rubrics” and post-hoc video analysis | . Evaluation: Simulation-based testing: . | small number of high-quality scenes | greater than 10 tasks per scene | tasks is not visually obvious | . Sim+Real cotraining: . | we don’t use sim for datagen | we cotrain with sim teleop to use sim for eval | . Eval: Simulation-based testing: . | repeatable (up to GPU determinism) | sim rollouts from last week are still useful today | many more rollouts -&gt; better statistics | Runs automatically in cloud at interval checkpoints | strong correspondence | . Statistical testing: . | assuming i.i.d. bernoulli samples with unknown probability of success p. compute interval of confidence within 95% of p. | note: testing checkpoints, not architectures | many sources of randomness in pipeline: objects, environments, initial conditions. | diffusion gives a stochastic policy | randomness in training (not included in analysis) | high variability across tasks | . How many rollouts do we need? . | with N rollouts we can calculate expected value and variance | rollouts for sim + real | sufficient condition for intervals | can we do better? separate two policies. | we do bayesian analysis (violin plots) | . Experiment Design: . | We’re in low data regime . | ~200 demos | vs. 2000-5000 from gemini-robotics | . | many tasks are intentionally difficult . | severe distribution shifts | diverse initial conditions | . | philosophy . | at 0% or 100% success, we have no signal | in the middle, we can see the difference | . | . CLIP » ResNet; DiT » UNet; Relative Actions . Many architecture changes end up mostly in the noise: . | it’s really statistically in the noise for many papers choosing certain architectures. | hard to differentiate small changes | . Hypothesis 1 was fucked for some tasks . | it’s because of the language steering | . Hypothesis 1 finetuning ot unseen tasks: . | LBM got the same performance at only 15% of the data. | . Hypothesis 2 was pretty good . | huge difference between singletask and fine-tuned multi-task. | . 10 rollouts isn’t enough to be rigorous (it’s just a vibe check). ",
    "url": "/blog/conference-notes/icra-2025/workshop/beyond-pick-place.html#russ-tedrake---multitask-pretraining",
    
    "relUrl": "/conference-notes/icra-2025/workshop/beyond-pick-place.html#russ-tedrake---multitask-pretraining"
  },"37": {
    "doc": "Beyond WL-test",
    "title": "Beyond Weisfeiler-Lehman (approximate isomorphisms and metric embeddings)",
    "content": " ",
    "url": "/blog/graph-notes/beyond-wl.html#beyond-weisfeiler-lehman-approximate-isomorphisms-and-metric-embeddings",
    
    "relUrl": "/graph-notes/beyond-wl.html#beyond-weisfeiler-lehman-approximate-isomorphisms-and-metric-embeddings"
  },"38": {
    "doc": "Beyond WL-test",
    "title": "Higher-order WL test",
    "content": "One approach to extend the isomorphism test is to try k-WL tests and create GNNs that are based on these tests. https://arxiv.org/pdf/1905.1113 . From this paper, it takes a lot more computational cost and uses non-local operations. ",
    "url": "/blog/graph-notes/beyond-wl.html#higher-order-wl-test",
    
    "relUrl": "/graph-notes/beyond-wl.html#higher-order-wl-test"
  },"39": {
    "doc": "Beyond WL-test",
    "title": "Approximate isomorphism motivation",
    "content": "GNN can be seen as attempting to find a graph embedding \\(f(\\mathcal{G})\\) with the desired property: \\(\\begin{align} f(\\mathcal{G}) = f(\\mathcal{G'}) \\end{align}\\) . For the WL-1 test, if \\(\\mathcal{G} \\simeq \\mathcal{G}'\\), then \\(f(\\mathcal{G}) = f(\\mathcal{G}')\\) . This is necessary but not sufficient, so the statement: \\(f(\\mathcal{G}) = f(\\mathcal{G}')\\) implies \\(\\mathcal{G} \\simeq \\mathcal{G}'\\) is not necessarily true. In other words, 2 non-isomorphic graphs can result in equal embeddings. Practically speaking, it is better to deal with approximate isomorphisms. Since this is no longer a boolean (yes/no) thing, we must define this through a metric \\(d\\). Below is an example of 3 graphs \\(\\mathcal{G}, \\mathcal{G}', \\mathcal{G}''\\). We want to say that \\(\\mathcal{G}\\) is approximately isomorphic to \\(\\mathcal{G}'\\). We also want to say that \\(\\mathcal{G}'\\) and \\(\\mathcal{G}''\\) are not that isomorphic. The distance in this case should be much higher. ",
    "url": "/blog/graph-notes/beyond-wl.html#approximate-isomorphism-motivation",
    
    "relUrl": "/graph-notes/beyond-wl.html#approximate-isomorphism-motivation"
  },"40": {
    "doc": "Beyond WL-test",
    "title": "Graph Edit Distance",
    "content": "The graph edit distance (GED) is a measure of similarity between 2 graphs. The GED between two graphs \\(\\mathcal{G}_1, \\mathcal{G}_2\\) is defined as: \\(\\begin{equation} \\mathcal{D}(\\mathcal{G}_1, \\mathcal{G}_2) = \\displaystyle \\min_{(e_1,...,e_k) \\in \\mathcal{P}(\\mathcal{G}_1, \\mathcal{G}_2)} \\sum_{i=1}^k c(e_i) \\end{equation}\\) . In this case, \\(\\mathcal{P}(\\mathcal{G}_1, \\mathcal{G}_2)\\) is the set of edit paths transforming \\(\\mathcal{G}_1\\) to \\(\\mathcal{G}_2\\). \\(c(e) \\geq 0\\) is the cost function of a certain edit operation \\(e\\). This is the unique set of edit operators: . | vertex insertion | vertex deletion | vertex substitution | edge insertion | edge deletion | edge substitution | . An edit path is an ordered list of these edit operators to go from \\(\\mathcal{G}_1\\) to \\(\\mathcal{G}_2\\). We have seen many things like this in our data structures class (word edit distance). However, this is very discrete in nature and running the GED may take a long time and also be very hard to analyse. However, this is a proper measure of similarity between graphs. Just wanted to throw in an example that I’m most familiar with. Check out this page for another example of graph distance this page. ",
    "url": "/blog/graph-notes/beyond-wl.html#graph-edit-distance",
    
    "relUrl": "/graph-notes/beyond-wl.html#graph-edit-distance"
  },"41": {
    "doc": "Beyond WL-test",
    "title": "Graph metric through isometry embedding",
    "content": "An important property of a metric is \\(\\mathcal{D}(\\mathcal{G},\\mathcal{G}') = 0\\) iff \\(\\mathcal{G} \\simeq \\mathcal{G}'\\) which implies that isomorphic graphs have indiscernable embeddings. We consider an embedding to be an output of a function \\(f\\). We define an isometry embedding as . \\[\\begin{equation}\\label{isometry-eq} \\mathcal{D}(\\mathcal{G},\\mathcal{G}') = \\| f(\\mathcal{G}) - f(\\mathcal{G})' \\| \\end{equation}\\] which replaces the previous graph distance stuff with Euclidian distance. The thing is that designing this function \\(f\\) for every instance of graphs is way too tough. It’s possible that similar graphs are less similar in embedding space. ",
    "url": "/blog/graph-notes/beyond-wl.html#graph-metric-through-isometry-embedding",
    
    "relUrl": "/graph-notes/beyond-wl.html#graph-metric-through-isometry-embedding"
  },"42": {
    "doc": "Beyond WL-test",
    "title": "Approximate Isometry Embedding",
    "content": "Maybe the previous formulation is way too strict. If we relax Eq. (\\(\\ref{isometry-eq}\\)), then maybe we can get better properties. One such approach people found was metric dilation. \\[\\begin{equation}\\label{metric-dil} c^{-1} \\mathcal{D}(\\mathcal{G},\\mathcal{G}') \\leq \\| f(\\mathcal{G}) - f(\\mathcal{G}') \\| \\leq c \\mathcal{D}(\\mathcal{G}, \\mathcal{G}'), c \\geq 1 \\end{equation}\\] It kind of looks like two lipschitz conditions or a bi-lipschitz condition. This kind of embedding that follows this property is called an approximate isometry. This actually only addresses the problem where similar graphs are less similar in embedding space. However, you’ll notice when \\(\\mathcal{G} \\simeq \\mathcal{G}'\\), \\(\\mathcal{D}(\\mathcal{G}, \\mathcal{G}') = 0\\) which reduces Eq. (\\(\\ref{metric-dil}\\)) into: \\(\\|f(\\mathcal{G}) - f(\\mathcal{G}')\\| = 0\\). This is still a very strict condition and finding a function \\(f\\) that can achieve this would be too tough. Another trick people employ is to further relax this condition with metric distortion \\(\\epsilon\\): . \\[\\begin{equation} c^{-1} \\mathcal{D}(\\mathcal{G},\\mathcal{G}') - \\epsilon \\leq \\| f(\\mathcal{G}) - f(\\mathcal{G}') \\| \\leq c \\mathcal{D}(\\mathcal{G}, \\mathcal{G}') + \\epsilon, c \\geq 1 \\end{equation}\\] this sets a tolerance for how far apart graphs can be to be considered approximately isomorphic. ",
    "url": "/blog/graph-notes/beyond-wl.html#approximate-isometry-embedding",
    
    "relUrl": "/graph-notes/beyond-wl.html#approximate-isometry-embedding"
  },"43": {
    "doc": "Beyond WL-test",
    "title": "Probably Approximate Isometry",
    "content": "We can also add some probability on this boi. This is part of Probably Approximately Correct theory (PAC). \\[\\begin{equation} P(c^{-1} \\mathcal{D}(\\mathcal{G},\\mathcal{G}') \\leq \\| f(\\mathcal{G}) - f(\\mathcal{G}') \\| \\leq c \\mathcal{D}(\\mathcal{G}, \\mathcal{G}')) &gt; 1 - \\delta \\end{equation}\\] In this case, there is only metric dilation (approximate). Then metric distortion logic is all calculated under the probability inequality. The “probably” statement is obviously from the “P()” and will eventually yield better analysis. ",
    "url": "/blog/graph-notes/beyond-wl.html#probably-approximate-isometry",
    
    "relUrl": "/graph-notes/beyond-wl.html#probably-approximate-isometry"
  },"44": {
    "doc": "Beyond WL-test",
    "title": "Limitations",
    "content": "This doesn’t cover that fact that each node has a feature, and we can also calculate graph similarity through this information. We also have not designed GNNs using these newly introduced isometry embeddings. TODO: look at how to design GNNs based on this. ",
    "url": "/blog/graph-notes/beyond-wl.html#limitations",
    
    "relUrl": "/graph-notes/beyond-wl.html#limitations"
  },"45": {
    "doc": "Beyond WL-test",
    "title": "Beyond WL-test",
    "content": " ",
    "url": "/blog/graph-notes/beyond-wl.html",
    
    "relUrl": "/graph-notes/beyond-wl.html"
  },"46": {
    "doc": "Calculus of Variations - Chill",
    "title": "Calculus of Variations",
    "content": "I will yoink most of the notes from Zack Manchester’s Robot Dynamics 2022 class. ",
    "url": "/blog/prereq-notes/calc-variation.html#calculus-of-variations",
    
    "relUrl": "/prereq-notes/calc-variation.html#calculus-of-variations"
  },"47": {
    "doc": "Calculus of Variations - Chill",
    "title": "Calculus",
    "content": "\\[\\begin{align*} \\min_x f(x), \\space f: \\mathbb{R}^n \\to \\mathbb{R} \\\\ x \\in \\mathbb{R}^n \\end{align*}\\] We often want to solve “infinite-dimensional” optimization problems, where the decision variable is a function. How do we get to this point? . Let \\(y(t)\\) be such that \\(y: [0,T] \\to \\mathbb{R}\\) and \\(x \\in \\mathbb{R}^n\\), then \\(x = \\begin{bmatrix}y_1 &amp;&amp; y_2 &amp;&amp; ... &amp;&amp; y_n \\end{bmatrix}\\). If we let \\(n \\to \\infty\\), then we can get something “infinite-dimensional”. Also we recover the continuous function \\(y(t)\\). Example: . \\[f(x) = \\frac{1}{N}\\mathbf{x}^T \\mathbf{x} = \\sum_{n=1}^N \\frac{1}{N} y_n^2\\] Functions of functions are called functionals. This is a cost functional. \\(\\begin{equation*} \\min_x f(x) = \\frac{1}{N} \\mathbf{x}^T \\mathbf{x} = F(y(t)) = \\int_0^1 y(t)^2 \\space dt \\end{equation*}\\) . Normally what we do is… . \\[\\begin{align*} \\frac{\\partial f}{\\partial x} = \\frac{2}{N}x^T = 0 \\\\ x^* = 0 \\end{align*}\\] We can look at this in components. We have to use the first-order approximation (linearization). \\[\\begin{align*} \\min_x \\sum_{n=1}^N \\frac{1}{N} y_n^2 \\\\ F(y(t) + \\Delta y(t)) &amp;\\approx F(y) + \\frac{\\partial F}{\\partial y}(\\Delta y) \\\\ &amp;= \\int_0^1 y(t)^2 \\space dt + \\int_0^1 2y(t) \\Delta y(t) \\space dt \\end{align*}\\] At a minimum, the first-order term must be zero for all \\(\\Delta y(t)\\). This implies that \\(y(t) = 0\\). ",
    "url": "/blog/prereq-notes/calc-variation.html#calculus",
    
    "relUrl": "/prereq-notes/calc-variation.html#calculus"
  },"48": {
    "doc": "Calculus of Variations - Chill",
    "title": "Standard Notation for \\(Delta\\)",
    "content": "A variation of a functional \\(\\delta F\\) is … the first-order term in the Taylor expansion of \\(F(y + \\delta y)\\) around \\(y\\). \\[\\begin{align*} \\delta F = \\int_0^1 2y(t) \\delta y(t) \\space dt \\end{align*}\\] It is not common to write \\(\\frac{\\delta F}{\\delta y}\\) to denote the functional derivative. ",
    "url": "/blog/prereq-notes/calc-variation.html#standard-notation-for-delta",
    
    "relUrl": "/prereq-notes/calc-variation.html#standard-notation-for-delta"
  },"49": {
    "doc": "Calculus of Variations - Chill",
    "title": "More interesting example (catenary)",
    "content": ". | g: acceleration due to gravity | \\(\\Delta s_n\\): incremental cable length. | . Total cable length is \\(\\ell\\): \\(\\begin{align*} \\ell = \\sum_{n=1}^{N-1} \\Delta s_n = \\sum_{n=1}^{N-1} \\sqrt{\\Delta x_n^2 + \\Delta y_n^2} = \\sum_{n=1}^{N-1} \\sqrt{1 + \\left(\\frac{\\Delta y_n}{\\Delta x_n}\\right)^2} \\Delta x_n \\end{align*}\\) . Total mass: . \\[\\begin{align*} m = \\sum_{n=1}^{N-1} \\rho \\Delta s_n \\\\ \\rho = \\frac{m}{\\ell} \\end{align*}\\] If we minimize the potential energy, we get the shape of the cable. \\[\\begin{align*} V &amp;= \\sum_{n=1}^{N-1} \\rho \\Delta s_n g(\\frac{y_{n+1} + y_n}{2}) \\\\ &amp;= \\sum_{n=1}^{N-1} \\rho g \\frac{y_{n+1} + y_n}{2} \\Delta x \\sqrt{1 + \\left(\\frac{\\Delta y_n}{\\Delta x_n}\\right)^2} \\end{align*}\\] Optimization will have a constraint: . \\[\\begin{align*} \\min_{y_{1:N-1}} \\sum_{n=1}^{N-1} \\rho g \\frac{y_{n+1} + y_n}{2} \\Delta x \\sqrt{1 + \\left(\\frac{\\Delta y_n}{\\Delta x_n}\\right)^2} \\\\ \\text{s.t. } \\sum_{n=1}^{N-1} \\Delta s_n = \\ell \\end{align*}\\] NOTE that \\(y_0, y_N\\) are fixed (boundary conditions). Generic version: . \\[\\begin{align*} \\min_x \\sum_{n=1}^{N-1} f(y_n, y_{n+1}) \\\\ \\text{s.t. } \\sum_{n=1}^{N-1} c(y_n) = 0 \\\\ L(x,\\lambda) = \\sum_{n=1}^{N-1} f(y_1, ..., y_{n-1}) + \\lambda c(y_n) \\end{align*}\\] Last equation is the Lagrangian. KKT Conditions: . \\[\\begin{align*} \\frac{\\partial L}{\\partial \\delta x} &amp;= \\sum_{n=1}^{N-1} D_1 f(y_n, y_{n+1}) \\Delta y_n \\\\ &amp;+ D_2 f(y_n, y_{n+1}) \\Delta y_{n+1} \\\\ &amp;+ \\lambda D c(y_n) \\Delta y_n \\\\ &amp;= 0 \\space \\forall \\Delta x \\\\ \\frac{\\partial L}{\\partial \\lambda} &amp;= c(x) \\\\ &amp;= 0 \\end{align*}\\] If we assume endpoints are fixed then we will get for our first-order necessary condition: . \\[\\begin{align*} \\frac{\\partial L}{\\partial \\delta x} &amp;= \\sum_{n=1}^{N-1} D_2 f(y_{n-1}, y_n) \\Delta y_n + D_1 f(y_n, y_{n+1}) \\Delta y_n + \\lambda D c(y_n) \\Delta y_n \\\\ &amp;= (D_2 f(y_{n-1}, y_n) + D_1 f(y_n, y_{n+1}) + \\lambda D c(y_n)) \\Delta y_n &amp;= 0 \\end{align*}\\] The punch line is… . \\[\\begin{align*} D_2 f(y_{n-1}, y_n) + D_1 f(y_n, y_{n+1}) + \\lambda D c(y_n) = 0 \\end{align*}\\] Now we can look at the limit \\(N \\to \\infty\\). We get… . \\[\\begin{align*} \\min_{y(t)} \\int_0^T f(y(t), \\dot y(t)) \\space dt \\\\ \\text{s.t. } \\int_0^T s(y(t)) \\space dt = \\ell \\\\ L(y(t), \\lambda) = \\int_0^T f(y(t), \\dot y(t)) + \\lambda s(y(t)) \\space dt \\end{align*}\\] KKT Conditions in Variational Calculus: . \\[\\begin{align*} \\frac{\\partial L}{\\partial y(t)}[\\Delta y(t)] &amp;= \\int_0^T D_1 f(y(t), \\dot y(t)) \\Delta y(t) \\\\ &amp;+ D_2 f(y(t), \\dot y(t)) \\Delta \\dot y(t) \\\\ &amp;+ \\lambda D s(y(t)) \\Delta y(t) \\space dt \\\\ &amp;= 0 \\space \\forall \\Delta y(t) \\end{align*}\\] \\[\\begin{align*} \\frac{\\partial L}{\\partial \\lambda}[\\Delta y] = \\int_0^T c(y(t)) \\space dt = 0 \\end{align*}\\] Similar to discrete case, we want to factor out \\(\\Delta y(t)\\) to derive local optimality conditions. The key move is integration by parts which is the common trick for variational calculus. \\[\\begin{align*} \\frac{d}{dt}(u(t) \\star v(t)) &amp;= \\dot u(t) \\star v(t) + u(t) \\star \\dot v(t) \\\\ u(t) \\star v(t) \\Big|_0^T &amp;= \\int_0^T \\dot u(t) \\star v(t) \\space dt + \\int_0^T u(t) \\star \\dot v(t) \\space dt \\\\ \\int_0^T u(t) \\star \\dot v(t) \\space dt &amp;= u(t) \\star v(t) \\Big|_0^T - \\int_0^T \\dot u(t) \\star v(t) \\space dt \\end{align*}\\] ",
    "url": "/blog/prereq-notes/calc-variation.html#more-interesting-example-catenary",
    
    "relUrl": "/prereq-notes/calc-variation.html#more-interesting-example-catenary"
  },"50": {
    "doc": "Calculus of Variations - Chill",
    "title": "Calculus of Variations - Chill",
    "content": " ",
    "url": "/blog/prereq-notes/calc-variation.html",
    
    "relUrl": "/prereq-notes/calc-variation.html"
  },"51": {
    "doc": "Calculus of Variations - Serious",
    "title": "Calculus of Variations - Serious",
    "content": "I yoinked these notes from Brian Keng wh ofollows Svetitsky’s Notes on Functionals. This is more true to the actual calculus of variations math. Zack Manchester’s notes were very layman and described the methodology of using calculus of variations in ways we could understand. I now want to understand the deeper math and terminology. ## . ",
    "url": "/blog/prereq-notes/calc-variation2.html",
    
    "relUrl": "/prereq-notes/calc-variation2.html"
  },"52": {
    "doc": "Code",
    "title": "GROOT Dataset Format",
    "content": "Processing a single hdf5 file with the following structure: . { \"data\": { \"demo_0\": { \"actions\": (T, A), \"obs\": { \"agentview_rgb\": (T, H, W, 3), \"agentview_depth\": (T, H, W), \"agentview_masks\": (T, H1, W1), \"agentview_extrinsics\": (T, 4, 4), \"ee_states\": (T, 16), \"gripper_states\": (T, 1) } } } } . This is how to access the code. I put it in the form of print statements: . with h5py.File(example_demo_file) as f: print(f'f[\"data\"]: {f[\"data\"]}') print(f'f[\"data\"].keys(): {f[\"data\"].keys()}') print(f'f[\"data/demo_0\"].keys(): {f[\"data/demo_0\"].keys()}') print(f'f[\"data/demo_0/actions\"].shape (T,A): {np.array(f[\"data/demo_0/actions\"]).shape}') print(f'f[\"data/demo_0/obs\"].keys(): {f[\"data/demo_0/obs\"].keys()}') print(f'f[\"data/demo_0/obs\"][\"agentview_rgb\"].shape (T, H, W, 3): {f[\"data/demo_0/obs\"][\"agentview_rgb\"].shape}') print(f'f[\"data/demo_0/obs\"][\"agentview_depth\"].shape (T, H, W): {f[\"data/demo_0/obs\"][\"agentview_depth\"].shape}') print(f'f[\"data/demo_0/obs\"][\"agentview_masks\"].shape (T, H1, W1): {f[\"data/demo_0/obs\"][\"agentview_masks\"].shape}') print(f'f[\"data/demo_0/obs\"][\"agentview_extrinsics\"].shape (T, 4, 4): {f[\"data/demo_0/obs\"][\"agentview_extrinsics\"].shape}') print(f'f[\"data/demo_0/obs\"][\"ee_states\"].shape (T, 16): {f[\"data/demo_0/obs\"][\"ee_states\"].shape}') print(f'f[\"data/demo_0/obs\"][\"gripper_states\"].shape (T, 1): {f[\"data/demo_0/obs\"][\"gripper_states\"].shape}') . Code output: . f[\"data\"]: &lt;HDF5 group \"/data\" (1 members)&gt; f[\"data\"].keys(): &lt;KeysViewHDF5 ['demo_0']&gt; f[\"data/demo_0\"].keys(): &lt;KeysViewHDF5 ['actions', 'obs']&gt; f[\"data/demo_0/actions\"].shape (T,A): (165, 7) f[\"data/demo_0/obs\"].keys(): &lt;KeysViewHDF5 ['agentview_depth', 'agentview_extrinsics', 'agentview_masks', 'agentview_rgb', 'ee_states', 'gripper_states', 'joint_states']&gt; f[\"data/demo_0/obs\"][\"agentview_rgb\"].shape (T, H, W, 3): (165, 224, 224, 3) f[\"data/demo_0/obs\"][\"agentview_depth\"].shape (T, H, W): (165, 224, 224) f[\"data/demo_0/obs\"][\"agentview_masks\"].shape (T, H1, W1): (165, 480, 480) f[\"data/demo_0/obs\"][\"agentview_extrinsics\"].shape (T, 4, 4): (165, 4, 4) f[\"data/demo_0/obs\"][\"ee_states\"].shape (T, 16): (165, 16) f[\"data/demo_0/obs\"][\"gripper_states\"].shape (T, 1): (165, 1) . ",
    "url": "/blog/code-notes/groot/code.html#groot-dataset-format",
    
    "relUrl": "/code-notes/groot/code.html#groot-dataset-format"
  },"53": {
    "doc": "Code",
    "title": "LIBERO Dataset Format",
    "content": "Processing a single hdf5 file with the following structure: The difference we notice is that in LIBERO, it is missing the following observation keys: . | agentview_depth (normalized depth data, can get from env) NOTE: GROOT O3DPointCloud will handle unnormalization | agentview_extrinsics (camera extrinsic matrix, can get from env) | agentview_masks (segmentation masks of objects, must use XMem + Annotation, last mask is the robot) | . Additionally, demos here have more stuff to work with: . | rewards (for RL) | robot_states | states (used to replay benchmark and get other modalities if we need) | . { \"data\": { \"demo_0\": { \"actions\": (T, A), \"obs\": { \"agentview_rgb\": (T, H, W, 3), \"ee_states\": (T, 16), \"gripper_states\": (T, 1) } } } } . with h5py.File(demo_file, 'r') as f: print(f'f[\"data\"]: {f[\"data\"]}') print(f'f[\"data\"].keys(): {f[\"data\"].keys()}') print(f'f[\"data/demo_0\"].keys(): {f[\"data/demo_0\"].keys()}') print(f'f[\"data/demo_0/actions\"].shape (T,A): {np.array(f[\"data/demo_0/actions\"]).shape}') print(f'f[\"data/demo_0/obs\"].keys(): {f[\"data/demo_0/obs\"].keys()}') print(f'f[\"data/demo_0/obs\"][\"agentview_rgb\"].shape (T, H, W, 3): {f[\"data/demo_0/obs\"][\"agentview_rgb\"].shape}') print(f'f[\"data/demo_0/obs\"][\"ee_states\"].shape (T, 16): {f[\"data/demo_0/obs\"][\"ee_states\"].shape}') print(f'f[\"data/demo_0/obs\"][\"gripper_states\"].shape (T, 1): {f[\"data/demo_0/obs\"][\"gripper_states\"].shape}') . Code output: . f[\"data\"]: &lt;HDF5 group \"/data\" (50 members)&gt; f[\"data\"].keys(): &lt;KeysViewHDF5 ['demo_0', 'demo_1', 'demo_10', 'demo_11', 'demo_12', 'demo_13', 'demo_14', 'demo_15', 'demo_16', 'demo_17', 'demo_18', 'demo_19', 'demo_2', 'demo_20', 'demo_21', 'demo_22', 'demo_23', 'demo_24', 'demo_25', 'demo_26', 'demo_27', 'demo_28', 'demo_29', 'demo_3', 'demo_30', 'demo_31', 'demo_32', 'demo_33', 'demo_34', 'demo_35', 'demo_36', 'demo_37', 'demo_38', 'demo_39', 'demo_4', 'demo_40', 'demo_41', 'demo_42', 'demo_43', 'demo_44', 'demo_45', 'demo_46', 'demo_47', 'demo_48', 'demo_49', 'demo_5', 'demo_6', 'demo_7', 'demo_8', 'demo_9']&gt; f[\"data/demo_0\"].keys(): &lt;KeysViewHDF5 ['actions', 'dones', 'obs', 'rewards', 'robot_states', 'states']&gt; f[\"data/demo_0/actions\"].shape (T,A): (148, 7) f[\"data/demo_0/obs\"].keys(): &lt;KeysViewHDF5 ['agentview_rgb', 'ee_ori', 'ee_pos', 'ee_states', 'eye_in_hand_rgb', 'gripper_states', 'joint_states']&gt; f[\"data/demo_0/obs\"][\"agentview_rgb\"].shape (T, H, W, 3): (148, 128, 128, 3) f[\"data/demo_0/obs\"][\"ee_states\"].shape (T, 16): (148, 6) f[\"data/demo_0/obs\"][\"gripper_states\"].shape (T, 1): (148, 2) . ",
    "url": "/blog/code-notes/groot/code.html#libero-dataset-format",
    
    "relUrl": "/code-notes/groot/code.html#libero-dataset-format"
  },"54": {
    "doc": "Code",
    "title": "Dataset Format Comparison:",
    "content": "With this comparison, it is clear we need to need to re-render LIBERO and get the following observations: . | agentview_depth (unnormalized depth) | agentview_masks (segmentation masks) | . ",
    "url": "/blog/code-notes/groot/code.html#dataset-format-comparison",
    
    "relUrl": "/code-notes/groot/code.html#dataset-format-comparison"
  },"55": {
    "doc": "Code",
    "title": "Use annotation to get masks",
    "content": ". Afterwards, we use the annotations and XMem to get agentview_masks. This allows us to track any object in the scene. ",
    "url": "/blog/code-notes/groot/code.html#use-annotation-to-get-masks",
    
    "relUrl": "/code-notes/groot/code.html#use-annotation-to-get-masks"
  },"56": {
    "doc": "Code",
    "title": "Training GROOT",
    "content": "Preparing the GROOT dataset requires the use of Point-MAE, which is completely incompatible with our current anaconda environment. Even using Point-MAE requires really old versioning. This is fucked up. Current conflicts . | Point-MAE_ops requires torch to use CUDA 12.4 (system cuda), DinoV2 requires torch to use CUDA 11.7 | We can’t use CUDA 11.7 because desktop is on CUDA 12.4 | . ",
    "url": "/blog/code-notes/groot/code.html#training-groot",
    
    "relUrl": "/code-notes/groot/code.html#training-groot"
  },"57": {
    "doc": "Code",
    "title": "Code",
    "content": " ",
    "url": "/blog/code-notes/groot/code.html",
    
    "relUrl": "/code-notes/groot/code.html"
  },"58": {
    "doc": "Code",
    "title": "LIBERO Code Notes",
    "content": "NOTE : LIBERO has no pretrained weights or checkpoints, so keep in mind I’ll have to retrain some of these architectures from scratch. I am going to mainly use LIBERO in this codebase, so it is important to understand not only how to use it, but also all the libraries that it uses and how they work: . Current libraries used by LIBERO (that I’m not solid on): . | Robosuite | Hydra | WandB | easydict | transformers | einops | thop | bddl | . ",
    "url": "/blog/code-notes/benchmark-notes/libero/code.html#libero-code-notes",
    
    "relUrl": "/code-notes/benchmark-notes/libero/code.html#libero-code-notes"
  },"59": {
    "doc": "Code",
    "title": "Using LIBERO for training a policy",
    "content": "1. Create Config . First we use hydra to interface with our configs. import hydra from hydra import compose, initialize from omegaconf import OmegaConf import yaml from easydict import EasyDict # load default hydra config hydra.core.global_hydra.GlobalHydra.instance().clear() initialize(config_path='../libero/configs') hydra_cfg = compose(config_name=\"config\") yaml_config = OmegaConf.to_yaml(hydra_cfg) cfg = EasyDict(yaml.safe_load(yaml_config)) . # LIBERO/libero/configs/config.yaml # @package _global_ defaults: - _self_ - data: default - policy: bc_transformer_policy - train: default - eval: default - lifelong: base - test: null seed: 10000 use_wandb: false wandb_project: \"lifelong learning\" folder: null # use default path bddl_folder: null # use default path init_states_folder: null # use default path load_previous_model: false device: \"cuda\" task_embedding_format: \"bert\" task_embedding_one_hot_offset: 1 pretrain: false pretrain_model_path: \"\" benchmark_name: \"LIBERO_SPATIAL\" . This config file is actually the only config that will be processed in the codebase. In order to use other models, you must modify this config file. Kinda sucks, but that’s how it is. 2. Setup config . After creating the config, we need to continue doing some more configurations before training. from libero.libero import get_libero_path cfg.folder = get_libero_path(\"datasets\") cfg.bddl_folder = get_libero_path(\"bddl_files\") cfg.init_states_folder = get_libero_path(\"init_states\") cfg.eval.num_procs = 1 cfg.eval.n_eval = 5 cfg.train.n_epochs = 25 task_order = cfg.data.task_order_index # can be from {0 .. 21}, default to 0, . # LIBERO/libero/__init__.py libero_config_path = os.environ.get( \"LIBERO_CONFIG_PATH\", os.path.expanduser(\"~/.libero\") ) config_file = os.path.join(libero_config_path, \"config.yaml\") def get_libero_path(query_key): with open(config_file, \"r\") as f: config = dict(yaml.load(f.read(), Loader=yaml.FullLoader)) # Give warnings in case the user needs to access the paths for key in config: if not os.path.exists(config[key]): print(f\"[Warning]: {key} path {config[key]} does not exist!\") assert ( query_key in config ), f\"Key {query_key} not found in config file {config_file}. You need to modify it. Available keys are: {config.keys()}\" return config[query_key] . 3. Create Benchmark Object . In order to access datasets and benchmark related information, we need to create this benchmark object. from libero.libero import benchmark # stupid way of initializing benchmark/__init__.py from libero.libero.benchmark import get_benchmark benchmark = get_benchmark(cfg.benchmark_name)(task_order) . # LIBERO/libero/benchmark/__init__.py import abc BENCHMARK_MAPPING = {} def get_benchmark(benchmark_name): return BENCHMARK_MAPPING[benchmark_name.lower()] ## AN NOTE: this is a global dict mapping strings (as names) to Benchmark class ## which just gives relevant meta-info on the benchmark class Benchmark(abc.ABC): \"\"\"A Benchmark.\"\"\" def __init__(self, task_order_index=0): self.task_embs = None self.task_order_index = task_order_index def _make_benchmark(self): tasks = list(task_maps[self.name].values()) if self.name == \"libero_90\": self.tasks = tasks else: print(f\"[info] using task orders {task_orders[self.task_order_index]}\") self.tasks = [tasks[i] for i in task_orders[self.task_order_index]] self.n_tasks = len(self.tasks) def get_num_tasks(self): return self.n_tasks def get_task_names(self): return [task.name for task in self.tasks] def get_task_problems(self): return [task.problem for task in self.tasks] def get_task_bddl_files(self): return [task.bddl_file for task in self.tasks] def get_task_bddl_file_path(self, i): bddl_file_path = os.path.join( get_libero_path(\"bddl_files\"), self.tasks[i].problem_folder, self.tasks[i].bddl_file, ) return bddl_file_path def get_task_demonstration(self, i): assert ( 0 &lt;= i and i &lt; self.n_tasks ), f\"[error] task number {i} is outer of range {self.n_tasks}\" # this path is relative to the datasets folder demo_path = f\"{self.tasks[i].problem_folder}/{self.tasks[i].name}_demo.hdf5\" return demo_path def get_task(self, i): return self.tasks[i] def get_task_emb(self, i): return self.task_embs[i] def get_task_init_states(self, i): init_states_path = os.path.join( get_libero_path(\"init_states\"), self.tasks[i].problem_folder, self.tasks[i].init_states_file, ) init_states = torch.load(init_states_path) return init_states def set_task_embs(self, task_embs): self.task_embs = task_embs . 4. Retrieve datasets . Now that we hae the benchmark object, we can utilize its information along with the config to retrieve task datasets. For LIBERO, they use task embeddings to represent each task. This is done by using a pretrained model (e.g., BERT) to encode the task descriptions. The task embeddings are then used to condition the policy during training and evaluation. We can see it here through the get_task_embs function. from libero.lifelong.datasets import SequenceVLDataset, get_dataset from libero.lifelong.utils import get_task_embs # prepare datasets from the benchmark datasets = [] descriptions = [] shape_meta = None n_tasks = benchmark.n_tasks ### AN NOTE: loop through all tasks in benchmark and retrieve their datasets for i in range(n_tasks): # currently we assume tasks from same benchmark have the same shape_meta task_i_dataset, shape_meta = get_dataset( dataset_path=os.path.join(cfg.folder, benchmark.get_task_demonstration(i)), obs_modality=cfg.data.obs.modality, initialize_obs_utils=(i==0), seq_len=cfg.data.seq_len, ) # add language to the vision dataset, hence we call vl_dataset descriptions.append(benchmark.get_task(i).language) datasets.append(task_i_dataset) ### AN NOTE: collect all datasets into a single object task_embs = get_task_embs(cfg, descriptions) benchmark.set_task_embs(task_embs) datasets = [SequenceVLDataset(ds, emb) for (ds, emb) in zip(datasets, task_embs)] n_demos = [data.n_demos for data in datasets] n_sequences = [data.total_num_sequences for data in datasets] . # LIBERO/libero/lifelong/datasets.py def get_dataset( dataset_path, obs_modality, initialize_obs_utils=True, seq_len=1, frame_stack=1, filter_key=None, hdf5_cache_mode=\"low_dim\", *args, **kwargs ): ... # AN REDACTED return dataset, shape_meta class SequenceVLDataset(Dataset): def __init__(self, sequence_dataset, task_emb): self.sequence_dataset = sequence_dataset self.task_emb = task_emb self.n_demos = self.sequence_dataset.n_demos self.total_num_sequences = self.sequence_dataset.total_num_sequences def __len__(self): return len(self.sequence_dataset) def __getitem__(self, idx): return_dict = self.sequence_dataset.__getitem__(idx) return_dict[\"task_emb\"] = self.task_emb return return_dict . # LIBERO/libero/lifelong/utils.py def get_task_embs(cfg, descriptions): logging.set_verbosity_error() if cfg.task_embedding_format == \"one-hot\": # offset defaults to 1, if we have pretrained another model, this offset # starts from the pretrained number of tasks + 1 offset = cfg.task_embedding_one_hot_offset descriptions = [f\"Task {i+offset}\" for i in range(len(descriptions))] if cfg.task_embedding_format == \"bert\" or cfg.task_embedding_format == \"one-hot\": tz = AutoTokenizer.from_pretrained( \"bert-base-cased\", cache_dir=to_absolute_path(\"./bert\") ) model = AutoModel.from_pretrained( \"bert-base-cased\", cache_dir=to_absolute_path(\"./bert\") ) tokens = tz( text=descriptions, # the sentence to be encoded add_special_tokens=True, # Add [CLS] and [SEP] max_length=cfg.data.max_word_len, # maximum length of a sentence padding=\"max_length\", return_attention_mask=True, # Generate the attention mask return_tensors=\"pt\", # ask the function to return PyTorch tensors ) masks = tokens[\"attention_mask\"] input_ids = tokens[\"input_ids\"] task_embs = model(tokens[\"input_ids\"], tokens[\"attention_mask\"])[ \"pooler_output\" ].detach() elif cfg.task_embedding_format == \"gpt2\": tz = AutoTokenizer.from_pretrained(\"gpt2\") tz.pad_token = tz.eos_token model = AutoModel.from_pretrained(\"gpt2\") tokens = tz( text=descriptions, # the sentence to be encoded add_special_tokens=True, # Add [CLS] and [SEP] max_length=cfg.data.max_word_len, # maximum length of a sentence padding=\"max_length\", return_attention_mask=True, # Generate the attention mask return_tensors=\"pt\", # ask the function to return PyTorch tensors ) task_embs = model(**tokens)[\"last_hidden_state\"].detach()[:, -1] elif cfg.task_embedding_format == \"clip\": tz = AutoTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\") model = AutoModel.from_pretrained(\"openai/clip-vit-base-patch32\") tokens = tz( text=descriptions, # the sentence to be encoded add_special_tokens=True, # Add [CLS] and [SEP] max_length=cfg.data.max_word_len, # maximum length of a sentence padding=\"max_length\", return_attention_mask=True, # Generate the attention mask return_tensors=\"pt\", # ask the function to return PyTorch tensors ) task_embs = model.get_text_features(**tokens).detach() elif cfg.task_embedding_format == \"roberta\": tz = AutoTokenizer.from_pretrained(\"roberta-base\") tz.pad_token = tz.eos_token model = AutoModel.from_pretrained(\"roberta-base\") tokens = tz( text=descriptions, # the sentence to be encoded add_special_tokens=True, # Add [CLS] and [SEP] max_length=cfg.data.max_word_len, # maximum length of a sentence padding=\"max_length\", return_attention_mask=True, # Generate the attention mask return_tensors=\"pt\", # ask the function to return PyTorch tensors ) task_embs = model(**tokens)[\"pooler_output\"].detach() cfg.policy.language_encoder.network_kwargs.input_size = task_embs.shape[-1] return task_embs . 5. Writing Lifelong Policy . We use observation head from the codebase only for rgb_modules and language_modules. The example policy implementation shows how a typical lifelong-learning algorithm is trained. import robomimic.utils.tensor_utils as TensorUtils import torch import torch.nn as nn # AN NOTE: these are relevant parts to import for policies from libero.lifelong.models.modules.rgb_modules import * from libero.lifelong.models.modules.language_modules import * from libero.lifelong.models.base_policy import BasePolicy from libero.lifelong.models.policy_head import * from libero.lifelong.models.modules.transformer_modules import * class ExtraModalityTokens(nn.Module): def __init__( self, use_joint=False, use_gripper=False, use_ee=False, extra_num_layers=0, extra_hidden_size=64, extra_embedding_size=32, ): \"\"\" This is a class that maps all extra modality inputs into tokens of the same size \"\"\" super().__init__() self.use_joint = use_joint self.use_gripper = use_gripper self.use_ee = use_ee self.extra_embedding_size = extra_embedding_size joint_states_dim = 7 gripper_states_dim = 2 ee_dim = 3 self.num_extra = int(use_joint) + int(use_gripper) + int(use_ee) extra_low_level_feature_dim = ( int(use_joint) * joint_states_dim + int(use_gripper) * gripper_states_dim + int(use_ee) * ee_dim ) assert extra_low_level_feature_dim &gt; 0, \"[error] no extra information\" self.extra_encoders = {} def generate_proprio_mlp_fn(modality_name, extra_low_level_feature_dim): assert extra_low_level_feature_dim &gt; 0 # we indeed have extra information if extra_num_layers &gt; 0: layers = [nn.Linear(extra_low_level_feature_dim, extra_hidden_size)] for i in range(1, extra_num_layers): layers += [ nn.Linear(extra_hidden_size, extra_hidden_size), nn.ReLU(inplace=True), ] layers += [nn.Linear(extra_hidden_size, extra_embedding_size)] else: layers = [nn.Linear(extra_low_level_feature_dim, extra_embedding_size)] self.proprio_mlp = nn.Sequential(*layers) self.extra_encoders[modality_name] = {\"encoder\": self.proprio_mlp} for (proprio_dim, use_modality, modality_name) in [ (joint_states_dim, self.use_joint, \"joint_states\"), (gripper_states_dim, self.use_gripper, \"gripper_states\"), (ee_dim, self.use_ee, \"ee_states\"), ]: if use_modality: generate_proprio_mlp_fn(modality_name, proprio_dim) self.encoders = nn.ModuleList( [x[\"encoder\"] for x in self.extra_encoders.values()] ) def forward(self, obs_dict): \"\"\" obs_dict: { (optional) joint_stats: (B, T, 7), (optional) gripper_states: (B, T, 2), (optional) ee: (B, T, 3) } map above to a latent vector of shape (B, T, H) \"\"\" tensor_list = [] for (use_modality, modality_name) in [ (self.use_joint, \"joint_states\"), (self.use_gripper, \"gripper_states\"), (self.use_ee, \"ee_states\"), ]: if use_modality: tensor_list.append( self.extra_encoders[modality_name][\"encoder\"]( obs_dict[modality_name] ) ) x = torch.stack(tensor_list, dim=-2) return x class MyTransformerPolicy(BasePolicy): \"\"\" Input: (o_{t-H}, ... , o_t) Output: a_t or distribution of a_t \"\"\" def __init__(self, cfg, shape_meta): super().__init__(cfg, shape_meta) policy_cfg = cfg.policy ### 1. encode image embed_size = policy_cfg.embed_size transformer_input_sizes = [] self.image_encoders = {} for name in shape_meta[\"all_shapes\"].keys(): if \"rgb\" in name or \"depth\" in name: kwargs = policy_cfg.image_encoder.network_kwargs kwargs.input_shape = shape_meta[\"all_shapes\"][name] kwargs.output_size = embed_size kwargs.language_dim = ( policy_cfg.language_encoder.network_kwargs.input_size ) self.image_encoders[name] = { \"input_shape\": shape_meta[\"all_shapes\"][name], \"encoder\": eval(policy_cfg.image_encoder.network)(**kwargs), } self.encoders = nn.ModuleList( [x[\"encoder\"] for x in self.image_encoders.values()] ) ### 2. encode language policy_cfg.language_encoder.network_kwargs.output_size = embed_size self.language_encoder = eval(policy_cfg.language_encoder.network)( **policy_cfg.language_encoder.network_kwargs ) ### 3. encode extra information (e.g. gripper, joint_state) self.extra_encoder = ExtraModalityTokens( use_joint=cfg.data.use_joint, use_gripper=cfg.data.use_gripper, use_ee=cfg.data.use_ee, extra_num_layers=policy_cfg.extra_num_layers, extra_hidden_size=policy_cfg.extra_hidden_size, extra_embedding_size=embed_size, ) ### 4. define temporal transformer policy_cfg.temporal_position_encoding.network_kwargs.input_size = embed_size self.temporal_position_encoding_fn = eval( policy_cfg.temporal_position_encoding.network )(**policy_cfg.temporal_position_encoding.network_kwargs) self.temporal_transformer = TransformerDecoder( input_size=embed_size, num_layers=policy_cfg.transformer_num_layers, num_heads=policy_cfg.transformer_num_heads, head_output_size=policy_cfg.transformer_head_output_size, mlp_hidden_size=policy_cfg.transformer_mlp_hidden_size, dropout=policy_cfg.transformer_dropout, ) policy_head_kwargs = policy_cfg.policy_head.network_kwargs policy_head_kwargs.input_size = embed_size policy_head_kwargs.output_size = shape_meta[\"ac_dim\"] self.policy_head = eval(policy_cfg.policy_head.network)( **policy_cfg.policy_head.loss_kwargs, **policy_cfg.policy_head.network_kwargs ) self.latent_queue = [] self.max_seq_len = policy_cfg.transformer_max_seq_len def temporal_encode(self, x): pos_emb = self.temporal_position_encoding_fn(x) x = x + pos_emb.unsqueeze(1) # (B, T, num_modality, E) sh = x.shape self.temporal_transformer.compute_mask(x.shape) x = TensorUtils.join_dimensions(x, 1, 2) # (B, T*num_modality, E) x = self.temporal_transformer(x) x = x.reshape(*sh) return x[:, :, 0] # (B, T, E) def spatial_encode(self, data): # 1. encode extra extra = self.extra_encoder(data[\"obs\"]) # (B, T, num_extra, E) # 2. encode language, treat it as action token B, T = extra.shape[:2] text_encoded = self.language_encoder(data) # (B, E) text_encoded = text_encoded.view(B, 1, 1, -1).expand( -1, T, -1, -1 ) # (B, T, 1, E) encoded = [text_encoded, extra] # 3. encode image for img_name in self.image_encoders.keys(): x = data[\"obs\"][img_name] B, T, C, H, W = x.shape img_encoded = self.image_encoders[img_name][\"encoder\"]( x.reshape(B * T, C, H, W), langs=data[\"task_emb\"] .reshape(B, 1, -1) .repeat(1, T, 1) .reshape(B * T, -1), ).view(B, T, 1, -1) encoded.append(img_encoded) encoded = torch.cat(encoded, -2) # (B, T, num_modalities, E) return encoded def forward(self, data): x = self.spatial_encode(data) x = self.temporal_encode(x) dist = self.policy_head(x) return dist def get_action(self, data): self.eval() with torch.no_grad(): data = self.preprocess_input(data, train_mode=False) x = self.spatial_encode(data) self.latent_queue.append(x) if len(self.latent_queue) &gt; self.max_seq_len: self.latent_queue.pop(0) x = torch.cat(self.latent_queue, dim=1) # (B, T, H_all) x = self.temporal_encode(x) dist = self.policy_head(x[:, -1]) action = dist.sample().detach().cpu() return action.view(action.shape[0], -1).numpy() def reset(self): self.latent_queue = [] . 6. Lifelong Data Structure . We need to wrap our code around a data structure that uses proper training BC handling such as experience replay. I forgot this existed, so I will write a page on this later as this may have been the source of my troubles. from libero.lifelong.algos.base import Sequential ### All lifelong learning algorithm should inherit the Sequential algorithm super class class MyLifelongAlgo(Sequential): \"\"\" The experience replay policy. \"\"\" def __init__(self, n_tasks, cfg, **policy_kwargs): super().__init__(n_tasks=n_tasks, cfg=cfg, **policy_kwargs) # define the learning policy self.datasets = [] self.policy = eval(cfg.policy.policy_type)(cfg, cfg.shape_meta) def start_task(self, task): # what to do at the beginning of a new task super().start_task(task) def end_task(self, dataset, task_id, benchmark): # what to do when finish learning a new task self.datasets.append(dataset) def observe(self, data): # how the algorithm observes a data and returns a loss to be optimized loss = super().observe(data) return loss . 7. Training Loop . We can finally write our training loop which isn’t so complicated. For my purpose, I wil just keep i = idx and train the agent on one task. import numpy as np from tqdm import trange from libero.lifelong.metric import evaluate_loss, evaluate_success from libero.lifelong.utils import safe_device, create_experiment_dir algo = safe_device(MyLifelongAlgo(n_tasks, cfg), cfg.device) result_summary = { 'L_conf_mat': np.zeros((n_tasks, n_tasks)), # loss confusion matrix 'S_conf_mat': np.zeros((n_tasks, n_tasks)), # success confusion matrix 'L_fwd' : np.zeros((n_tasks,)), # loss AUC, how fast the agent learns 'S_fwd' : np.zeros((n_tasks,)), # success AUC, how fast the agent succeeds } gsz = cfg.data.task_group_size for i in trange(n_tasks): algo.train() s_fwd, l_fwd = algo.learn_one_task(datasets[i], i, benchmark, result_summary) # s_fwd is success rate AUC, when the agent learns the {0, e, 2e, ...} epochs # l_fwd is BC loss AUC, similar to s_fwd result_summary[\"S_fwd\"][i] = s_fwd result_summary[\"L_fwd\"][i] = l_fwd if cfg.eval.eval: algo.eval() # we only evaluate on the past tasks: 0 .. i L = evaluate_loss(cfg, algo, benchmark, datasets[:i+1]) # (i+1,) S = evaluate_success(cfg, algo, benchmark, list(range((i+1)*gsz))) # (i+1,) result_summary[\"L_conf_mat\"][i][:i+1] = L result_summary[\"S_conf_mat\"][i][:i+1] = S torch.save(result_summary, os.path.join(cfg.experiment_dir, f'result.pt')) . 8. Visualize resultss . result_summary = torch.load(os.path.join(cfg.experiment_dir, f'result.pt')) print(result_summary[\"S_conf_mat\"]) print(result_summary[\"S_fwd\"]) import torch import numpy as np from pathlib import Path benchmark_map = { \"libero_10\" : \"LIBERO_10\", \"libero_90\" : \"LIBERO_90\", \"libero_spatial\": \"LIBERO_SPATIAL\", \"libero_object\" : \"LIBERO_OBJECT\", \"libero_goal\" : \"LIBERO_GOAL\", } algo_map = { \"base\" : \"Sequential\", \"er\" : \"ER\", \"ewc\" : \"EWC\", \"packnet\" : \"PackNet\", \"multitask\": \"Multitask\", \"custom_algo\" : \"MyLifelongAlgo\", } policy_map = { \"bc_rnn_policy\" : \"BCRNNPolicy\", \"bc_transformer_policy\": \"BCTransformerPolicy\", \"bc_vilt_policy\" : \"BCViLTPolicy\", \"custom_policy\" : \"MyTransformerPolicy\", } seeds = [10000] N_SEEDS = len(seeds) N_TASKS = 10 def get_auc(experiment_dir, bench, algo, policy): N_EP = cfg.train.n_epochs // cfg.eval.eval_every + 1 fwds = np.zeros((N_TASKS, N_EP, N_SEEDS)) for task in range(N_TASKS): counter = 0 for k, seed in enumerate(seeds): name = f\"{experiment_dir}/task{task}_auc.log\" try: succ = torch.load(name)[\"success\"] # (n_epochs) idx = succ.argmax() succ[idx:] = succ[idx] fwds[task, :, k] = succ except: print(\"Some errors when loading results\") continue return fwds def compute_metric(res): mat, fwts = res # fwds: (num_tasks, num_save_intervals, num_seeds) num_tasks, num_seeds = mat.shape[1:] ret = {} # compute fwt fwt = fwts.mean(axis=(0,1)) ret[\"fwt\"] = fwt # compute bwt bwts = [] aucs = [] for seed in range(num_seeds): bwt = 0.0 auc = 0.0 for k in range(num_tasks): bwt_k = 0.0 auc_k = 0.0 for tau in range(k+1, num_tasks): bwt_k += mat[k,k,seed] - mat[tau,k,seed] auc_k += mat[tau,k,seed] if k + 1 &lt; num_tasks: bwt_k /= (num_tasks - k - 1) auc_k = (auc_k + fwts[k,:,seed].mean()) / (num_tasks - k) bwt += bwt_k auc += auc_k bwts.append(bwt / num_tasks) aucs.append(auc / num_tasks) bwts = np.array(bwts) aucs = np.array(aucs) ret[\"bwt\"] = bwts ret[\"auc\"] = aucs return ret experiment_dir = \"experiments\" benchmark_name = \"libero_object\" algo_name = \"custom_algo\" policy_name = \"custom_policy\" fwds = get_auc(cfg.experiment_dir, benchmark_name, algo_name, policy_name) conf_mat = result_summary[\"S_conf_mat\"][..., np.newaxis] metric = compute_metric((conf_mat, fwds)) print(metric) . 9. Visualize rollouts . This is the kind of stuff we want. Once we train a model, we can visualize the rollouts and see how well it performs. from IPython.display import HTML from base64 import b64encode import imageio from libero.libero.envs import OffScreenRenderEnv, DummyVectorEnv from libero.lifelong.metric import raw_obs_to_tensor_obs # You can turn on subprocess env_num = 1 action_dim = 7 # If it's packnet, the weights need to be processed first task_id = 9 task = benchmark.get_task(task_id) task_emb = benchmark.get_task_emb(task_id) if cfg.lifelong.algo == \"PackNet\": algo = algo.get_eval_algo(task_id) algo.eval() env_args = { \"bddl_file_name\": os.path.join( cfg.bddl_folder, task.problem_folder, task.bddl_file ), \"camera_heights\": cfg.data.img_h, \"camera_widths\": cfg.data.img_w, } env = DummyVectorEnv( [lambda: OffScreenRenderEnv(**env_args) for _ in range(env_num)] ) init_states_path = os.path.join( cfg.init_states_folder, task.problem_folder, task.init_states_file ) init_states = torch.load(init_states_path) env.reset() init_state = init_states[0:1] dones = [False] algo.reset() obs = env.set_init_state(init_state) # Make sure the gripepr is open to make it consistent with the provided demos. dummy_actions = np.zeros((env_num, action_dim)) for _ in range(5): obs, _, _, _ = env.step(dummy_actions) steps = 0 obs_tensors = [[]] * env_num while steps &lt; cfg.eval.max_steps: steps += 1 data = raw_obs_to_tensor_obs(obs, task_emb, cfg) action = algo.policy.get_action(data) obs, reward, done, info = env.step(action) for k in range(env_num): dones[k] = dones[k] or done[k] obs_tensors[k].append(obs[k][\"agentview_image\"]) if all(dones): break # visualize video # obs_tensor: (env_num, T, H, W, C) images = [img[::-1] for img in obs_tensors[0]] fps = 30 writer = imageio.get_writer('tmp_video.mp4', fps=fps) for image in images: writer.append_data(image) writer.close() video_data = open(\"tmp_video.mp4\", \"rb\").read() video_tag = f'&lt;video controls alt=\"test\" src=\"data:video/mp4;base64,{b64encode(video_data).decode()}\"&gt;' HTML(data=video_tag) . ",
    "url": "/blog/code-notes/benchmark-notes/libero/code.html#using-libero-for-training-a-policy",
    
    "relUrl": "/code-notes/benchmark-notes/libero/code.html#using-libero-for-training-a-policy"
  },"60": {
    "doc": "Code",
    "title": "Code",
    "content": " ",
    "url": "/blog/code-notes/benchmark-notes/libero/code.html",
    
    "relUrl": "/code-notes/benchmark-notes/libero/code.html"
  },"61": {
    "doc": "Code",
    "title": "Writing code",
    "content": "So there’s multiple ways we can work with the current FRI connection to the Kuka Sunrise controller. At the end of the day, FRI setup between the computer and controller runs a UDP connection that sends LCM-wrapped packets. While lots of my own personal code uses Drake, you can bypass this completely by sending the LCM messages directly! This means that the only time you’re using drake is to run the network driver (connection) which is completely separate from your own personal code. ",
    "url": "/blog/kuka-notes/code.html#writing-code",
    
    "relUrl": "/kuka-notes/code.html#writing-code"
  },"62": {
    "doc": "Code",
    "title": "Running Network driver",
    "content": "Prerequisites: . | You have bazel! | . $ bazel build $ cd drake-iiwa-driver/bazel-bin/kuka-driver $ ./kuka_driver --fri_port [insert port number here] . If anything weird is going on, just ctrl+c and re-run the driver :). Make sure you are running the KukaFRIPositionController on the Kuka pendant! . ",
    "url": "/blog/kuka-notes/code.html#running-network-driver",
    
    "relUrl": "/kuka-notes/code.html#running-network-driver"
  },"63": {
    "doc": "Code",
    "title": "Using Drake (Python)",
    "content": "We need these python packages . | drake | manipulation | . Technically, we don’t need manipulation package. However, it writes up a nice manipulation station code that allows us to work with the Kuka robot hardware. YAML Configuration . We can write up the hardware configuration below. Here, we can write up the base transform of the Kuka robot, the default joint positions, and which FRI mode to use (position, impedance, torque). Here is an example yaml file . # iiwa_standard_setup.yaml Demo: directives: # Add iiwa - add_model: name: iiwa file: package://drake/manipulation/models/iiwa_description/urdf/iiwa14_polytope_collision.urdf default_joint_positions: iiwa_joint_1: [0] iiwa_joint_2: [0] iiwa_joint_3: [0] iiwa_joint_4: [0] iiwa_joint_5: [0] iiwa_joint_6: [0] iiwa_joint_7: [0] - add_weld: parent: world child: iiwa::base model_drivers: iiwa: !IiwaDriver control_mode: position_only . If we want to interface this in Drake, here’s the code to do so: . from manipulation.station import MakeHardwareStation, MakeHardwareStationInterface, load_scenario from pydrake.all import ( StartMeshcat, ) meshcat = StartMeshcat() scenario = load_scenario(filename=\"iiwa_standard_setup.yaml\", scenario_name=\"Demo\") manipulation_station = MakeHardwareStationInterface( scenario, meshcat=meshcat, # can also be None package_xmls=[package_file] ) # manipulation_station is now a System Block with inputs and outputs # by default, its name is \"iiwa\", # so accessing the ports is \"iiwa.[insert name]\" ''' manipulation_station Inputs: - iiwa.position: float32[7] (joint position targets) - iiwa.feedforward_torque: float32[7] (joint feedforward torques) manipulation_station Outputs: - iiwa.position_measured: float32[7] (joint position measurements) - iiwa.position_commanded: float32[7] (joint positions commanded) For a more extensive list check out ManipulationStation in drake: - https://drake.mit.edu/doxygen_cxx/classdrake_1_1examples_1_1manipulation__station_1_1_manipulation_station.html ''' . # NOTE: re-using variables from above python snippet # NOTE: this is code to make kuka go to a specific joint position from pydrake.all import ( DiagramBuilder, Simulator, ConstantVectorSource, RigidTransform ) import numpy as np DESIRED_JOINT = np.array([0.0, np.pi/6, 0.0, -80*np.pi/180, 0.0, np.pi/6, 0.0]) builder = DiagramBuilder() manipulation_station_block = builder.AddSystem(manipulation_station) desired_joint_block = builder.AddSystem(ConstantVectorSource(DESIRED_JOINT)) builder.Connect( desired_joint_block.get_output_port(), manipulation_station_block.get_input_port(\"iiwa.position\") ) diagram = builder.Build() ''' ASCII Art of current Diagram we made: ------------------------ ------&gt; | manipulation_station | -------&gt; FRI | ------------------------ | | ----------------------- ----------------| desired_joint_block | DESIRED_JOINT ----------------------- ''' simulator = Simulator(diagram) simulator.AdvanceTo(30.0) . In order to do more complicated things in Drake, we need to use more of the drake library and have a deeper understanding of how their Diagrams/Systems work. That’s a lot to ask for the reader right now, so this is the barebones example on how to use this for hardware. ",
    "url": "/blog/kuka-notes/code.html#using-drake-python",
    
    "relUrl": "/kuka-notes/code.html#using-drake-python"
  },"64": {
    "doc": "Code",
    "title": "Using raw LCM",
    "content": "UNDER CONSTRUCTION . ",
    "url": "/blog/kuka-notes/code.html#using-raw-lcm",
    
    "relUrl": "/kuka-notes/code.html#using-raw-lcm"
  },"65": {
    "doc": "Code",
    "title": "Code",
    "content": " ",
    "url": "/blog/kuka-notes/code.html",
    
    "relUrl": "/kuka-notes/code.html"
  },"66": {
    "doc": "Contact Rich Workshop",
    "title": "Contact-Rich Workshop (5/19/2025)",
    "content": " ",
    "url": "/blog/conference-notes/icra-2025/workshop/contact-rich.html#contact-rich-workshop-5192025",
    
    "relUrl": "/conference-notes/icra-2025/workshop/contact-rich.html#contact-rich-workshop-5192025"
  },"67": {
    "doc": "Contact Rich Workshop",
    "title": "Parth Shah (TRI)",
    "content": "Training LBM with Vision Language Action Model (VLA). Can language steer behavior? (RACER) . please decode this 5133733 . They’re collecting data in simulation and transferring training on that. Transferring to real-world. They’re chasing after long-horizon tasks. They claim that their LBM is able to work pretty well. STRONG CLAIM: they claim to be close to solving dexterity. Not too convinced. They need to show more results (especially in-the-wild). ",
    "url": "/blog/conference-notes/icra-2025/workshop/contact-rich.html#parth-shah-tri",
    
    "relUrl": "/conference-notes/icra-2025/workshop/contact-rich.html#parth-shah-tri"
  },"68": {
    "doc": "Contact Rich Workshop",
    "title": "Kris Hauser (UIUC)",
    "content": "Reasoning about “stuffs”. Rigid Object model-based world: . | highly explaniable | precise? | simulation | . Policy Learning . | low complexity | low explainability | . Claim : scene graph nor pixels suffice. Desiderata : . | Fully deformable 3d models | Adaptive to observed behavior | Integration of touch | heterogeneous materials (not just one material per body) | incremental and lifelong mapping? | cross-modal correlation | . Sleeper . ",
    "url": "/blog/conference-notes/icra-2025/workshop/contact-rich.html#kris-hauser-uiuc",
    
    "relUrl": "/conference-notes/icra-2025/workshop/contact-rich.html#kris-hauser-uiuc"
  },"69": {
    "doc": "Contact Rich Workshop",
    "title": "Contact Rich Workshop",
    "content": " ",
    "url": "/blog/conference-notes/icra-2025/workshop/contact-rich.html",
    
    "relUrl": "/conference-notes/icra-2025/workshop/contact-rich.html"
  },"70": {
    "doc": "Scaling GNNs",
    "title": "Scaling GNNs",
    "content": " ",
    "url": "/blog/graph-notes/deep-gnn.html",
    
    "relUrl": "/graph-notes/deep-gnn.html"
  },"71": {
    "doc": "Scaling GNNs",
    "title": "Depth on graphs",
    "content": "Problems specific to graphs . | over-smoothing: node features converge to the same vector | over-squashing: when message-passing fails to propagate information efficiently (bottle-necked) on a graph. | . Regularization techniques to address these depth problems . | DropEdge (edge-wise dropout) | PairNorm (pairwise distance normalization on node features) | NodeNorm (node-wise mean and variance normalization) | . Architectural techniques . | Residual connections (jumping knowledge or affine residual connection) | NOTE: while these residual techniques allow us to train on deeper layers, there is not much significant improvements. | . From the table above, the left column is the technique used on top of a GCN. The individual elements of the table represents the mean accuracy of node classification on a CoAuthor Citation Network. The “+/-“ is the standard deviation. We can also see that the best performance stems from using NodeNorm on a 2-layer GCN. This goes against the intuition that deeper is better even if you could train deeper networks. This goes against the intuition on grid-structured data (images) where deeper architectures have brought breakthroughs in performance. This paper shows that deeper graphs on structured data such as molecules and point clouds https://arxiv.org/pdf/1904.03751 can be beneficial. How come it’s different from graph datasets like Cora, Pubmed, or CoauthorsCS (citation datasets)? . Differences are that the molecule and point cloud graphs are relatively small in graph diameter. That means that going from one node to another takes only a few hops. Another thing to consider is whether a problem will require a short-range or long-range hop information. Some molecular datasets are still tough because they require long-range information (nodes that are far apart need to pass information to each other). Deep GNNs are needed for these datasets too. Instead of running deep networks with shallow filters (low message-passing, more layers stacked), we can do the opposite (high message-passing, shallow layers stacked). This was tested in the SIGN paper (Scalable Inception-like Graph Networks) https://arxiv.org/pdf/2004.11198. ",
    "url": "/blog/graph-notes/deep-gnn.html#depth-on-graphs",
    
    "relUrl": "/graph-notes/deep-gnn.html#depth-on-graphs"
  },"72": {
    "doc": "Scaling GNNs",
    "title": "GCN overview",
    "content": "This is a simple GNN architecture that can work on very large graphs (demonstrated on Twitch follow graph dataset). A GCN message-passing operation on a graph \\(\\mathcal{G} = (\\mathcal{V}, \\mathcal{E})\\) can be written as below: . \\[\\begin{equation} \\mathbf{Y} = \\text{ReLU}(\\mathbf{A}\\mathbf{X}\\mathbf{W}) \\end{equation}\\] Here, \\(\\mathbf{W} \\in \\mathbb{R}^{n_f \\times \\lvert \\mathcal{V} \\rvert}\\) is a learnable weight matrix where \\(n_f\\) is the feature dimension and \\(\\lvert \\mathcal{V} \\rvert\\) is the number of nodes. \\(\\mathbf{X} \\in \\mathbb{R}^{\\lvert \\mathcal{V} \\rvert \\times n_f}\\) is the data matrix. \\(\\mathbf{A} \\in \\mathbb{R}^{n_y \\times \\lvert \\mathcal{V} \\rvert}\\) is a message-passing matrix (normally just adjacency where each row corresponds to a message to pass to 1 node). This matrix can also be weighted (like in GAT). Node-wise classification for a 2-layer GCN would look like this: . \\[\\begin{equation} \\mathbf{Y} = \\text{softmax}(\\mathbf{A}_2 \\text{ReLU}(\\mathbf{A}\\mathbf{X}\\mathbf{W}_1)\\mathbf{W}_2) \\end{equation}\\] . ",
    "url": "/blog/graph-notes/deep-gnn.html#gcn-overview",
    
    "relUrl": "/graph-notes/deep-gnn.html#gcn-overview"
  },"73": {
    "doc": "Scaling GNNs",
    "title": "Batch issue with training",
    "content": "Nowadays, we do process mini-batches on our datasets. This actually assumes that each datapoint is sampled iid from the data distribution. In our case, the nodes have dependencies to each other. This makes running batched operations very difficult. This is because batching is fundamentally sampling from the dataset of nodes and since these nodes depend on each other, it could make some nodes appear more often in the batching phase. Early GNN works like GCN, ChebNet, MoNet, and GAT were trained using full-batch gradient descent. In other words,This meant that time complexity of an L-layer GCN would be \\(\\mathcal{O}(L\\lvert \\mathcal{V} \\rvert n_f^2)\\) and memory complexity \\(\\mathcal{O}(L\\lvert \\mathcal{V} \\rvert n_f + Ln_f^2)\\). ",
    "url": "/blog/graph-notes/deep-gnn.html#batch-issue-with-training",
    
    "relUrl": "/graph-notes/deep-gnn.html#batch-issue-with-training"
  },"74": {
    "doc": "Scaling GNNs",
    "title": "GraphSAGE",
    "content": "GraphSAGE was the first work to tackle this issue (Bronstein considers this a seminal paper). The SAGE acronym stands for “sample and aggregate”. It essentially does neighborhood sampling and mini-batching to train GNNs on large graphs. The takeaway is that to compute the training loss of a single node with an L-layer GCN, only the L-hop neighbors of that node are necessary. This doesn’t address “small-world” graph networks which are low-diameter but high connectivity. The high-connectivity can make 2-hop neighborhoods already contain way too many nodes (in the millions for social networks). GraphSAGE tackles this by sampling neighbors up to L-th hop: starting from training node, it samples uniformly (with replacement) a fixed number k of 1-hop neighbors, L times. For every node, we are guarantteed to have a bounded L-hop sampled neighborhood of \\(\\mathcal{O}(k^L)\\) nodes. If we then construct a batch with b training nodes, each with its own independent L-hop neighborhood, we get memory complexity of \\(\\mathcal{O}(bk^L)\\) Now the memory complexity of one batch compute is \\(\\mathcal{O}(bLn_f^2k^L)\\). The thing with GraphSAGE is that it could introduce a lot of redundant computation since sampled nodes can appear a lot of times in a dataset. NOTE: this is specifically useful for node-wise classification or regression as everything is done per node (thus the training loss can be distributed between nodes and now you’re sampling between nodes). If you aggregate the node features like PointNet, then there is no benefit to this. Lots of follow-up works like ClusterGCN and GraphSAINT . ",
    "url": "/blog/graph-notes/deep-gnn.html#graphsage",
    
    "relUrl": "/graph-notes/deep-gnn.html#graphsage"
  },"75": {
    "doc": "Scaling GNNs",
    "title": "SIGN",
    "content": "Empirical Findings: . | Simple fixed aggregators (like GCN) outperform GAT and MPNN. | It is an open question whether depth in GNNs are needed. | . The key in this work is to run the linear diffusion operator in parallel: . \\[\\begin{align*} \\mathbf{Z}_0 &amp;= \\mathbf{X}\\mathbf{W_0} \\\\ \\mathbf{Z}_1 &amp;= \\mathbf{A_1}\\mathbf{X}\\mathbf{W_1} \\\\ \\mathbf{Z}_2 &amp;= \\mathbf{A_2}\\mathbf{X}\\mathbf{W_2} \\\\ &amp;... \\\\ \\mathbf{Z}_r &amp;= \\mathbf{A_r}\\mathbf{X}\\mathbf{W_r} \\\\ \\mathbf{Z} &amp;= \\begin{bmatrix} \\mathbf{Z}_0 &amp; \\mathbf{Z}_1^T &amp; \\mathbf{Z}_2^T &amp; ... &amp; \\mathbf{Z}_r^T \\end{bmatrix}^T \\\\ \\mathbf{Y} &amp;= \\text{softmax}(\\text{ReLU}(\\mathbf{Z}) \\mathbf{W}') \\end{align*}\\] The \\(\\mathcal{Z}_i\\) are pre-computed which shows that \\(\\mathcal{A}_i\\mathcal{X}\\) are pre-computed (everything but the learnable parameter). This method can be trained using the simple mini-batch gradient descent method. ",
    "url": "/blog/graph-notes/deep-gnn.html#sign",
    
    "relUrl": "/graph-notes/deep-gnn.html#sign"
  },"76": {
    "doc": "Differentiable Manifold (2.2)",
    "title": "Differentiable Manifold (2.2)",
    "content": " ",
    "url": "/blog/class-notes/00-Geometric-Mechanics/Textbook/diff-manifold.html",
    
    "relUrl": "/class-notes/00-Geometric-Mechanics/Textbook/diff-manifold.html"
  },"77": {
    "doc": "Distributed Notes",
    "title": "Distributed Compute in Deep Learning",
    "content": "In this note, we will cover the use of multiple GPUs in deep learning. This will help us extract the most performance out of our hardware. ",
    "url": "/blog/code-notes/distributed-dl.html#distributed-compute-in-deep-learning",
    
    "relUrl": "/code-notes/distributed-dl.html#distributed-compute-in-deep-learning"
  },"78": {
    "doc": "Distributed Notes",
    "title": "1. Different types of Parallelisms",
    "content": ". 1.1 Data Parallelism: . This is the most common type of parallelism. We split the data across 4 GPUs and each GPU will then compute gradients for its own subset of data. We accumulate these gradients across all GPU devices and update our model parameters. In a sense, this allows us to calculate gradients on larger batches and reduce the time it takes to update our model parameters. NOTE: this requires us to have a copy of the model parameters on each GPU device. We can also do Parameter Sharding where we shard our model parameters across multiple GPUs. We must then communicate the model parameters across these GPUs to calculate each batch. We also communicate the gradients back to the original GPU that has the model parameters. There are other things we can shard as is done with the ZeRO optimizer such as gradients and optimizer states. this is a visual of what we mean by parameters sharding. “os” is optimizer state, “g” is gradients, and “p” is parameters. Use Cases: . | large model sizes | large batches to process | . While Parameter Sharding is great, take note that it requires us to communicate between GPUs. This overhead for large enough models can be a bottleneck. Additionally, with larger data that uses smaller batch sizes, there’s not much benefit in doing Data Parallelism in general. 1.2 Pipeline Parallelism: . Another way of parallelizing our model is to parallelize the forward/backward pass of our model itself. Micro-batching: we can achieve pipeline parallelism through by splitting our input batch into micro-batches and processing them sequentially. While it is sequential, don’t be fooled. We are processing multiple micro-batches at the same time but they are in different stages of the model. The same is done in the backward pass. You can actually see that there is a lot of time wasted because of the sequential nature of this process. GPU 3 is waiting for GPU 2 to finish which waits for the previous GPUs to finish. Can we reduce the time it takes for GPU 3 to wait for GPU 2? . We can do this through Looping Pipelines. As we can see, instead of having a GPU hold a larger sequential portion of the model, we let it have a smaller portion of different parts as shown in the diagram. This reduces the “stage execution time” of the GPU and reduces the waiting time bottleneck that we discussed. We can see the difference between this and the previous naive pipeline parallelism. 1.3 Tensor Parallelism: . Tensor parallelism also parallelizes the model. However, it differs by parallelizing the model weights along the feature dimensions. This is way trickier to implement, but it will allow us to get around the stage execution time bottleneck that we saw in the previous pipeline parallelism. Above is a figure showing the non-trivial issue of tensor parallelism. We need to figure out how to assign the gray cells to a GPU. As you can see \\(A_{0,i}\\) row needs to multiply with \\(x_0,x_1,x_2,x_3\\) which are in different GPUs, so we can infer that communication between GPUs is required. Should \\(A_{0,i}\\) be assigned to GPU 0 (gather) or should it be split across GPUs (scatter)? This is the crux of tensor parallelism. A visual representation of the scatter-gather operation is shown below. Gather strategy is better when input dimension is much larger than output dimension. Scatter strategy is better when input dimension is much smaller than output dimension. We choose whether to gather or scatter per layer. NOTE:: 3D Parallelism is a combination of data parallelism, pipeline parallelism, and tensor parallelism. This is done to train LLMs. ",
    "url": "/blog/code-notes/distributed-dl.html#1-different-types-of-parallelisms",
    
    "relUrl": "/code-notes/distributed-dl.html#1-different-types-of-parallelisms"
  },"79": {
    "doc": "Distributed Notes",
    "title": "Distributed Notes",
    "content": " ",
    "url": "/blog/code-notes/distributed-dl.html",
    
    "relUrl": "/code-notes/distributed-dl.html"
  },"80": {
    "doc": "Isaac Gym Doc Notes",
    "title": "Isaac Gym Notes (for An)",
    "content": "This is the breakdown of the Isaac Gym codebase. Here are important links: . | Documentation | IsaacGym Forum | . ",
    "url": "/blog/code-notes/isaac/doc_notes.html#isaac-gym-notes-for-an",
    
    "relUrl": "/code-notes/isaac/doc_notes.html#isaac-gym-notes-for-an"
  },"81": {
    "doc": "Isaac Gym Doc Notes",
    "title": "Vectorized Environments",
    "content": "IsaacGym environments are vectorized environments. This means they can run multiple environments in parallel. For us, this means that it follows the OpenAI Gym API for vectorized environments: . Here’s example code from IsaacGymEnvs: . import isaacgym import isaacgymenvs import torch num_envs = 2000 envs = isaacgymenvs.make( seed=0, task=\"Ant\", num_envs=num_envs, sim_device=\"cuda:0\", rl_device=\"cuda:0\", ) print(\"Observation space is\", envs.observation_space) print(\"Action space is\", envs.action_space) obs = envs.reset() for _ in range(20): random_actions = 2.0 * torch.rand((num_envs,) + envs.action_space.shape, device = 'cuda:0') - 1.0 envs.step(random_actions) . Here’s another example from the Gym API site: . envs = gym.vector.make(\"CartPole-v1\", num_envs=3) envs.reset() actions = np.array([1, 0, 1]) observations, rewards, dones, infos = envs.step(actions) . Here’s an example implementation of a Gym Environment: . class DictEnv(gym.Env): observation_space = gym.spaces.Dict({ \"position\": gym.spaces.Box(-1., 1., (3,), np.float32), \"velocity\": gym.spaces.Box(-1., 1., (2,), np.float32) }) action_space = gym.spaces.Dict({ \"fire\": gym.spaces.Discrete(2), \"jump\": gym.spaces.Discrete(2), \"acceleration\": gym.spaces.Box(-1., 1., (2,), np.float32) }) def reset(self): return self.observation_space.sample() def step(self, action): observation = self.observation_space.sample() return (observation, 0., False, {}) . no extra methods are needed. We mainly focus on reset and step and make sure we define observation_space and action_space correctly. ",
    "url": "/blog/code-notes/isaac/doc_notes.html#vectorized-environments",
    
    "relUrl": "/code-notes/isaac/doc_notes.html#vectorized-environments"
  },"82": {
    "doc": "Isaac Gym Doc Notes",
    "title": "1. Simulation Setup",
    "content": "1.1 Gym . CoreAPI is located in isaacgym.gymapi'. All Gym API functions are accessed by Gym` object acquired by the following code: . from isaacgym import gymapi gym = gymapi.acquire_gym() . 1.2 Sim . sim_params = gymapi.SimParams() sim = gym.create_sim( compute_device_id, graphics_device_id, gymapi.SIM_PHYSX, sim_params ) . we use create_sim to create a simulation which contains graphics context and physics context. Allows us to load assets, create environments, and interact with simulation. We have two choices for simulation: . | gymapi.SIM_PHYSX - PhysX simulation: robust rigid body and articulation (CPU/GPU) with new tensor api support. | gymapi.SIM_FLEX - Flex simulation: soft body and rigid body simulation (GPU only) does not fully support new tensor api. | . 1.3 Simulation Parameters . This is how to configure the simulation parameters: . # get default set of parameters sim_params = gymapi.SimParams() # set common params sim_params.dt = 1 / 60 sim_params.substeps = 2 sim_params.up_axis = gymapi.UP_AXIS_Z sim_params.gravity = gymapi.Vec3(0.0, 0.0, -9.8) # set PhysX specific params sim_params.physx.use_gpu = True sim_params.physx.solver_type = 1 sim_params.physx.num_position_iterations = 6 sim_params.physx.num_velocity_iterations = 1 sim_params.physx.contact_offset = 0.01 sim_params.physx.rest_offset = 0.0 # set Flex specific params sim_params.flex.solver_type = 5 sim_params.flex.num_outer_iterations = 4 sim_params.flex.num_inner_iterations = 20 sim_params.flex.num_position_iterations = 4 sim_params.flex.relaxation = 0.8 sim_params.flex.warm_start = 0.5 sim = gym.create_sim( compute_device_id, graphics_device_id, gymapi.SIM_PHYSX, sim_params ) . 1.4 Ground Plane . plane_params = gymapi.PlaneParams() plane_params.normal = gymapi.Vec3(0.0, 0.0, 1.0) # z-up plane_params.distance = 0.0 plane_params.static_friction = 1.0 plane_params.dynamic_friction = 1.0 plane_params.restitution = 0.0 gym.add_ground(sim, plane_params) . 1.5 Load Assets . Gym supports URDF and MJCF file formats. Here’s a quick way to load those files into a simulation: . asset_root = \"../../assets\" asset_file = \"urdf/franka_description/robots/franka_panda.urdf\" asset = gym.load_asset(sim, asset_root, asset_file) . The load_asset function uses the file name extension to determine which processing code to use. We can also pass extra information to the asset importer: . asset_options = gymapi.AssetOptions() asset_options.fix_base_link = True asset_options.armature = 0.01 asset = gym.load_asset(sim, asset_root, asset_file, asset_options) . This doesn’t automatically add it to the simulation, but it is the first step. 1.6 Environment and Actors . Environment consists of collection of actors and sensors simulated together. The overall reason to pack them together is performance. Lets setup a simple environment: . spacing = 2.0 lower = gymapi.Vec3(-spacing, 0.0, -spacing) upper = gymapi.Vec3(spacing, spacing, spacing) env = gym.create_env(sim, lower, upper, 8) . this creates 8 environments. If we want to create an actor, we can do so below: . pose = gymapi.Transform() pose.p = gymapi.Vec3(0.0, 0.0, 0.0) pose.r = gymapi.Quat(0.0, 0.0, 0.0, 1.0) actor_handle = gym.create_actor( env, asset, pose, \"MyActor\", 0, 1 ) . Here’s a quick example of going from axis-angle to quaternion: . pose.r = gymapi.Quat.from_axis_angle(gymapi.Vec3(1, 0, 0), -0.5 * math.pi) . Any argument after MyActor are optional. The two args are collision_group and collision_filter . The collision_group assigns the actor to a collision group. This prevents actors from the same environment from colliding with each other. The collision_filter is a bit mask that helps filter collision between bodies (kinda like preventing self-collision checks if we just have a robot). num_envs = 64 envs_per_row = 8 #infer columns from num_envs env_spacing = 2.0 env_lower = gymapi.Vec3(-env_spacing, 0.0, -env_spacing) env_upper = gymapi.Vec3(env_spacing, env_spacing, env_spacing) envs = [] actor_handles = [] for i in range(num_envs): env = gym.create_env(sim, env_lower, env_upper, envs_per_row) envs.append(env) height = random.uniform(1.0, 2.5) pose = gymapi.Transform() pose.p = gymapi.Vec3(0.0, height, 0.0) actor_handle = gym.create_actor( env, asset, pose, \"MyActor\", i, 1 ) actor_handles.append(actor_handle) . Be aware that once you finish populating one environment and start creating another, the first environment can no longer have stuff added to it. This is because of how memory is allocated. We can find ways to “fake” having dynamic number of actors, but that is for later. 1.7 Running Sim . Super Simple: . while True: # step the physics gym.simulate(sim) # each iteration is one timestep gym.fetch_results(sim, True) . 1.8 Adding a Viewer . If we want to visualize our setup, we need to add a viewer. cam_props = gymapi.CameraProperties() viewer = gym.create_viewer(sim, cam_props) . If we want to update the viewer (re-draw on it), we need to execute the following every iteration: . gym.step_graphics(sim) gym.draw_viewer(viewer, sim, True) . The step_graphics function synchronizes visual representation with simulation context. The draw_viewer renders the visual representation of the simulation. They are separate because step_graphics can be called without draw_viewer or even the need for a viewer. To synchronize visual update frequency with real time we add this to the loop: . gym.sync_frame_time(sim) . This throttles down the simulation rate to match real-time. If it’s slower than real time, this has no effect. We can terminate simulation by checking if viewer was closed with query_viewer_has_closed. while not gym.query_viewer_has_closed(viewer): gym.simulate(sim) gym.fetch_results(sim, True) gym.step_graphics(sim) gym.draw_viewer(viewer, sim, True) gym.sync_frame_time(sim) . ",
    "url": "/blog/code-notes/isaac/doc_notes.html#1-simulation-setup",
    
    "relUrl": "/code-notes/isaac/doc_notes.html#1-simulation-setup"
  },"83": {
    "doc": "Isaac Gym Doc Notes",
    "title": "2. Assets",
    "content": "Lets start off with loading an asset. asset_root = \"../../assets\" asset_file = \"urdf/franka_description/robots/franka_panda.urdf\" asset = gym.load_asset(sim, asset_root, asset_file) . We can specify options for asset. asset_options = gymapi.AssetOptions() asset_options.fix_base_link = True asset_options.armature = 0.01 asset_options.use_mesh_materials = True asset_options.mesh_normal_mode = gymapi.COMPUTE_PER_VERTEX # asset_options.mesh_normal_mode = gymapi.COMPUTE_PER_FACE # asset_options.convex_decomposition_from_submeshes = True asset = gym.load_asset(sim, asset_root, asset_file, asset_options) . 2.1 Overriding Inertial Properties . The URDF and MJCF file formats also specify inertial properties. We can override these properties. asset_options.override_com = True asset_options.override_inertia = True . This will use values computed from geometries of collision shapes. 2.2 Convex Decomposition . asset_options.vhacd_enabled = True . This enables convex decomposition which makes much more complex collision shape (matches actual mesh). 2.3 Procedural Assets (Primitives) . We can create simple geometric assets. Just need to do the following: . asset_options = gym.AssetOptions() asset_options.density = 10.0 box_asset = gym.create_box(sim, width, height, depth, asset_options) sphere_asset = gym.create_sphere(sim, radius, asset_options) capsule_asset = gym.create_capsule(sim, radius, length, asset_options) . ",
    "url": "/blog/code-notes/isaac/doc_notes.html#2-assets",
    
    "relUrl": "/code-notes/isaac/doc_notes.html#2-assets"
  },"84": {
    "doc": "Isaac Gym Doc Notes",
    "title": "3. Physics Simulation",
    "content": "3.1 Creating Actors . An actor is an instance of GymAsset. create_actor creates actor and adds it to environment and returns handle to it. Much better to save these handles rather than look them up every time we need to access them. envs = [] actor_handles = [] print(\"Creating %d environments\" % num_envs) for i in range(num_envs): env = gym.create_env(sim, env_lower, env_upper, num_per_row) envs.append(evs) # add actor actor_handle = gym.create_actor(env, asset, pose, \"actor\", i, 1) actor_handles.append(actor_handle) . actor_handles are specific to an environment. Functions that operate on actor are get_actor_*, set_actor_*, and apply_actor_*. 3.2 Aggregates (only on PhysX) . Aggregate is collection of actors. They do not provide extra simulation functionality, but they tell you that a set of actors will be clustered together, which allow PhysX to optimize its spatial data operations. Creating them gives modest performance boost. gym.begin_aggregate(env, max_bodies, max_shapes, True) gym.create_actor(env, ...) gym.create_actor(env, ...) gym.end_aggregate(env) . 3.3 Actor Components . num_bodies = gym.get_actor_rigid_body_count(env, actor_handle) num_joints = gym.get_actor_joint_count(env, actor_handle) num_dofs = gym.get_actor_dof_count(env, actor_handle) . Joints - these can be fixed, revolute, prismatic. Degrees of Freedom (DOF) - these are the actuated joints. They are the joints that can be controlled. 3.4 Controlling Actors . Controlling actors is done through using the DOFs. For each DOF, we can set drive mode, limits, stiffness, damping, and targets. Scaling Actors - we can scale their size at runtime. It changes its mas, joint positions, prismatic joint limits, and collision geometry. The CoM isn’t updated though. Resetting transforms or velocities or applying forces at the same time as scaling may yield incorrect results. DOF Properties and drive Modes - can be accessed with get_asset_dof_properties and individual actors (get_actor_dof_properties/set_actor_dof_properties). | Name | Data Type | Description | . | hasLimits | bool | Whether DOF has limits | . | lower | float32 | Lower limit | . | upper | float32 | Upper limit | . | driveMode | gymapi.DofDriveMode | DOF drive mode | . | stiffness | float32 | Drive Stiffness | . | damping | float32 | Drive Damping | . | velocity | float32 | Max Velocity | . | effort | float32 | Max effort (force/torque) | . | friction | float32 | DOF friction | . | armature | float32 | DOF armature | . DOF_MODE_NONE lets joints move freely. props = gym.get_actor_dof_properties(env, actor_handle) props['driveMode'].fill(gymapi.DOF_MODE_NONE) props['stiffness'].fill(0.0) props['damping'].fill(0.0) gym.set_actor_dof_properties(env, actor_handle, props) . DOF_MODE_EFFORT lets us apply efforts to the DOF using apply_actor_dof_efforts. # configure the joints for effort control mode (once) props = gym.get_actor_dof_properties(env, actor_handle) props[\"driveMode\"].fill(gymapi.DOF_MODE_EFFORT) props[\"stiffness\"].fill(0.0) props[\"damping\"].fill(0.0) gym.set_actor_dof_properties(env, actor_handle, props) # apply efforts (every frame) efforts = np.full(num_dofs, 100.0).astype(np.float32) gym.apply_actor_dof_efforts(env, actor_handle, efforts) . DOF_MODE_POS lets u set target positions for a PD controller. props = gym.get_actor_dof_properties(env, actor_handle) props[\"driveMode\"].fill(gymapi.DOF_MODE_POS) props[\"stiffness\"].fill(1000.0) props[\"damping\"].fill(200.0) gym.set_actor_dof_properties(env, actor_handle, props) targets = np.zeros(num_dofs).astype('f') gym.set_actor_dof_position_targets(env, actor_handle, targets) . if DOF is linear, target is in meters. If DOF is angular, target is in radians. if we want to set random positions for the DOF, here it is: . dof_props = gym.get_actor_dof_properties(envs, actor_handles) lower_limits = dof_props['lower'] upper_limits = dof_props['upper'] ranges = upper_limits - lower_limits pos_targets = lower_limits + ranges * np.random.random(num_dofs).astype('f') gym.set_actor_dof_position_targets(env, actor_handle, pos_targets) . DOF_MODE_VEL is for velocity control. props = gym.get_actor_dof_properties(env, actor_handle) props[\"driveMode\"].fill(gymapi.DOF_MODE_VEL) props[\"stiffness\"].fill(0.0) props[\"damping\"].fill(600.0) gym.set_actor_dof_properties(env, actor_handle, props) vel_targets = np.random.uniform(-math.pi, math.pi, num_dofs).astype('f') gym.set_actor_dof_velocity_targets(env, actor_handle, vel_targets) . Unlike efforts, the ones with controllers only require targets to be set every tame targets need change. Efforts need to be set every frame. NOTE: Tensor Control API allows alternative ways to applying controls without setting in CPU. We can run simulations entirely on GPU. 3.5 Physics State . 3.5.1 Rigid Body States . actor_body_states = gym.get_actor_rigid_body_states(env, actor_handle, gymapi.STATE_ALL) env_body_states = gym.get_env_rigid_body_states(env, gymapi.STATE_ALL) sim_body_states = gym.get_sim_rigid_body_states(sim, gymapi.STATE_ALL) . You can get states for an actor, an environment, or entire simulation. This returns structured numpy arrays. Last argument is a flag for what it should return. STATE_POS only returns positions. STATE_VEL only returns velocities. STATE_ALL returns both positions and velocities. We can access the states like so: . body_states[\"pose\"] # all poses (position and orientation) body_states[\"pose\"][\"p\"]) # all positions (Vec3: x, y, z) body_states[\"pose\"][\"r\"]) # all orientations (Quat: x, y, z, w) body_states[\"vel\"] # all velocities (linear and angular) body_states[\"vel\"][\"linear\"] # all linear velocities (Vec3: x, y, z) body_states[\"vel\"][\"angular\"] # all angular velocities (Vec3: x, y, z) . We can set the states like so: . gym.set_actor_rigid_body_states(env, actor_handle, body_states, gymapi.STATE_ALL) gym.set_env_rigid_body_states(env, body_states, gymapi.STATE_ALL) gym.set_sim_rigid_body_states(sim, body_states, gymapi.STATE_ALL) . We can only find the index of a specific rigid body using the following: . i1 = gym.find_actor_rigid_body_index(env, actor_handle, \"body_name\", gymapi.DOMAIN_ACTOR) i2 = gym.find_actor_rigid_body_index(env, actor_handle, \"body_name\", gymapi.DOMAIN_ENV) i3 = gym.find_actor_rigid_body_index(env, actor_handle, \"body_name\", gymapi.DOMAIN_SIM) . 3.5.2 DOF States . We can also work with reduced coordinates. dof_states = gym.get_actor_dof_states(env, actor_handle, gymapi.STATE_ALL) gym.set_actor_dof_states(env, actor_handle, dof_states, gymapi.STATE_ALL) . dof_states[\"pos\"] # all positions dof_states[\"vel\"] # all velocities . ",
    "url": "/blog/code-notes/isaac/doc_notes.html#3-physics-simulation",
    
    "relUrl": "/code-notes/isaac/doc_notes.html#3-physics-simulation"
  },"85": {
    "doc": "Isaac Gym Doc Notes",
    "title": "4. Tensor API",
    "content": "This API use GPU-compatible data representations for interacting with simulations. Tensors from this API are “global” tensors (they hold values for all actors in simulation). This sort of structure makes it very easy to enable data parallelism. 4.1 Simulation Setup . This is currently only available for PhysX simulation. sim_params = gymapi.SimParams() ... sim_params.use_gpu_pipeline = True sim_params.physx.use_gpu = True sim = gym.create_sim(compute_device_id, graphics_device_id, gymapi.SIM_PHYSX, sim_params) . We must call prepare_sim to initialize internal data structures used by tensor API. gym.prepare_sim(sim) . 4.2 Physics State . After prepare_sim, we can acquire physics state tensors. 4.2.1 Actor Root State Tensor . A Gym actor has 1+ rigid bodies. All actors have a root body. The root state tensor holds state of all actor root bodies in simulation. The state of root body is 13-dimensional [position, quaternion, linear velocity, angular velocity]. _root_tensor = gym.acquire_actor_root_state_tensor(sim) . This returns generic tensor descriptor. In order to access contents of tensor, we warp it in PyTorch Tensor object using gymtorch.wrap_tensor. root_tensor = gymtorch.wrap_tensor(_root_tensor) . now this is a tensor in pytorch. here we can see the structure of the tensor: . root_positions = root_tensor[:, 0:3] root_orientations = root_tensor[:, 3:7] root_linvels = root_tensor[:, 7:10] root_angvels = root_tensor[:, 10:13] . To update contents of tensors with latest state, call: . gym.refresh_actor_root_state_tensor(sim) . this will update the tensor with latest state. Generally, do this after each call to `gym.simulate’. # ...create sim, envs, and actors here... gym.prepare_sim() # acquire root state tensor descriptor _root_tensor = gym.acquire_actor_root_state_tensor(sim) # wrap it in a PyTorch Tensor and create convenient views root_tensor = gymtorch.wrap_tensor(_root_tensor) root_positions = root_tensor[:, 0:3] root_orientations = root_tensor[:, 3:7] root_linvels = root_tensor[:, 7:10] root_angvels = root_tensor[:, 10:13] # main simulation loop while True: # step the physics simulation gym.simulate(sim) # refresh the state tensors gym.refresh_actor_root_state_tensor(sim) # ...use the latest state tensors here... We can also set the root state tensor. # acquire root state tensor descriptor _root_tensor = gym.acquire_actor_root_state_tensor(sim) # wrap it in a PyTorch Tensor root_tensor = gymtorch.wrap_tensor(_root_tensor) # save a copy of the original root states saved_root_tensor = root_tensor.clone() step = 0 # main simulation loop while True: # step the physics simulation gym.simulate(sim) step += 1 if step % 100 == 0: gym.set_actor_root_state_tensor(sim, gymtorch.unwrap_tensor(saved_root_tensor)) . We can also set root state tensor for a subset of actors using set_actor_root_state_tensor_indexed. 4.2.3 Degrees of Freedom . Articulated actors have a number of DoFs whose state can be queried and changed. State of each DoF is represented with 2 float32 values: position and velocity. The DoF state tensor contains state of all DoFs in simulation. Number of DoFs can be obtained by calling gym.get_sim_dof_count(sim). The DoF states are laid out sequentially. Number of DoFs for a specific actor can be obtained by calling gym.get_actor_dof_count(env, actor). Global index of a specific DoF can be obtained a number of ways: . | gym.get_actor_dof_index(env, actor_handle, i, gymapi.DOMAIN_SIM). | gym.find_actor_dof_index(env, actor_handle, dof_name, gymapi.DOMAIN_SIM) | . We can access dof state as follows: . _dof_states = gym.acquire_dof_state_tensor(sim) dof_states = gymtorch.wrap_tensor(_dof_states) . Function refresh_dof_state_tensor populates tensor with latest data from simulation. just like the root state tensor, we can set the DoF state tensor using gym.set_dof_state_tensor(sim, _dof_states). We can also set the DoF state tensor for a subset of actors using set_actor_dof_state_tensor_indexed. 4.2.4 Rigid Body States . Rigid body states contains state of all rigid bodies in simulation. Each rigid body has 13-dimensional state vector [position, quaternion, linear velocity, angular velocity]. We can get total number of rigid bodies using gym.get_sim_rigid_body_count(sim). We can get rigid body states from an actor using gym.get_actor_rigid_body_states. We can get actor index using gym.get_actor_rigid_body_index or gym.find_actor_rigid_body_index. We can get rigid body states using gym.acquire_rigid_body_state_tensor(sim). Which will need to by wrapped using gymtorch.wrap_tensor. _rb_states = gym.acquire_rigid_body_state_tensor(sim) rb_states = gymtorch.wrap_tensor(_rb_states) . We can populate tensor using gym.refresh_rigid_body_state_tensor(sim). 4.2.5 Jacobian and Mass Matrices . In order to get these, you need the name of the actor which is provided when calling create_actor. Once you have the name, you can use the following to get the Jacobian and Mass Matrix: . _jacobian = gym.acquire_jacobian_tensor(sim, \"franka\") _massmatrix = gym.acquire_mass_matrix_tensor(sim, \"franka\") # wrap tensor jacobian = gymtorch.wrap_tensor(_jacobian) mass_matrix = gymtorch.wrap_tensor(_massmatrix) . to refresh we call the following: . gym.refresh_jacobian_tensors(sim, \"franka\") gym.refresh_mass_matrix_tensors(sim, \"franka\") . Shape of Mass Matrix is (num_dofs, num_dofs). Shape of Mass Matrix Tensor is (num_envs, num_dofs, num_dofs). Shape of jacobian depends on number of links, fixed-base or floating-base, and DOFs. Each body has 6 rows in jacobian representing linear and angular motions. The column represents the DOFs. If actor base is floating, Jacobian shape is (num_envs, num_links, 6, num_dofs + 6). The first 6 columns are the base DOFs. Here’s an example of looking up jacobian of a link: . link_dict = gym.get_asset_rigid_body_dict(franka_asset) dof_dict = gym.get_asset_dof_dict(franka_asset) link_index = link_dict[\"panda_hand\"] dof_index = dof_dict[\"panda_joint5\"] # for all envs: jacobian[:, link_index, :, dof_index + 6] # for a specific env: jacobian[env_index, link_index, :, dof_index + 6] . If base is fixed, we don’t have those extra 6 columns. Thus, the shape is (num_envs, num_links - 1, 6, num_dofs). # for all envs: jacobian[:, link_index - 1, :, dof_index] # for a specific env: jacobian[env_index, link_index - 1, :, dof_index] . 4.3 Contact Tensors . We can call the net contact forces experienced by each rigid body during last sim step. This is not each individual contact force, but the net force applied to the rigid body. _net_cf = gym.acquire_net_contact_force_tensor(sim) net_cf = gymtorch.wrap_tensor(_net_cf) # netcf shape: (num_rigid_bodies, 3) gym.refresh_net_contact_force_tensor(sim) . 4.4 Control Tensors . We show how to apply actions to simulation using tensor API. # get total number of DOFs num_dofs = gym.get_sim_dof_count(sim) # generate a PyTorch tensor with a random force for each DOF actions = 1.0 - 2.0 * torch.rand(num_dofs, dtype=torch.float32, device=\"cuda:0\") # apply the forces gym.set_dof_actuation_force_tensor(sim, gymtorch.unwrap_tensor(actions)) . Actuation forces will only be applied for DOFs whose driveMode was set to DOF_MODE_EFFORT. We can also run set_dof_position_target_tensor or set_dof_velocity_target_tensor depending on our drive mode. We can apply forces to rigid bodies using apply_rigid_body_force_tensors. # apply both forces and torques gym.apply_rigid_body_force_tensors(sim, force_tensor, torque_tensor, gymapi.ENV_SPACE) # apply only forces gym.apply_rigid_body_force_tensors(sim, force_tensor, None, gymapi.ENV_SPACE) # apply only torques gym.apply_rigid_body_force_tensors(sim, None, torque_tensor, gymapi.ENV_SPACE) . 4.5 Common Problem (Tensor Lifetime) . Tensor in PyTorch are subject to garbage collection. Gym C++ runtime is not conected with python interpreter, so it doesn’t participate in the reference counting of objects. We should be careful to make sure objects are not garbage-collected. indices_torch = torch.LongTensor([0, 17, 42]) indices_gym = gymtorch.unwrap_tensor(indices_torch.to(torch.int32)) set_actor_root_state_tensor_indexed(sim, states, indices_gym, 3) . This doesn’t seem like an issue, but indices_torch.to(torch.int32) will be garbage collected since there is no reference to it. This is because gymtorch.unwreap_tensor does not participate in the reference counting. Instead, we should do the following: . indices_torch = torch.LongTensor([0, 17, 42]) indices_torch32 = indices_torch.to(torch.int32) # &lt;--- this reference will keep the tensor alive set_actor_root_state_tensor_indexed(sim, states, gymtorch.unwrap_tensor(indices_torch32), 3) . now indices_torch.to(torch.int32) will not be garbage collected since indices_torch32 is referencing it. 4.6 Limitations . Only supported for PhysX backend. A tensor “setter” function should only be called once per step such as set_actor_root_state_tensor. We can call other setter functions, but only once per function. gym.set_actor_root_state_tensor_indexed(sim, root_states, indices1, n1) gym.set_actor_root_state_tensor_indexed(sim, root_states, indices2, n2) . this is no-no. Also only call refresh functions once per step, and it should be called before any “setter” functions. If we call refresh after, we will only get stale data in the next loop. Keep that in mind. ",
    "url": "/blog/code-notes/isaac/doc_notes.html#4-tensor-api",
    
    "relUrl": "/code-notes/isaac/doc_notes.html#4-tensor-api"
  },"86": {
    "doc": "Isaac Gym Doc Notes",
    "title": "5. Force Sensors",
    "content": "5.1 Rigid Body Force Sensors . We can attach force sensors to rigid bodies to measure forces and torques. The readings are net forces and torques acting on parent body which include external forces, contact forces, and internal forces (joint drives). Body resting on ground plane will have a net force of zero. Force sensors are created on assets. To create force sensor, you specify rigid body index where sensor is attached and relative poes of the sensor with respect to body origin. body_idx = gym.find_actor_rigid_body_index(asset, \"bodyName\") sensor_pose1 = gymapi.Transform(gymapi.Vec3(0.2, 0.0, 0.0)) sensor_pose2 = gymapi.Transform(gymapi.Vec3(-0.2, 0.0, 0.0)) sensor_idx1 = gym.create_asset_force_sensor(asset, body_idx, sensor_pose1) sensor_idx2 = gym.create_asset_force_sensor(asset, body_idx, sensor_pose2) . You can also pass additional properties to sensor: . sensor_props = gymapi.ForceSensorProperties() sensor_props.enable_forward_dynamics_forces = True sensor_props.enable_constraint_solver_forces = True sensor_props.use_world_frame = True sensor_idx = gym.create_asset_force_sensor(asset, body_idx, sensor_pose, sensor_props) . After constructing these guys, you can access individual force sensors using index: . actor_handle = gym.create_actor(env, asset, ...) num_sensors = gym.get_actor_force_sensor_count(env, actor_handle) for i in range(num_sensors): sensor = gym.get_actor_force_sensor(env, actor_handle, i) . During simulation, you can query latest sensor readings: . sensor_data = sensor.get_forces() print(sensor_data.force) #Vec3 print(sensor_data.torque) #Vec3 . Get total number of force sensors gym.get_sim_force_sensor_count(sim). 5.1.1 Tensor API . We can get a Gym tensor descriptor for all force sensors in simulation using gym.acquire_force_sensor_tensor(sim). _fsdata = gym.acquire_force_sensor_tensor(sim) fsdata = gymtorch.wrap_tensor(_fsdata) . The shape of this tensor is (num_force_sensors, 6) and data type is float32. The first 3 floats are force and last 3 are torque. After each sim step, you can get latest sensor readings calling gym.refresh_force_sensor_tensor(sim). 5.2 Joint Force Sensors . This is easier to do. Just call enable_actor_dof_force_sensors after creating the actor. actor = gym.create_actor(env, ...) gym.enable_actor_dof_force_sensors(env, actor) . During simulation, we retrieve forces using get_actor_dof_forces. forces = gym.get_actor_dof_forces(env, actor_handle) . This returns numpy array of total forces acting on this actor’s DOFF. These forces will be zero if we do not enable joint force sensors. 5.2.1 Tensor API . We can also use the tensor API to get forces. _forces = gym.acquire_dof_force_tensor(sim) forces = gymtorch.wrap_tensor(_forces) . This is a 1-dim tensor of float32 corresponding to each DOF in simulation. We can get latest sensor reading by calling gym.refresh_dof_force_tensor(sim). DOF force sensors are only available for PhysX backend. ",
    "url": "/blog/code-notes/isaac/doc_notes.html#5-force-sensors",
    
    "relUrl": "/code-notes/isaac/doc_notes.html#5-force-sensors"
  },"87": {
    "doc": "Isaac Gym Doc Notes",
    "title": "6. Graphics and Camera Sensors",
    "content": "The point of this section is to show how to add camera sensors into our setup. 6.1 Camera Properties . All cameras can be created with parameters passed in GymCameraProperties. viewer = gym.create_viewer(sim, gymapi.CameraProperties()) ... camera_props = gymapi.CameraProperties() camera_props.horizontal_fov = 75.0 # degrees camera_props.width = 1920 camera_props.height = 1080 viewer = gym.create_viewer(sim, camera_props) . 6.2 Camera Sensors . Camera sensors are meant to simulate cameras as if they were an actual sensor. They are not meant to be used for rendering. camera_props = gymapi.CameraProperties() camera_props.width = 128 camera_props.height = 128 camera_handle = gym.create_camera_sensor(env, camera_props) . We can set the camera location using set_camera_location: . gym.set_camera_location(camera_handle, env, gymapi.Vec3(x,y,z), gymapi.Vec3(tx,ty,tz)) . where (x,y,z) is the camera position and (tx,ty,tz) is position camera is looking at. All of this is in environment local coordinates. The method I like more is to specify GymTransform: . transform = gymapi.Transform() transform.p = (x,y,z) transform.r = gymapi.Quat.from_axis_angle(gymapi.Vec3(0,1,0), np.radians(45.0)) gym.set_camera_transform(camera_handle, env, body_handle, transform, gymapi.FOLLOW_TRANSFORM) . Last argument determines attachment behavior. | gymapi.FOLLOW_POSITION - camera maintains fixed offset from rigid body but will not rotate. | gymapi.FOLLOW_TRANSFORM - camera maintains fixed transform relative to rigid body. | . if we don’t specify, the camera will be fixed in world space. All cameras are rendered together in one API cal. gym.render_all_camera_sensors(sim) . We can access camera image using get_camera_image: . color_image = gym.get_camera_image(sim, camera_handle, gymapi.IMAGE_COLOR) depth_image = gym.get_camera_image(sim, camera_handle, gymapi.IMAGE_DEPTH) seg_image = gym.get_camera_image(sim, camera_handle, gymapi.IMAGE_SEGMENTATION) optical_flow_image = gym.get_camera_image(sim, camera_handle, gymapi.IMAGE_OPTICAL_FLOW) . One key thing is to keep the camera stuff in GPU. camera_props = gymapi.CameraProperties() camera_props.enable_tensors = True cam_handle = gym.create_camera_sensor(env, camera_props) . camera_tensor = gym.get_camera_image_gpu_tensor(sim, env, cam_handle, gymapi.IMAGE_COLOR) torch_camera_tensor = gymtorch.wrap_tensor(camera_tensor) . this allows us to get the camera image as a tensor and use it in PyTorch. Here is a loop for cameras when using simulation in isaacgym: . while True: gym.simulate(sim) gym.fetch_results(sim, True) gym.step_graphics(sim) gym.render_all_camera_sensors(sim) gym.start_access_image_tensors(sim) # # User code to digest tensors # gym.end_access_image_tensors(sim) . ",
    "url": "/blog/code-notes/isaac/doc_notes.html#6-graphics-and-camera-sensors",
    
    "relUrl": "/code-notes/isaac/doc_notes.html#6-graphics-and-camera-sensors"
  },"88": {
    "doc": "Isaac Gym Doc Notes",
    "title": "Python API",
    "content": ". | Checkout IsaacGym_Preview_TacSL_Package/isaacgym/docs/api/python/gym_py.html | . ",
    "url": "/blog/code-notes/isaac/doc_notes.html#python-api",
    
    "relUrl": "/code-notes/isaac/doc_notes.html#python-api"
  },"89": {
    "doc": "Isaac Gym Doc Notes",
    "title": "Isaac Gym Doc Notes",
    "content": " ",
    "url": "/blog/code-notes/isaac/doc_notes.html",
    
    "relUrl": "/code-notes/isaac/doc_notes.html"
  },"90": {
    "doc": "FOTS",
    "title": "FOTS",
    "content": "\\[\\begin{equation} M_c = M_{\\text{ini}} + \\Delta d_d + \\Delta d_s + \\Delta d_t \\end{equation}\\] . | \\(M_c\\): current marker position | \\(M_{\\text{ini}}\\): initial marker position (without object contact) | \\(\\Delta d_d\\): dilate motion displacement | \\(\\Delta d_s\\): shear motion displacement | \\(\\Delta d_t\\): twist motion displacement | . \\[\\begin{align} \\Delta d_d &amp;= \\sum_{i=1}^N \\Delta h_i \\cdot (M - C_i) \\cdot \\exp(-\\lambda_d \\|M - C_i\\|^2) \\end{align}\\] . | \\(M\\): another way of saying \\(M_{\\text{ini}}\\) | \\(\\Delta h_i\\): height of \\(C_i\\) (from depth map). | \\(C_i\\): markers in contact. | \\(N\\): number of \\(C_i\\). | . \\[\\begin{align} \\Delta d_s &amp;= \\min\\{\\Delta s, \\Delta s_{\\max} \\} \\cdot \\exp(-\\lambda_s \\| M - G \\|^2_2) \\\\ \\Delta d_t &amp;= \\min\\{\\Delta \\theta, \\Delta \\theta_{\\max} \\} \\cdot \\exp(-\\lambda_t \\| M - G \\|^2_2) \\end{align}\\] . | \\(G\\): projection point of object coordinate system origin on gel surface along normal direction. | \\(\\Delta s\\): translation distance of G relative to gel surface. | \\(\\Delta \\theta\\): rotation angle of object coordinate system relative to gel surface. | . We calibrate \\(\\lambda_d, \\lambda_s, \\lambda_t\\) for FOTS. ",
    "url": "/blog/deep-dive-notes/tactile-rl-notes/fots.html",
    
    "relUrl": "/deep-dive-notes/tactile-rl-notes/fots.html"
  },"91": {
    "doc": "FOTS",
    "title": "FOTS Code Implementation",
    "content": "This is the implementation of marker motion in FOTS. def __init__(....): ... # self.W, self.H are dimensions of image self.x = np.arange(0, self.W, 1) self.y = np.arange(0, self.H, 1) self.xx, self.yy = np.meshgrid(self.y, self.x) def _marker_motion(self): xind = (np.random.random(self.N * self.M) * self.W).astype(np.int16) yind = (np.random.random(self.N * self.M) * self.H).astype(np.int32) x = np.arange(23, 320, 29)[:self.N] y = np.arange(15, 240, 26)[:self.M] xind, yind = np.meshgrid(x, y) xind = (xind.reshape([1, -1])[0]).astype(np.int16) yind = (yind.reshape([1, -1])[0]).astype(np.int16) xx_marker, yy_marker = self.xx[xind, yind].reshape([self.M, self.N]), self.yy[xind, yind].reshape([self.M, self.N]) self.xx,self.yy = xx_marker, yy_marker img = self._generate(xx_marker, yy_marker) xx_marker_, yy_marker_ = self._motion_callback(xx_marker, yy_marker) img = self._generate(xx_marker_, yy_marker_) self.contact = [] def _motion_callback(self,xx,yy): for i in range(self.N): for j in range(self.M): r = int(yy[j, i]) c = int(xx[j, i]) if self.mask[r,c] == 1.0: h = self.depth[r,c]*100 # meter to mm self.contact.append([r,c,h]) if not self.contact: xx,yy = self.xx,self.yy xx_,yy_ = self._dilate(self.lamb[0], xx ,yy) if len(self.traj) &gt;= 2: xx_,yy_ = self._shear(int(self.traj[0][0]*meter2pix + 120), int(self.traj[0][1]*meter2pix + 160), self.lamb[1], int((self.traj[-1][0]-self.traj[0][0])*meter2pix), int((self.traj[-1][1]-self.traj[0][1])*meter2pix), xx_, yy_) theta = max(min(self.traj[-1][2]-self.traj[0][2], 50 / 180.0 * math.pi), -50 / 180.0 * math.pi) xx_,yy_ = self._twist(int(self.traj[-1][0]*meter2pix + 120), int(self.traj[-1][1]*meter2pix + 160), self.lamb[2], theta, xx_, yy_) return xx_,yy_ . ",
    "url": "/blog/deep-dive-notes/tactile-rl-notes/fots.html#fots-code-implementation",
    
    "relUrl": "/deep-dive-notes/tactile-rl-notes/fots.html#fots-code-implementation"
  },"92": {
    "doc": "FOTS",
    "title": "FOTS Calibration",
    "content": "For calibration (based on README.md of codebase), we take ~45 tactile flow images of a sphere indenter on different locations (in order of dilate, shear, and twist, 15 images for each type). We get tactile flow through optical flow algorithm (Farneback) between initial image and deformed image. We label center, circumference, and contact points (depth &gt; 0) of dilate image. \\[\\begin{align} &amp;\\min_{\\lambda_d} \\sum_{d=1}^{|D|} \\sum_{m=1}^{|M|} (y^{(d,m)} - f_d^{(d,m)}(\\lambda_d))^2 \\\\ f_d(\\lambda_d) &amp;= \\sum_{c=1}^C \\Delta h_c \\cdot (M - C_c) \\cdot \\exp(-\\lambda_d \\|M - C_c\\|^2) \\\\ &amp;\\min_{\\lambda_s} \\sum_{s=1}^{|S|} \\sum_{m=1}^{|M|} (y^{(s,m)} - f_s^{(s,m)}(\\lambda_s))^2 \\\\ f_s(\\lambda_s) &amp;= \\Delta s \\cdot \\exp(-\\lambda_s \\| M - G \\|^2_2) \\\\ &amp;\\min_{\\lambda_t} \\sum_{t=1}^{|T|} \\sum_{m=1}^{|M|} (y^{(t,m)} - f_t^{(t,m)}(\\lambda_t))^2 \\\\ f_t(\\lambda_t) &amp;= \\Delta \\theta \\cdot \\exp(-\\lambda_t \\| M - G \\|^2_2) \\end{align}\\] ",
    "url": "/blog/deep-dive-notes/tactile-rl-notes/fots.html#fots-calibration",
    
    "relUrl": "/deep-dive-notes/tactile-rl-notes/fots.html#fots-calibration"
  },"93": {
    "doc": "GNN as Gradient Flow",
    "title": "Graph Neural Networks as gradient flows",
    "content": "This is kind of diving deeper into a physics-inspired architecture of the GNN Diffusion perspective. Gradient Flows . Consider a dynamical system governed by evolution equation: . \\[\\begin{equation} \\dot{\\mathbf{X}} = \\mathbf{F}(\\mathbf{X}(t)) \\end{equation}\\] Gradient Flows are special types of evolution equations of the form . \\[\\begin{equation} \\mathbf{F(\\mathbf{X}(t))} = -\\nabla \\mathcal{E}(\\mathbf{X}(t)) \\end{equation}\\] where \\(\\mathcal{E}\\) is an energy functional. Gradient flow makes \\(\\mathcal{E}\\) monotonically decrease during evolution. Diffusion Equation . The diffusion equation is given by: . \\[\\begin{equation} \\dot{\\mathbf{X}}(t) = \\Delta \\mathbf{X} \\end{equation}\\] which is an example of gradient flow. \\(\\Delta\\) is an nxn graph laplacian matrix. The diffusion equation is the gradient flow of the Dirichlet Energy: . \\[\\begin{align} \\mathcal{E}^{\\text{DIR}}(\\mathbf{X}) &amp;= \\frac{1}{2} \\text{trace}(\\mathbf{X}^T \\Delta \\mathbf{X}) \\\\ &amp;= \\frac{1}{2} \\sum_{uv} \\| (\\nabla \\mathbf{X})_{uv} \\|^2 \\end{align}\\] where \\((\\nabla \\mathbf{X})_{uv}\\) is the gradient of the features on every edge of the graphl The Dirichlet energy measures the smoothness of the features on the graph. As \\(t \\to \\infty\\), the features will converge to a vector and all become the same (oversmoothing). Convolutional GNN . The current diffusion equation is not too useful in Graph ML. We should do an interpretation of the equation that is most relevant to what we’ll use in practice (GNNs). Let this be our evolution equation: . \\[\\begin{equation} \\dot{\\mathbf{X}}(t) = \\text{GNN}(\\mathcal{G}, \\mathbf{X}(t) ; \\theta(t)) \\end{equation}\\] which we discretize with forward-euler using timestep \\(0 &lt; \\tau &lt; 1\\). \\[\\begin{equation} \\mathbf{X}(t + \\tau) = \\mathbf{X}(t) + \\tau \\text{GNN}(\\mathcal{G}, \\mathbf{X}(t), \\theta(t)) \\end{equation}\\] This is how we can describe GNN message-passing in terms of gradient flow. Say we were doing GCNs, we can write the GNN as: . \\[\\begin{equation} \\mathbf{X}(t + \\tau) = \\mathbf{X}(t) + \\tau \\sigma(-\\mathbf{X}(t) \\Omega(t) + \\tilde{\\mathbf{A}}\\mathbf{X}(t)\\mathbf{W}(t)) \\end{equation}\\] where \\(\\Omega, \\mathbf{W}\\) are learnable matrices and \\(\\tilde{\\mathbf{A}}\\) is the normalized adjacency matrix. Simplest convolutional graph network is when \\(\\Omega = 0\\). \\[\\begin{equation} \\end{equation}\\] Parametrising Energy . Instead of parametrising the evolution equation, we parametrise the energy and utilize its gradient flow as the evolution equation: . \\[\\begin{equation} \\dot{\\mathbf{X}}(t) = - \\nabla \\mathcal{E}(\\mathcal{G}, \\mathbf{X}(t) ; \\theta(t)) \\end{equation}\\] of some energy \\(\\mathcal{E}\\). Now we can study families of energy functions to find out which ones are effective to create evolution equations for our GNNs. Energy to Evolution . Consider the family of quadratic energy equations: . \\[\\begin{equation} \\mathcal{E}^{\\theta}(\\mathbf{X}) = \\frac{1}{2} \\sum_{u}\\langle \\mathbf{x}_u, \\Omega \\mathbf{x}_u \\rangle - \\frac{1}{2} \\sum_{uv} \\bar{a}_{uv} \\langle \\mathbf{x}_u, \\mathbf{W} \\mathbf{x}_v \\rangle \\end{equation}\\] where we parametrize \\(\\Omega, \\mathbf{W}\\). The first term is external energy on all particles. The second term is pair-wise interactions along edges of the graph (internal energy). Minimizing the second term makes the each term \\(\\mathbf{x}_u, \\mathbf{x}_v\\) of adjacent nodes attract along the positive eigenvectors of \\(\\mathbf{W}\\). Negative eigenvectors repel. The gradient flow of this is given by the following (also assume \\(\\mathbf{W},\\Omega\\) are symmetric): . \\[\\begin{align} \\dot{\\mathbf{X}}(t) &amp;= -\\nabla \\mathcal{E}^{\\theta}(\\mathbf{X}(t)) \\\\ &amp;= -\\mathbf{X}(t) \\frac{(\\Omega + \\Omega^T)}{2} + \\tilde{A}\\mathbf{X}(t)\\frac{(\\mathbf{W} + \\mathbf{W}^T)}{2} \\\\ &amp;= -\\mathbf{X}(t)\\Omega + \\tilde{A}\\mathbf{X}(t)\\mathbf{W} \\end{align}\\] Then when we use forward-euler, we get the following: . \\[\\begin{equation} \\mathbf{X}(t + \\tau) = \\mathbf{X}(t) + \\tau \\left( -\\mathbf{X}(t)\\Omega + \\tilde{A}\\mathbf{X}(t)\\mathbf{W} \\right) \\end{equation}\\] Dominant Effects in dynamics . We can interpret the dynamics of the layer-system by analyzing the Dirichlet energy function of the system. Attractive forces minimise the edge gradients which produces smoothing effects (blurring) that magnify low frequency signals of the node features while repulsive forces increase edge gradients and produce a sharpening effect. We can analyze the Dirichlet energy function to see this by checking if in the limit \\(\\mathcal{E}^{\\text{DIR}}(\\frac{\\mathbf{X}(t)}{\\| \\mathbf{X}(t) \\|})\\) tends to 0. We would have low-frequency dominated (LFD) dynamics. In the case of convergence to the largest eigenvalue, we call the dynamics high-frequency dominated (HFD) dynamics. Many studies have shown LFD systems do well in node classification while HFD systems do well for heterophilic tasks. The previous works (GRAND, PDE-GCN-D, Continuous-GNN) are unable to induce HFD dynamics and will suffer in heterophilic cases. However, through our current modelling (named GRAFF), we can learn on heterophilic settings. ",
    "url": "/blog/graph-notes/gnn-grad-flow.html#graph-neural-networks-as-gradient-flows",
    
    "relUrl": "/graph-notes/gnn-grad-flow.html#graph-neural-networks-as-gradient-flows"
  },"94": {
    "doc": "GNN as Gradient Flow",
    "title": "GNN as Gradient Flow",
    "content": " ",
    "url": "/blog/graph-notes/gnn-grad-flow.html",
    
    "relUrl": "/graph-notes/gnn-grad-flow.html"
  },"95": {
    "doc": "GNNs as PDEs",
    "title": "Graph Neural Diffusion",
    "content": "This is based on Graph Neural Diffusion (GND). ",
    "url": "/blog/graph-notes/gnn-pde.html#graph-neural-diffusion",
    
    "relUrl": "/graph-notes/gnn-pde.html#graph-neural-diffusion"
  },"96": {
    "doc": "GNNs as PDEs",
    "title": "Preliminary on Diffusion",
    "content": "Isaac Newton anonymously published “A Scale of the Degrees of Heat” in 1701. He proposed that heat diffuses through a medium, and the rate of diffusion is proportional to the temperature gradient. A modern phrasing states, “the temperature a hot body loses in a given time is proportional to the temperature difference between the object and the environment”. This gives rise to the heat diffusion equation: . \\[\\begin{equation} \\dot x = \\alpha \\Delta x \\end{equation}\\] \\(x(u,t)\\) is the temperature at time t and point u. The \\(\\Delta\\) is the Laplacian operator. It expresses local difference between temperature of point and its surroundings. The \\(\\alpha\\) is the thermal diffusivity constant. This PDE is linear and has a closed form solution. The solution is given by the convolution of the initial condition with a Gaussian kernel: . \\[\\begin{equation} x(u,t) = x(u,0) \\text{exp}^{-\\frac{\\lvert u \\rvert}{4t}} \\end{equation}\\] A more general form of the diffusion equation (Fourier’s heat transfer law) is: . \\[\\begin{equation} \\dot x(u,t) = div(\\alpha(u,t) \\nabla x(u,t)) \\end{equation}\\] . ",
    "url": "/blog/graph-notes/gnn-pde.html#preliminary-on-diffusion",
    
    "relUrl": "/graph-notes/gnn-pde.html#preliminary-on-diffusion"
  },"97": {
    "doc": "GNNs as PDEs",
    "title": "Diffusion PDEs",
    "content": "Diffusion PDEs arise in physical processes that transfer energy/matter/information. An image processing perspective is to interpret diffusion as a linear low-pass filter for image denoising. However, this filter for image denoising gives a blurring effect on areas with high color gradient (contrast). The bilateral filtering paper from Jitendra Malik’s group proposed using an adaptive diffusivity coefficient inversely dependent on the norm of the image gradient. Diffusion is strong in “flat” regions and weak in high gradient regions (Perona-Malik diffusion). Perona-Malik diffusion created an entire field of PDE-based techniques that drew inspiration and methods from geometry, calculus of variations, and numerical analysis. Bronstein was inspired to do differential geometry because of Ron Kimmel’s work on numerical geometry of images. Variational and PDE-based methods were widely used until 2020s when deep learning took over. ",
    "url": "/blog/graph-notes/gnn-pde.html#diffusion-pdes",
    
    "relUrl": "/graph-notes/gnn-pde.html#diffusion-pdes"
  },"98": {
    "doc": "GNNs as PDEs",
    "title": "GRAND Takeaway",
    "content": "Bronstein’s work on GRAND (Grand Neural Diffusion) takes a similar philosophy. GNNs operate by exchanging information between nodes through message-passing. The message-passing process can be interpreted as a diffusion process on the graph. The diffusion processes to a graph \\(\\mathcal{G} = (\\mathcal{V}, \\mathcal{E})\\) looks like this: . \\[\\begin{equation} \\mathbf{\\dot X}(t) = div(\\mathbf{A}(\\mathbf{X}(t)) \\nabla \\mathbf{X}(t)) \\end{equation}\\] where \\(\\mathbf{X}(t)\\) is a \\(\\lvert \\mathcal{V} \\rvert \\times n_f\\) matrix of node features at time t. \\((\\nabla \\mathbf{X})_{uv} = \\mathbf{x}_v - \\mathbf{x}_u\\) is the gradient. The divergence is defined as \\((\\text{div}(\\mathbf{X}(t)))_u = \\sum_{v \\in \\mathcal{N}(u)} \\mathbf{w}_{uv} \\mathbf{x}_{uv}\\) where \\(\\mathbf{w}_{uv}\\) is the edge feature between nodes u and v. Diffusivity is defined as $$\\mathbf{A}(\\mathbf{X}(t)) = \\text{diag}(\\alpha(\\mathbf{x}_u, \\mathbf{x}_v)) . Now with all of this, the final graph diffusion equation is: . \\[\\begin{equation} \\dot{\\mathbf{X}}(t) = (\\mathbf{A}(\\mathbf{X}(t)) - \\mathbf{I})\\mathbf{X}(t) \\end{equation}\\] In most cases, this has no closed-form solution (needs to be solved numerically). ",
    "url": "/blog/graph-notes/gnn-pde.html#grand-takeaway",
    
    "relUrl": "/graph-notes/gnn-pde.html#grand-takeaway"
  },"99": {
    "doc": "GNNs as PDEs",
    "title": "Integration of PDE Equation",
    "content": "We can simply start a forward time difference discretization of the diffusion PDE: . \\[\\begin{align} \\frac{[\\mathbf{X}(k+1) - \\mathbf{X}(k)]}{\\tau} &amp;= (\\mathbf{A}(\\mathbf{X}(k)) - \\mathbf{I})\\mathbf{X}(k) \\\\ \\mathbf{X}(k+1) - \\mathbf{X}(k) &amp;= \\tau(\\mathbf{A}(\\mathbf{X}(k)) - \\mathbf{I})\\mathbf{X}(k) \\\\ \\mathbf{X}(k+1) &amp;= \\tau(\\mathbf{A}(\\mathbf{X}(k)) - \\mathbf{I})\\mathbf{X}(k) + \\mathbf{X}(k) \\\\ \\mathbf{X}(k+1) &amp;= \\tau(\\mathbf{A}(\\mathbf{X}(k)) - \\frac{1-\\tau}{\\tau}\\mathbf{I})\\mathbf{X}(k) \\\\ \\mathbf{X}(k+1) &amp;= \\mathbf{Q}(k)\\mathbf{X}(k) \\end{align}\\] If we tried the backward time difference discretization, we would get: . \\[\\begin{align} [(1+\\tau)\\mathbf{I} - \\tau \\mathbf{A}(\\mathbf{X}(k))]\\mathbf{X}(k+1) &amp;= \\mathbf{X}(k) \\\\ \\mathbf{B}(k)\\mathbf{X}(k+1) &amp;= \\mathbf{X}(k) \\\\ \\mathbf{X}(k+1) &amp;= \\mathbf{B}^{-1}(k)\\mathbf{X}(k) \\end{align}\\] This is a semi-implicit scheme where we have to solve the inverse of \\(\\mathbf{B}\\) to get the next state. These are overall not the best in practice. Using Runge-Kutta methods are better. However, if you don’t know about higher-order multi-step methods, then stick with the Euler methods. ",
    "url": "/blog/graph-notes/gnn-pde.html#integration-of-pde-equation",
    
    "relUrl": "/graph-notes/gnn-pde.html#integration-of-pde-equation"
  },"100": {
    "doc": "GNNs as PDEs",
    "title": "Connection to current GNNs",
    "content": "We can look at the GNN architectures as a discretized instance of Graph Diffusion. Equation 10 is just Graph Attention Transformer where \\(\\mathbf{A}\\) is the attention. These GNNs use explicit single-step Euler scheme. You can think of diffusion stepping as a message-passing step. Adaptive time-stepping can allow us to use fewer GNN layers and message passing steps if we consider the continuous interpretation of GNN message-passing. Additionally, graph rewiring (spawned from this work) sis popular to address over-smoothing or bottlenecks. The diffusion framework offers a principled view on graph rewiring by considering the graph as a spatial discretization of some continuous object. ",
    "url": "/blog/graph-notes/gnn-pde.html#connection-to-current-gnns",
    
    "relUrl": "/graph-notes/gnn-pde.html#connection-to-current-gnns"
  },"101": {
    "doc": "GNNs as PDEs",
    "title": "GRAND (paper)",
    "content": " ",
    "url": "/blog/graph-notes/gnn-pde.html#grand-paper",
    
    "relUrl": "/graph-notes/gnn-pde.html#grand-paper"
  },"102": {
    "doc": "GNNs as PDEs",
    "title": "GNNs as PDEs",
    "content": " ",
    "url": "/blog/graph-notes/gnn-pde.html",
    
    "relUrl": "/graph-notes/gnn-pde.html"
  },"103": {
    "doc": "Graph Rewiring",
    "title": "Graph Rewiring",
    "content": ". ",
    "url": "/blog/graph-notes/graph-rewiring.html",
    
    "relUrl": "/graph-notes/graph-rewiring.html"
  },"104": {
    "doc": "Graph Rewiring",
    "title": "Over-squashing",
    "content": "Over-squashing is defined as: . Distortion of information flowing from distant nodes as a factor limiting the efficiency of messsage passing for tasks relying on long-distance interactions. Lets consider the \\((\\ell+1)\\)-th layer of an MPNN: . \\[\\begin{equation} \\mathbf{h}_i^{(\\ell+1)} = \\phi_\\ell (h_i^{(\\ell)}, \\sum_{j=1}^n \\mathbf{\\hat A}_{ij} \\psi_\\ell (\\mathbf{h}_i^{(\\ell)}, \\mathbf{h}_j^{(\\ell)})) \\end{equation}\\] where \\(\\psi\\) is a message function, \\(\\mathbf{\\hat A}\\) is the adjacency matrix, and \\(\\phi\\) is a node update function. Additionally, \\(\\psi\\) is lipschitz-bounded by \\(\\alpha\\) and \\(\\phi\\) is lipschitz-bounded by \\(\\beta\\) at layer \\(\\ell\\). Lets use a simple lipschitz condition: . \\[\\begin{align} \\| \\nabla \\phi \\| \\leq \\alpha \\\\ \\| \\nabla \\psi \\| \\leq \\beta \\end{align}\\] Consider a node \\(s\\) that minimally takes \\(r\\)-hops to reach node \\(i\\). This can be expressed in \\(d_G(i,s) = r+1\\). Over-squashing is understood as node \\(i\\)’s representation failing to be affected by the input feature of node \\(s\\). In other words, we should observe the jacobian. \\[\\begin{equation} \\frac{\\partial \\mathbf{h}_i^{(r+1)}}{\\partial \\mathbf{x}_s} \\leq (\\alpha \\beta)^{(r+1)} (\\mathbf{\\hat A}^{(r+1)})_{is} \\end{equation}\\] Since \\(\\alpha\\) and \\(\\beta\\) are constants, it is actually \\(\\mathbf{\\hat A}\\) or the structure of the graph that affects the over-squashing or jacobian. NOTE: \\(\\mathbf{\\hat A}^{(r+1)}\\) means that we passed messages \\(r\\) times or did \\(r\\) hops. Structural properties of a graph that lead too over-squashing are called bottlenecks. The overall thing we want to do to tackle over-squashing is to use Ricci curvature to identify bottlenecks and then rewire the graph to remove them. ",
    "url": "/blog/graph-notes/graph-rewiring.html#over-squashing",
    
    "relUrl": "/graph-notes/graph-rewiring.html#over-squashing"
  },"105": {
    "doc": "Graph Rewiring",
    "title": "Ricci Flows and Curvature (shitty overview)",
    "content": "Sectional Curvature . Sectional curvature helps understand curvature of Riemmanian manifold. When we average sectional curvature over all directions, we get the Ricci curvature (average of sectional curvature over all facets). Ricci curvature can be thought of as a curvature that quantitatively measures how space is curved. Graphs and Ricci Curvature . Most of the details are in this paper. This paper uses Balanced Forman Curvature to calculate the Ricci curvature of a graph. Using this idea, we can rewire graphs. ",
    "url": "/blog/graph-notes/graph-rewiring.html#ricci-flows-and-curvature-shitty-overview",
    
    "relUrl": "/graph-notes/graph-rewiring.html#ricci-flows-and-curvature-shitty-overview"
  },"106": {
    "doc": "Graph Rewiring",
    "title": "Paper",
    "content": " ",
    "url": "/blog/graph-notes/graph-rewiring.html#paper",
    
    "relUrl": "/graph-notes/graph-rewiring.html#paper"
  },"107": {
    "doc": "Great Lakes Cluster",
    "title": "Great Lakes Cluster",
    "content": "In order to get access, you have to request HPC login credentials from the IT department of your institution. In Getting Started, we can find the HPC User login. This link allows you to request access to great lakes. Once you get access, you can login using your uofm uniqname+password combo. ",
    "url": "/blog/code-notes/cluster-notes/great-lakes.html",
    
    "relUrl": "/code-notes/cluster-notes/great-lakes.html"
  },"108": {
    "doc": "Great Lakes Cluster",
    "title": "Starting out",
    "content": ". | Get into UofM Network (via VPN or on-campus). | login via ssh | . ssh [uniqname]@greatlakes.arc-ts.umich.edu . You’ll find yourself in your home directory, which is located at /home/[uniqname]. Keep in mind that you are in the login node. This is a place to view job results and submit new jobs. It cannot be used to run application workloads. We will mainly run sbatch and srun to work. We can check if we can submit jobs by running the following command: . [andang@gl-login1 ~]$ hostname -s gl-login1 [andang@gl-login1 ~]$ srun --cpu-bind=none hostname -s srun: job 29842513 queued and waiting for resources srun: job 29842513 has been allocated resources gl3051 [andang@gl-login1 ~]$ . As you can see, srun is a fully blocking command. ",
    "url": "/blog/code-notes/cluster-notes/great-lakes.html#starting-out",
    
    "relUrl": "/code-notes/cluster-notes/great-lakes.html#starting-out"
  },"109": {
    "doc": "Great Lakes Cluster",
    "title": "Loading modules",
    "content": "There is a list of modules available to load in slurm. Here are a list of commands we can use to work with modules: . module list # list all loaded modules module avail # list all available modules module load [module_name] # load a module module unload [module_name] # unload a module module purge # unload all modules module unload [module_name] # unload a specific module module spider # list all possible modules module whatis [module_name] # show information about a specific module module save [module_name] # save the current module state . Useful modules on Great Lakes include: . module load python #python3.13 module load mamba # smaller version of anaconda... also loads python, can't load python and mamba at the same time module load matlab module load gurobi module load julia module load cmake module load gcc module load git module load tmux module load ffmpeg module load cuda module load cudnn module load tensorflow module load tensorrt module load code-server . These are taken from the documentation on the man page. ",
    "url": "/blog/code-notes/cluster-notes/great-lakes.html#loading-modules",
    
    "relUrl": "/code-notes/cluster-notes/great-lakes.html#loading-modules"
  },"110": {
    "doc": "Great Lakes Cluster",
    "title": "Inside a node",
    "content": "We can mess around with a node by using salloc to allocate resources interactively. salloc --account=test . Once inside, we can access the /tmp directory which is unique for each node. We can check that it is specific to the node by running: . salloc --account=test touch /tmp/hello.txt cat /tmp/hello.txt exit cat /tmp/hello.txt . And we’ll notice that once outside, the file is not there anymore. This is because /tmp is a temporary directory that is unique to each node and is cleared when the node is rebooted or when the job ends. This is a good spot to store temporary files that you don’t need to keep after the job ends. NOTE: /home directory has a hard limit of 80 GB. ",
    "url": "/blog/code-notes/cluster-notes/great-lakes.html#inside-a-node",
    
    "relUrl": "/code-notes/cluster-notes/great-lakes.html#inside-a-node"
  },"111": {
    "doc": "Great Lakes Cluster",
    "title": "Creating a batch job",
    "content": "#!/bin/bash # COMMENT: The interpreter used to execute the script # COMMENT: #SBATCH directives that convey submission options: #SBATCH --job-name=example_job #SBATCH --mail-user=[uniqname]@umich.edu #SBATCH --mail-type=BEGIN,END #SBATCH --cpus-per-task=1 #SBATCH --nodes=1 #SBATCH --ntasks-per-node=1 #SBATCH --mem-per-cpu=1000m #SBATCH --time=10:00 #SBATCH --account=test #SBATCH --partition=standard #SBATCH --output=/home/%u/%x-%j.log # COMMENT: The application(s) to execute along with its input arguments and options: &lt;insert commands here&gt; . ",
    "url": "/blog/code-notes/cluster-notes/great-lakes.html#creating-a-batch-job",
    
    "relUrl": "/code-notes/cluster-notes/great-lakes.html#creating-a-batch-job"
  },"112": {
    "doc": "Great Lakes Cluster",
    "title": "One Node, One Processor",
    "content": "#!/bin/bash #SBATCH --job-name JOBNAME #SBATCH --nodes=1 #SBATCH --cpus-per-task=1 #SBATCH --mem-per-cpu=1g #SBATCH --time=00:15:00 #SBATCH --account=test #SBATCH --partition=standard #SBATCH --mail-type=NONE # COMMENT:The application(s) to execute along with its input arguments and options: echo \"Hello World\" srun --cpu-bind=none hostname -s . This allocates one node with one processor, 1 GB of memory, and a time limit of 15 minutes. We print “Hello World” to the console. Output: . Hello World srun: job 29838616 queued and waiting for resources srun: job 29838616 has been allocated resources gl3018 . ",
    "url": "/blog/code-notes/cluster-notes/great-lakes.html#one-node-one-processor",
    
    "relUrl": "/code-notes/cluster-notes/great-lakes.html#one-node-one-processor"
  },"113": {
    "doc": "Great Lakes Cluster",
    "title": "One Node, GPU",
    "content": "#!/bin/bash #SBATCH --job-name test_gpu #SBATCH --account=test #SBATCH --nodes=1 #SBATCH --time=00:15:00 #SBATCH --cpus-per-task=4 #SBATCH --mem-per-cpu=1g #SBATCH --partition=gpu #SBATCH --gres=gpu:1 --gpus=1 #SBATCH --mail-type=NONE #SBATCH --output=/home/%u/workspace/%x-%j.out # COMMENT: The application(s) to execute along with its input arguments and options: module load mamba cuda cudnn python hello.py nvidia-smi . Using this, we can allocate a gpu node and run nvidia-smi to check the gpu status. This all works. ",
    "url": "/blog/code-notes/cluster-notes/great-lakes.html#one-node-gpu",
    
    "relUrl": "/code-notes/cluster-notes/great-lakes.html#one-node-gpu"
  },"114": {
    "doc": "Great Lakes Cluster",
    "title": "Interactive jobs",
    "content": "We can run salloc to do interafctive job. [user@gl-login1 ~]$ salloc --account=test salloc: Pending job allocation 10081688 salloc: job 10081688 queued and waiting for resources salloc: job 10081688 has been allocated resources salloc: Granted job allocation 10081688 salloc: Waiting for resource configuration salloc: Nodes gl3052 are ready for job [user@gl3052 ~]$ hostname gl3052.arc-ts.umich.edu [user@gl3052 ~]$ . ",
    "url": "/blog/code-notes/cluster-notes/great-lakes.html#interactive-jobs",
    
    "relUrl": "/code-notes/cluster-notes/great-lakes.html#interactive-jobs"
  },"115": {
    "doc": "Great Lakes Cluster",
    "title": "Going on interactive mode",
    "content": "Check out this link for more information on how to do things interactively on Great Lakes. ",
    "url": "/blog/code-notes/cluster-notes/great-lakes.html#going-on-interactive-mode",
    
    "relUrl": "/code-notes/cluster-notes/great-lakes.html#going-on-interactive-mode"
  },"116": {
    "doc": "Great Lakes Cluster",
    "title": "Scratch directory",
    "content": "Scratch directory is maintained by the account you are under (not the user, but the SLURM account which is typically from the PI). It is located at /scratch/[account_name]/[subaccount_name]/[uniqname]. We can check the usage of the scratch directory by running: . scratch-quota [scratch_directory] . We can also check user usage of /home directory by running: . home-quota . ",
    "url": "/blog/code-notes/cluster-notes/great-lakes.html#scratch-directory",
    
    "relUrl": "/code-notes/cluster-notes/great-lakes.html#scratch-directory"
  },"117": {
    "doc": "Great Lakes Cluster",
    "title": "Turbo directory",
    "content": "For great lakes, we can access this through /nfs/turbo/[turbo_account_name]/. Use this to store large files for a long time (doesn’t automatically scrap like in /scratch). We can also mount turbo directories. This is how you do it with sshfs: . sudo apt-get install sshfs sudo modprobe fuse mkdir ~/remote_dir sshfs username@greatlakes.arc-ts.umich.edu:/nfs/turbo/[account_name]/[uniqname]/something ~/remote_dir . To unmount you can use: . fusermount -u ~/remote_dir # hasn't worked for me sudo umount -l ~/remote_dir # this works consistently . ",
    "url": "/blog/code-notes/cluster-notes/great-lakes.html#turbo-directory",
    
    "relUrl": "/code-notes/cluster-notes/great-lakes.html#turbo-directory"
  },"118": {
    "doc": "GDL 1: Groups",
    "title": "GDL 1: Groups, Representation, and Equivariant Maps",
    "content": " ",
    "url": "/blog/prereq-notes/groups.html#gdl-1-groups-representation-and-equivariant-maps",
    
    "relUrl": "/prereq-notes/groups.html#gdl-1-groups-representation-and-equivariant-maps"
  },"119": {
    "doc": "GDL 1: Groups",
    "title": "Symmetry Groups",
    "content": "Example: 2D Rotation Group \\(\\text{SO}(2)\\) . | \\[\\{\\frac{\\pi}{4}, \\frac{\\pi}{2}, \\frac{3\\pi}{2}, \\pi, ...\\}\\] | these rotations can be composed | there is identity rotation | each rotation has an inverse | the compositions are associative | . Definition B.1 (Group) A group is a tuple \\((G,\\cdot)\\), consisting of a set \\(G\\) and a binary operation \\(\\cdot : G \\times G \\to G, (g,h) \\mapsto g \\cdot h\\) which satisfies the following three group axioms: . \\[\\begin{align*} \\text{associativity:} \\space &amp; \\forall g,h,k \\in G, (g \\cdot h) \\cdot k = g \\cdot (h \\cdot k) \\\\ \\text{identity:} \\space &amp; \\exists e \\in G, \\forall g \\in G, g \\cdot e = g = e \\cdot g \\\\ \\text{inverse:} \\space &amp; \\forall g \\in G, \\exists g^{-1} \\in G, g \\cdot g^{-1} = e = g^{-1} \\cdot g \\end{align*}\\] Examples: . | translation group: \\((\\mathbb{R}^d, +)\\) | unitary group: \\(U(1) := \\{ e^{i\\phi} \\mid \\phi \\in [0, 2\\pi) \\}\\) | general linear group: \\(GL(d) := \\{ g \\in \\mathbb{R}^{d \\times d} \\mid det(g) \\neq 0 \\}\\) | trivial group: { e } | . Counterexamples: . | cex1: \\(\\{e^{i\\phi} \\mid \\phi \\in [0, \\pi) \\}\\) . | closure: \\(e^{i\\phi_1}e^{i\\phi_2} = e^{i(\\phi_1 + \\phi_2)}, \\phi_1 = \\phi_2 = \\frac{\\pi}{2}\\) | . | cex2: \\(\\{ g \\in \\mathbb{R}^{d \\times d} \\}\\) . | inverse: not all matrices have inverses | . | . Groups may come with additional structure: . | Topological group: . | structure on G: topology | binary operation: continuous map | . | Lie Group: . | structure on G: smooth manifold | binary operation: smooth map | . | Finite Group: . | structure on G: finite set | binary operation: any function between finite sets | . | . ",
    "url": "/blog/prereq-notes/groups.html#symmetry-groups",
    
    "relUrl": "/prereq-notes/groups.html#symmetry-groups"
  },"120": {
    "doc": "GDL 1: Groups",
    "title": "Abelian Groups",
    "content": "NOTE we do not assume commutativity in the definition of a group. Definition B.2 (Abelian Group) A group \\((G,\\cdot)\\) is called abelian if \\(\\forall g,h \\in G, g \\cdot h = h \\cdot g\\) . ",
    "url": "/blog/prereq-notes/groups.html#abelian-groups",
    
    "relUrl": "/prereq-notes/groups.html#abelian-groups"
  },"121": {
    "doc": "GDL 1: Groups",
    "title": "Subgroups",
    "content": "Definition B.3 (Subgroup) A subset \\(H \\subseteq G\\) is called a subgroup of \\(G\\) if \\(H\\) is closed under composition and inverses: . | composition: \\(\\forall g,h \\in H\\) one has \\(gh \\in H\\) | inverse: \\(\\forall g \\in H\\) one has \\(g^{-1} \\in H\\) | . Subgroups are themselves also groups. Usual notation is \\(H \\leq G\\). Examples: . | discrete translations: \\((\\mathbb{Z}^d, +) \\leq (\\mathbb{R}^d, +)\\) | rotations: \\(\\text{SO}(2) \\leq \\text{SO}(3)\\) | trivial: \\(\\{e \\} \\leq G\\) | . ",
    "url": "/blog/prereq-notes/groups.html#subgroups",
    
    "relUrl": "/prereq-notes/groups.html#subgroups"
  },"122": {
    "doc": "GDL 1: Groups",
    "title": "Product of Groups",
    "content": "Definition B.4 (Product of Groups) Let \\((G_1, \\cdot_1)\\) and \\((G_2, \\cdot_2)\\) be two groups. The product group \\((G_1 \\times G_2, \\cdot)\\) is defined as follows: . \\[\\begin{equation*} G_1 \\times G_2 \\to G_1 \\times G_2, \\quad ((g_1,g_2), (h_1,h_2)) \\mapsto (g_1 \\cdot_1 h_1, g_2 \\cdot_2 h_2) \\end{equation*}\\] Example: . | Cylinder Symmetry Group: \\(\\text{SO}(2) \\times (\\mathbb{R}, +)\\) | . Counterexample: . | 2D Rigid Transform: \\(\\text{SO}(2) \\times (\\mathbb{R}^2,+)\\) . | this is a semi-direct product, not a direct product | . | . ",
    "url": "/blog/prereq-notes/groups.html#product-of-groups",
    
    "relUrl": "/prereq-notes/groups.html#product-of-groups"
  },"123": {
    "doc": "GDL 1: Groups",
    "title": "Group Homomorphisms",
    "content": "Definition B.5 (Group Homomorphism) A group homomorphism \\(\\phi: G_1 \\to G_2\\) is a map between two groups \\((G_1, \\cdot_1)\\) and \\((G_2, \\cdot_2)\\) such that \\(\\forall g,h \\in G_1\\) one has \\(\\phi(g \\cdot_1 h) = \\phi(g) \\cdot_2 \\phi(h)\\) . This implies that \\(\\phi(g^{-1}) = \\phi(g)^{-1}\\) and that \\(g(e_{G_1}) = e_{G_2}\\). Example: . | complex exponentiation: \\(\\text{exp}(i(\\cdot)): \\quad (\\mathbb{R}, +) \\mapsto U(1)\\), \\(x \\mapsto e^{ix}\\) | . We can lose information about a group. as seen with complex exponentiation. It wraps a real number along a circle. ",
    "url": "/blog/prereq-notes/groups.html#group-homomorphisms",
    
    "relUrl": "/prereq-notes/groups.html#group-homomorphisms"
  },"124": {
    "doc": "GDL 1: Groups",
    "title": "Group isomorphisms",
    "content": "Definition B.6 (Group Isomorphism) A group isomorphism \\(\\phi: G_1 \\to G_2\\) is a bijective group homomorphism. As in, it is an invertible group homomorphism. ",
    "url": "/blog/prereq-notes/groups.html#group-isomorphisms",
    
    "relUrl": "/prereq-notes/groups.html#group-isomorphisms"
  },"125": {
    "doc": "GDL 1: Groups",
    "title": "Group Actions",
    "content": "Definition B.7 (Group Action) A left group action of a group \\((G, \\cdot)\\) on a set \\(X\\) is a map \\(\\star: G \\times X \\to X\\) such that: . | associativity: \\((g \\cdot h) \\star x = g \\star (h \\star x)\\) \\(\\forall g,h \\in G\\), \\(x \\in X\\) | identity: \\(e \\star x = x\\), \\(\\forall x \\in X\\) | . ",
    "url": "/blog/prereq-notes/groups.html#group-actions",
    
    "relUrl": "/prereq-notes/groups.html#group-actions"
  },"126": {
    "doc": "GDL 1: Groups",
    "title": "Group orbit",
    "content": "Definition B.8 (Group Orbit) The orbit of an element \\(x \\in X\\) under the left action \\(\\star\\) of \\(G\\) is the set: . \\[\\begin{equation*} G \\star x = \\{ g \\star x \\mid g \\in G \\} \\end{equation*}\\] The orbit is different depending on which element we are operating on. For example, Let \\(G = \\text{SO}(2)\\) and \\(X = \\mathbb{R}^2\\), then the orbit of \\(x = (1,0)\\) is the circle of radius 1. The orbit of \\(x = (0,1)\\) is also the circle of radius 1. The orbit of \\(x = (2,0)\\) is the circle of radius 2. Equivalence Relation . “being in the same orbit” defines an equivalence relation. | relfexivity: \\(x \\sim_\\star x\\) that is, x is contained in its own orbit \\(G \\star x\\). | symmetry: \\(x \\sim_\\star y \\iff y \\sim_\\star x\\) | transitivity: \\(x \\sim_\\star y \\land y \\sim_\\star z \\implies x \\sim_\\star z\\) . | if \\(x\\) is in the same orbit as \\(y\\) and \\(y\\) is in the same orbit as \\(z\\), then \\(x\\) is in the same orbit as \\(z\\). | . | . These 3 properties define an equivalence relation. ",
    "url": "/blog/prereq-notes/groups.html#group-orbit",
    
    "relUrl": "/prereq-notes/groups.html#group-orbit"
  },"127": {
    "doc": "GDL 1: Groups",
    "title": "Quotient Set andd Quotient Map",
    "content": "Definition B.9 (Quotient Set) The quotient set induced by G-action \\(\\star\\) on \\(X\\) is the set of all orbits: . \\[\\begin{equation*} G \\backslash X = \\{ G \\star x \\mid x \\in X \\} \\end{equation*}\\] The corresponding quotient map collapses elements of \\(X\\) into their orbit: . \\[\\begin{equation*} q_\\star: X \\to G \\backslash X, \\quad x \\mapsto G \\star x \\end{equation*}\\] ",
    "url": "/blog/prereq-notes/groups.html#quotient-set-andd-quotient-map",
    
    "relUrl": "/prereq-notes/groups.html#quotient-set-andd-quotient-map"
  },"128": {
    "doc": "GDL 1: Groups",
    "title": "Transitive Action / Homogeneous Space",
    "content": "Definition B.10 (Transitive Action) A group action \\(\\star\\) is called transitive if \\(\\forall x,y \\in X\\) there exists \\(g \\in G\\) such that \\(g \\star x = y\\). \\(X\\) is then called a homogeneous space. ",
    "url": "/blog/prereq-notes/groups.html#transitive-action--homogeneous-space",
    
    "relUrl": "/prereq-notes/groups.html#transitive-action--homogeneous-space"
  },"129": {
    "doc": "GDL 1: Groups",
    "title": "Stabilizer Subgroup",
    "content": "Definition B.11 (Stabilizer Subgroup) Let \\(\\star\\) be a G-action on a set \\(X\\). The stabilizer subgroup of \\(x \\in X\\) is defined as: . \\[\\begin{equation*} \\text{Stab}_x := \\{ g \\in G \\mid g \\star x = x \\} \\leq G \\end{equation*}\\] ",
    "url": "/blog/prereq-notes/groups.html#stabilizer-subgroup",
    
    "relUrl": "/prereq-notes/groups.html#stabilizer-subgroup"
  },"130": {
    "doc": "GDL 1: Groups",
    "title": "Invariant Map",
    "content": "Definition B.12 (Invariant Map) A map \\(f: X \\to Y\\) is called invariant under the action \\(\\star\\) if \\(\\forall g \\in G, x \\in X\\) one has \\(f(g \\star x) = f(x)\\). Invariant maps “descent to the quotient”. i.e. for any G-invariant map \\(L: X \\to Y\\), there exists a unique map \\(\\tilde{L}: G \\backslash X \\to Y\\) such that \\(L = \\tilde{L} \\circ q_\\star\\). ",
    "url": "/blog/prereq-notes/groups.html#invariant-map",
    
    "relUrl": "/prereq-notes/groups.html#invariant-map"
  },"131": {
    "doc": "GDL 1: Groups",
    "title": "Equivariant Maps",
    "content": "Definition B.13 (Equivariant Map) A map \\(f: X \\to Y\\) is called equivariant under the action \\(\\star\\) if \\(\\forall g \\in G, x \\in X\\) one has \\(f(g \\star_X x) = g \\star_Y f(x)\\). Most popular example is convolutions: . \\[\\begin{align*} K*: L^2(\\mathbb{R}) \\to L^2(\\mathbb{R}), f \\mapsto K * f := \\int_\\mathbb{R} K(x-y)f(y)dy \\end{align*}\\] \\[\\begin{align*} (K * (g \\star f))(x) &amp;= \\int_\\mathbb{R}K(x-y)(g \\star f)(y) dy \\\\ &amp;= \\int_\\mathbb{R} K(x-y)f(y-g)dy \\\\ &amp;= \\int_\\mathbb{R} K((x-g)-z)f(z)dz \\\\ &amp;= (K * f)(x-g) \\\\ &amp;= (g \\star (K * f) )(x) \\end{align*}\\] So now we know that convolutions are translation equivariant. ",
    "url": "/blog/prereq-notes/groups.html#equivariant-maps",
    
    "relUrl": "/prereq-notes/groups.html#equivariant-maps"
  },"132": {
    "doc": "GDL 1: Groups",
    "title": "Group Representation Theory",
    "content": "Definition B.14 (Linear group representation) A linear group representation of a group \\(G\\) on a real vector space \\(\\mathbb{R}^N\\) is a group homomorphism . \\[\\rho: G \\to GL(R^N)\\] Easy to think as way of mapping from group element to a matrix. More topics on this area: . | Tensor Representations | Intertwiner | Irreducible Representations | Isomorphic Representations | Schur’s Lemma | Complete reducibility of unitary representations | Clebsch-Gordan Decomposition | Peter-Weyl Theorem and Fourier Transforms | . ",
    "url": "/blog/prereq-notes/groups.html#group-representation-theory",
    
    "relUrl": "/prereq-notes/groups.html#group-representation-theory"
  },"133": {
    "doc": "GDL 1: Groups",
    "title": "GDL 1: Groups",
    "content": " ",
    "url": "/blog/prereq-notes/groups.html",
    
    "relUrl": "/prereq-notes/groups.html"
  },"134": {
    "doc": "IsaacGymEnv Assets Notes",
    "title": "5. Understanding Assets in IsaacGymEnv",
    "content": "REMEMBER: . | We can only load URDF and MJCF files in IsaacGym. | The way that isaacgym loads assets is through gym.load_asset. | . We load plug asset and socket asset in TacSLEnvInsertion which inherits from TacSLBase. In TacSLBase, we load the franka asset and table asset. ",
    "url": "/blog/code-notes/isaac/gymenv-assets.html#5-understanding-assets-in-isaacgymenv",
    
    "relUrl": "/code-notes/isaac/gymenv-assets.html#5-understanding-assets-in-isaacgymenv"
  },"135": {
    "doc": "IsaacGymEnv Assets Notes",
    "title": "5.1 Plug and Socket Assets",
    "content": "The plug and socket assets are loaded in TacSLEnvInsertion._import_env_assets function. This function takes in a desired subassembly we specified such as the following: . | round_peg_hole_4mm | round_peg_hole_8mm | round_peg_hole_12mm and so on. | . When the code is creating the environments for the first time, it can use the choices of subassemblies to randomize what assets an environment will have. These assets do not change during the course of the process lifetime. NOTE: It seems that the TacSLEnvInsertion and TacSLTaskInsertion classes have fundamentally hard-coded the plug/socket asset paradigm into their design. This means we cannot support multi-object insertion feature into this without significant change to these classes. It’s much better fundamentally to create our own environment and our own task based on that environment. These assets are known in TacSLEnvInsertion. The yaml file containing the urdf paths for these assets are hard-coded in self.asset_info_insertion to a particular path. IsaacGymEnvs/assets/tacsl/yaml/industreal_asset_info_pegs.yaml is the path. ",
    "url": "/blog/code-notes/isaac/gymenv-assets.html#51-plug-and-socket-assets",
    
    "relUrl": "/code-notes/isaac/gymenv-assets.html#51-plug-and-socket-assets"
  },"136": {
    "doc": "IsaacGymEnv Assets Notes",
    "title": "IsaacGymEnv Assets Notes",
    "content": " ",
    "url": "/blog/code-notes/isaac/gymenv-assets.html",
    
    "relUrl": "/code-notes/isaac/gymenv-assets.html"
  },"137": {
    "doc": "IsaacGymEnv Camera Notes",
    "title": "2. Understanding how cameras work in IsaacGymEnv",
    "content": " ",
    "url": "/blog/code-notes/isaac/gymenv-cam.html#2-understanding-how-cameras-work-in-isaacgymenv",
    
    "relUrl": "/code-notes/isaac/gymenv-cam.html#2-understanding-how-cameras-work-in-isaacgymenv"
  },"138": {
    "doc": "IsaacGymEnv Camera Notes",
    "title": "2.1 Camera creation and access",
    "content": "Cameras are created in isaacgym using gym.create_camera_sensor. where gym = gymapi.acquire_gym(). This is embedded into isaacgymenvs/tacsl_sensors/tacsl_sensors.py in CameraSensor.setup_env_cameras. def setup_env_cameras(self, env_ptr, camera_spec_dict): \"\"\" Set up environment cameras. Args: env_ptr: Pointer to the environment. camera_spec_dict (dict): Dictionary of camera specifications. Returns: dict: Dictionary of camera handles. \"\"\" camera_handles = {} for name, camera_spec in camera_spec_dict.items(): camera_properties = gymapi.CameraProperties() ... # setup camera_properties using camera_spec camera_handle = self.isaac_gym.create_camera_sensor(env_ptr, camera_properties) camera_handles[name] = camera_handle print(f'Created camera {name} with handle {camera_handle} for env {env_ptr}') if camera_spec.is_body_camera: ... # extra handling if camera is placed on a rigid body that moves else: transform = gymapi.Transform(gymapi.Vec3(*camera_spec.camera_pose[0]), gymapi.Quat(*camera_spec.camera_pose[1])) self.isaac_gym.set_camera_transform(camera_handle, env_ptr, transform) # sets camera transform in the environment # Return a dictionary of camera handles for each camera return camera_handles . Now the real question is how do you get camera_spec_dict? And what is inside of camera_spec? . We understand this by looking at calls of setup_env_cameras. This is also located in CameraSensors.create_camera_actors: . class CameraSensor:/home/andang/workspace/isaac/IsaacGymEnvs/isaacgymenvs/camera_fused.png def create_camera_actors(self, camera_spec_dict, env_idx): ... env_camera_handles = self.setup_env_cameras(env_ptr, camera_spec_dict) ... and create_camera_actors is called in tacsl_env_insertion at _create_sensors: . def _create_sensors(self): self.camera_spec_dict = dict() self.camera_handles_list = [] self.camera_tensors_list = [] if self.cfg_task.env.use_isaac_gym_tactile: tactile_sensor_configs = self._compose_tactile_image_configs() self.set_compliant_dynamics_for_tactile_sensors(tactile_sensor_configs) camera_spec_dict_tactile = self.get_tactile_rgb_camera_configs(tactile_sensor_configs) self.camera_spec_dict.update(camera_spec_dict_tactile) if self.cfg_task.env.use_camera: camera_spec_dict = self.get_regular_camera_specs() self.camera_spec_dict.update(camera_spec_dict) if self.camera_spec_dict: # tactile cameras created along with other cameras in create_camera_actors camera_handles_list, camera_tensors_list = self.create_camera_actors(self.camera_spec_dict) self.camera_handles_list += camera_handles_list self.camera_tensors_list += camera_tensors_list ... # just more code for shear stuff . We see that self.camera_spec_dict is constructed by adding two dicts from self.get_regular_camera_specs() and self.get_tactile_rgb_camera_configs(tactile_sensor_configs). Since I am more concerned with visual cameras and not tactile cameras, I will focus on get_regular_camera_specs. class TacSLSensors(TactileFieldSensor, TactileRGBSensor, CameraSensor): def get_regular_camera_specs(self): camera_spec_dict = {} if self.cfg_task.env.use_camera: camera_spec_dict = {c_cfg.name: c_cfg for c_cfg in self.cfg_task.env.camera_configs} return camera_spec_dict . I used pdb.set_trace() to figure out what default cameras are set up for tacsl. The default camera configs defined by camera_spec_dict are shown as: . { 'wrist': { 'name': 'wrist', 'is_body_camera': True, 'actor_name': 'franka', 'attach_link_name': 'panda_hand', 'image_size': [1024, 1024], 'image_type': 'rgb', 'horizontal_fov': 75.0, 'camera_pose': [[0.045, 0, 0.04], [0.5, 0, 0.866, 0]] }, 'wrist_2': { 'name': 'wrist_2', 'is_body_camera': True, 'actor_name': 'franka', 'attach_link_name': 'panda_hand', 'image_size': [1024, 1024], 'image_type': 'rgb', 'horizontal_fov': 75.0, 'camera_pose': [[-0.045, 0, 0.04], [0.866, 0, 0.5, 0]] }, 'front': { 'name': 'front', 'is_body_camera': False, 'actor_name': None, 'attach_link_name': None, 'image_size': [1024, 1024], 'image_type': 'rgb', 'horizontal_fov': 75.0, 'camera_pose': [[0.8, 0.0, 0.5], [-0.258819045, 1.58480958e-17, 0.965925826, 5.91458986e-17]] }, 'side': { 'name': 'side', 'is_body_camera': False, 'actor_name': None, 'attach_link_name': None, 'image_size': [1024, 1024], 'image_type': 'rgb', 'horizontal_fov': 75.0, 'camera_pose': [[0.3, -0.6797, 0.7099], [-0.1830127, 0.1830127, 0.6830127, 0.6830127]] } } . As we can see, there are 4 cameras. There are 2 wrist cameras and 2 external cameras. I’ll visualize what these cameras output using opencv. By the way, these camera configs are located in isaacgymenvs/cfg/task/TacSLTaskInsertion.yaml. We can add cameras and modify there. Note that the current setup uses only RGB. ",
    "url": "/blog/code-notes/isaac/gymenv-cam.html#21-camera-creation-and-access",
    
    "relUrl": "/code-notes/isaac/gymenv-cam.html#21-camera-creation-and-access"
  },"139": {
    "doc": "IsaacGymEnv Camera Notes",
    "title": "2.2 Getting Object Point Clouds from Cameras",
    "content": "There is no code that uses depth in this codebase, I’m going to change that. First step is to add depth image and segmentation cameras. I modify isaacgymenvs/cfg/task/TacSLTaskInsertion.yaml to add depth and segmentation cameras. Here’s an example: . - name: wrist is_body_camera: True actor_name: franka attach_link_name: panda_hand image_size: [1024, 1024] image_type: all horizontal_fov: 75.0 camera_pose: [[0.045, 0, 0.04], [0.5, 0, 0.866, 0]] . We change image_type to all to get RGB, depth, and segmentation images. This now means we have to modify isaacgymenvs/tacsl_sensors/tacsl_sensors.py to handle the new image types. We start off with create_tensors_for_env_cameras in CameraSensor: . def create_tensors_for_env_cameras(self, env_ptr, env_camera_handles, camera_spec_dict): env_camera_tensors = {} for name in env_camera_handles: camera_handle = env_camera_handles[name] ... elif camera_spec_dict[name].image_type == 'all': # obtain camera tensor camera_tensor = self.gym.get_camera_image_gpu_tensor(self.sim, env_ptr, camera_handle, gymapi.IMAGE_COLOR) camera_tensor_depth = self.gym.get_camera_image_gpu_tensor(self.sim, env_ptr, camera_handle, gymapi.IMAGE_DEPTH) camera_tensor_seg = self.gym.get_camera_image_gpu_tensor(self.sim, env_ptr, camera_handle, gymapi.IMAGE_SEGMENTATION) torch_camera_tensor = gymtorch.wrap_tensor(camera_tensor) torch_camera_tensor_depth = gymtorch.wrap_tensor(camera_tensor_depth) torch_camera_tensor_seg = gymtorch.wrap_tensor(camera_tensor_seg) env_camera_tensors[name] = torch_camera_tensor env_camera_tensors[name + '_depth'] = torch_camera_tensor_depth env_camera_tensors[name + '_seg'] = torch_camera_tensor_seg continue ... return env_camera_tensors . We can still keep env_camera_tensors structure mostly the same but just add extra keys for depth and segmentation. Next up, we modify get_camera_image_tensors_dict in CameraSensor to handle the new image types: . def get_camera_image_tensors_dict(self): \"\"\" Get the dictionary of camera image tensors. Returns: dict: Dictionary of camera image tensors. \"\"\" # transforms and information must be communicated from the physics simulation into the graphics system if self.device != 'cpu': self.gym.fetch_results(self.sim, True) self.gym.step_graphics(self.sim) self.gym.render_all_camera_sensors(self.sim) self.gym.start_access_image_tensors(self.sim) camera_image_tensors_dict = dict() for name in self.camera_spec_dict: camera_spec = self.camera_spec_dict[name] ... # other cases for handling image types elif camera_spec['image_type'] == 'all': ## RGB save num_channels = 3 camera_images = torch.zeros( (self.num_envs, camera_spec.image_size[0], camera_spec.image_size[1], num_channels), device=self.device, dtype=torch.uint8) for id in np.arange(self.num_envs): camera_images[id] = self.camera_tensors_list[id][name][:, :, :num_channels].clone() camera_image_tensors_dict[name] = camera_images ## Depth save num_channels = 1 camera_images = torch.zeros( (self.num_envs, camera_spec.image_size[0], camera_spec.image_size[1]), device=self.device, dtype=torch.float) for id in np.arange(self.num_envs): # Note that isaac gym returns negative depth # See: https://carbon-gym.gitlab-master-pages.nvidia.com/carbgym/graphics.html?highlight=image_depth#camera-image-types camera_images[id] = self.camera_tensors_list[id][name][:, :].clone() * -1. camera_images[id][camera_images[id] == np.inf] = 0.0 camera_image_tensors_dict[name + '_depth'] = camera_images ## Segmentation save num_channels = 1 camera_images = torch.zeros( (self.num_envs, camera_spec.image_size[0], camera_spec.image_size[1]), device=self.device, dtype=torch.int32) for id in np.arange(self.num_envs): camera_images[id] = self.camera_tensors_list[id][name + '_seg'][:, :].clone() camera_image_tensors_dict[name + '_seg'] = camera_images continue ... # other cases for handling image types return camera_image_tensors_dict . We add the depth and segmentation images to the camera_image_tensors_dict dictionary. This immediately translates to us getting depth and segmentation images from the cameras when we call it in our observation function. Here’s a visual of normalized depth: . Our segmentation images will just be zero at this point without further modification. We need to add self.gym.set_rigid_body_segmentation_id(env_ptr, plug_handle, 0, 1) in tacsl_env_insertion.py to set what should be segmented. With this, we can visualize the segmentation: . We also make sure self.image_obs_keys in tacsl_task_insertion.py does not include the new depth and segmentation keys by setting the following in TacSLTaskInsertion.__init__: . self.image_obs_keys = [k for k, v in self.obs_dims.items() if len(v) &gt; 2 and 'force_field' not in k and not k.endswith('_depth') and not k.endswith('_seg')] . ** There was a lot of pain involved in getting intrinsics/extrinsics to work. ** . | IsaacGym does not support intrinsic camera parameters! . | We had to calculate our own intrinsics from projection matrix. | Their camera convention is messed up. Camera needs extra arbitrary rotations and flips to get the right orientation. | . | Getting point clouds was a huge hassle in the codebase. | Using only segmentations currently. | . ",
    "url": "/blog/code-notes/isaac/gymenv-cam.html#22-getting-object-point-clouds-from-cameras",
    
    "relUrl": "/code-notes/isaac/gymenv-cam.html#22-getting-object-point-clouds-from-cameras"
  },"140": {
    "doc": "IsaacGymEnv Camera Notes",
    "title": "IsaacGymEnv Camera Notes",
    "content": " ",
    "url": "/blog/code-notes/isaac/gymenv-cam.html",
    
    "relUrl": "/code-notes/isaac/gymenv-cam.html"
  },"141": {
    "doc": "IsaacGymEnv Env Notes",
    "title": "6. Creating an Env in IsaacGymEnv",
    "content": "In order to understand how to create an environment in IsaacGymEnv, we take a look at the Abstract Base Class (ABC) in IsaacGymEnv. class FactoryABCEnv(ABC): @abstractmethod def __init__(self): \"\"\"Initialize instance variables. Initialize base superclass. Acquire tensors.\"\"\" @abstractmethod def _get_env_yaml_params(self): \"\"\"Initialize instance variables from YAML files.\"\"\" @abstractmethod def create_envs(self): \"\"\"Set env options. Import assets. Create actors.\"\"\" @abstractmethod def _import_env_assets(self): \"\"\"Set asset options. Import assets.\"\"\" @abstractmethod def _create_actors(self): \"\"\"Set initial actor poses. Create actors. Set shape and DOF properties.\"\"\" @abstractmethod def _acquire_env_tensors(self): \"\"\"Acquire and wrap tensors. Create views.\"\"\" @abstractmethod def refresh_env_tensors(self): \"\"\"Refresh tensors.\"\"\" # NOTE: Tensor refresh functions should be called once per step, before setters. Here’s a breakdown of how these methods will be used: . | _get_env_yaml_params: this is called in __init__ of the class that defines the method. | create_envs: this will be called by FactoryBase instead of the subclass. | _import_env_assets: called inside create_envs to import assets. | _create_actors: called inside create_envs to create actors. | _acquire_env_tensors: called in __init__ of the class that defines the method. | refresh_env_tensors: use it whenever you want environment tensors to match simulation context. | . Additionally, we specify environment specific parameters in isaacgymenvs/cfg/task/. For TacSLEnvInsertion we have the following YAML file: . # See schema in factory_schema_config_env.py for descriptions of common parameters. defaults: - TacSLBase - _self_ - /factory_schema_config_env sim: disable_franka_collisions: False env: env_name: 'TacSLEnvInsertion' desired_subassemblies: ['round_peg_hole_8mm', 'round_peg_hole_12mm', 'round_peg_hole_16mm', 'rectangular_peg_hole_8mm', 'rectangular_peg_hole_12mm', 'rectangular_peg_hole_16mm'] plug_lateral_offset: 0.1 # Y-axis offset of plug before initial reset to prevent initial interpenetration with socket . These parameters are used for environment generation (how big a socket/plug) should be. In the environment, it also has knowledge of the task yaml file which it uses to load assets. In this class you do the following: . | create assets | create actors and track their handles | set friction properties of objects | create environment tensors (e.g. franka_base_pos, cube_pos/cube_quat) | . ",
    "url": "/blog/code-notes/isaac/gymenv-env.html#6-creating-an-env-in-isaacgymenv",
    
    "relUrl": "/code-notes/isaac/gymenv-env.html#6-creating-an-env-in-isaacgymenv"
  },"142": {
    "doc": "IsaacGymEnv Env Notes",
    "title": "IsaacGymEnv Env Notes",
    "content": " ",
    "url": "/blog/code-notes/isaac/gymenv-env.html",
    
    "relUrl": "/code-notes/isaac/gymenv-env.html"
  },"143": {
    "doc": "IsaacGymEnv Network Notes",
    "title": "3. IsaacGymEnv Network Notes",
    "content": "This is a tougher set of notes because the codebase seemingly disconnects the tacsl code from the network building code. Additionally, rl_games abstracts away a lot of details, but since we want to add our own networks, we will have to remove that abstraction. GOAL: Understand enough to add PointNet to the network code. ",
    "url": "/blog/code-notes/isaac/gymenv-network.html#3-isaacgymenv-network-notes",
    
    "relUrl": "/code-notes/isaac/gymenv-network.html#3-isaacgymenv-network-notes"
  },"144": {
    "doc": "IsaacGymEnv Network Notes",
    "title": "3.1 How does input work?",
    "content": "3.1.1 Input Preprocessing Specification . We can work from the training script commandline arguments to see how the network is built. Here’s an example for tactile training. task.env.obsDims={ ee_pos:[3], ee_quat:[4], socket_pos:[3], socket_quat:[4], dof_pos:[9] } task.env.obsDims={ left_tactile_camera_taxim:[80,60,3], right_tactile_camera_taxim:[80,60,3], object_pcd:[500,3] } task.env.stateDims={ ee_pos:[3], ee_quat:[4], plug_pos:[3], plug_quat:[4], socket_pos_gt:[3], socket_quat:[4], dof_pos:[9], ee_lin_vel:[3], ee_ang_vel:[3], plug_socket_force:[3], plug_left_elastomer_force:[3], plug_right_elastomer_force:[3] } train.params.network.input_preprocessors={ ee_pos:{}, ee_quat:{}, socket_pos:{}, socket_quat:{}, dof_pos:{} } train.params.config.central_value_config.network.input_preprocessors={ ee_pos:{}, ee_quat:{}, plug_pos:{}, plug_quat:{}, socket_pos_gt:{}, socket_quat:{}, dof_pos:{}, ee_lin_vel:{}, ee_ang_vel:{}, plug_socket_force:{}, plug_left_elastomer_force:{}, plug_right_elastomer_force:{} } train.params.network.input_preprocessors={ left_tactile_camera_taxim:{ cnn:{ type: conv2d_spatial_softargmax, activation: relu, initializer: {name: default}, regularizer: {name: 'None'}, convs: [{filters:32, kernel_size:8, strides:2, padding:0}, {filters:64, kernel_size:4, strides:1, padding:0}, {filters:64, kernel_size:3, strides:1, padding:0}] } } right_tactile_camera_taxim:{ cnn:{ type: conv2d_spatial_softargmax, activation: relu, initializer: {name: default}, regularizer: {name: 'None'}, convs: [{filters:32, kernel_size:8, strides:2, padding:0}, {filters:64, kernel_size:4, strides:1, padding:0}, {filters:64, kernel_size:3, strides:1, padding:0}] } } } . We can make multiple observations here: . | We split between observation dimensions and state dimensions. | State dimensions will be used for the central value network. | Observation dimensions will be used for the policy network (keys in train.params.network.input_preprocessors). | All of the architectural details for observations are in the input_preprocessors dictionary. | Specifying nothing {} means that input is passed through without modification. | The cnn key specifies a convolutional neural network with the given parameters. | Based on isaacgymenvs/cfg/train/AllegroHandPPO.yaml, we can also use mlp as a preprocessor too. | . 3.1.2 Input Preprocessing Code . IsaacGymEnvs uses A2CBuilder which it created which inherits from NetworkBuilder from rl_games. Inside of A2CBuilder, it also creates Network which inherits from NetworkBuilder.BaseNetwork from rl_games also. It seems that Network is what we want to modify: . class Network(NetworkBuilder.BaseNetwork): def __init__(self, params, **kwargs): # Network structure: # obs is a dictionary, each gets processed according to the network in input_params. # Then features from each are flattened, concatenated, and go through an MLP. # for example # camera -&gt; CNN -&gt; # |-&gt; MLP -&gt; action # joint_pos -&gt; MLP -&gt; ... self.load(params) ... for input_name, input_config in self.input_params.items(): has_cnn = 'cnn' in input_config has_mlp = 'mlp' in input_config member_input_shape = torch_ext.shape_whc_to_cwh(input_shape[input_name]) networks_actor = [] networks_critic = [] # add CNN head if it is part of the config if has_cnn: cnn_config = input_config['cnn'] cnn_args = { 'ctype': cnn_config['type'], 'input_shape': member_input_shape, 'convs': cnn_config['convs'], 'activation': cnn_config['activation'], 'norm_func_name': self.normalization, } networks_actor.append(self._build_conv(**cnn_args)) cnn_init = self.init_factory.create(**input_config['cnn']['initializer']) # to be use later if self.separate: networks_critic.append(self._build_conv(**cnn_args)) # output shape from the CNN (left tactile + right tactile) probs 128 each next_input_shape = self._calc_input_size( member_input_shape, networks_actor[-1] if has_cnn else None ) if has_mlp: mlp_config = input_config['mlp'] ... # mlp_args stuff networks_actor.append(self._build_mlp(**mlp_args)) if self.separate: networks_critic.append(self._build_mlp(**mlp_args)) next_input_shape = mlp_config['units'][-1] # add the output size of the mlp but states are low dim and no linear layers so just 256 + 23 # mlp? self.input_networks_actor[input_name] = nn.Sequential(*networks_actor) self.input_networks_critic[input_name] = nn.Sequential(*networks_critic) input_out_size += next_input_shape in_mlp_shape = input_out_size def load(self, params): ... self.input_params = params['input_preprocessors'] # AN NOTE: where our command is processed ... As we can see, it takes each observation taken by input_preprocessing and throws it into a CNN or MLP or None. You can track this by the next_input_shape variable which increases depending on whether it is using a CNN or MLP or None. To better understand how to add our on observation head, we look at _build_conv which is inside rl_games: . def _build_conv(self, ctype, **kwargs): print('conv_name:', ctype) if ctype == 'conv2d': return self._build_cnn2d(**kwargs) ... # other conv types def _build_cnn2d(self, input_shape, convs, activation, conv_func=torch.nn.Conv2d, norm_func_name=None, add_spatial_softmax=False, add_flatten=False): in_channels = input_shape[0] layers = [] for conv in convs: layers.append(conv_func(in_channels=in_channels, out_channels=conv['filters'], kernel_size=conv['kernel_size'], stride=conv['strides'], padding=conv['padding'])) conv_func=torch.nn.Conv2d act = self.activations_factory.create(activation) layers.append(act) in_channels = conv['filters'] if norm_func_name == 'layer_norm': layers.append(torch_ext.LayerNorm2d(in_channels)) elif norm_func_name == 'batch_norm': layers.append(torch.nn.BatchNorm2d(in_channels)) if add_spatial_softmax: layers.append(SpatialSoftArgmax(normalize=True)) if add_flatten: layers.append(torch.nn.Flatten()) return nn.Sequential(*layers) . We use the arguments from the command as __init__ arguments to the builder and then construct our observation head in a straightforward manner. We just have to make sure we return one nn.Module instead of many. 3.1.3 Adding your own Input Preprocessor . First you have to add _build_pointnet in the rl_games implementation along with _build_conv and _build_mlp. It’s pretty straightforward. Next, you add this to the Network.__init__: . class Network(NetworkBuilder.BaseNetwork): def __init__(self, params, **kwargs): # Network structure: # obs is a dictionary, each gets processed according to the network in input_params. # Then features from each are flattened, concatenated, and go through an MLP. # for example # camera -&gt; CNN -&gt; # |-&gt; MLP -&gt; action # joint_pos -&gt; MLP -&gt; actions_num = kwargs.pop('actions_num') input_shape = kwargs.pop('input_shape') self.value_size = kwargs.pop('value_size', 1) self.num_seqs = num_seqs = kwargs.pop('num_seqs', 1) NetworkBuilder.BaseNetwork.__init__(self) self.load(params) # self.actor_cnn = nn.Sequential() # self.critic_cnn = nn.Sequential() self.input_networks_actor = nn.ModuleDict() self.input_networks_critic = nn.ModuleDict() self.actor_mlp = nn.Sequential() self.critic_mlp = nn.Sequential() # size of the preprocessing of inputs before the main MLP input_out_size = 0 for input_name, input_config in self.input_params.items(): has_cnn = 'cnn' in input_config has_mlp = 'mlp' in input_config has_pnet = 'pnet' in input_config member_input_shape = torch_ext.shape_whc_to_cwh(input_shape[input_name]) networks_actor = [] networks_critic = [] if has_pnet: pnet_config = input_config['pnet'] pnet_args = { 'in_channel': pnet_config['in_channel'], 'out_channel': pnet_config['out_channel'], 'use_layernorm': False, 'final_norm': 'none', 'use_projection': True } networks_actor.append(self._build_pointnet(**pnet_args)) # TODO: make init if self.separate: networks_critic.append(self._build_pointnet(**pnet_args)) next_input_shape = pnet_config['out_channel'] # output shape from the pointnet ... # the other types . Next, you add this command to your training script: . task.env.obsDims={ object_pcd:[500,3] } train.params.network.input_preprocessors={ object_pcd:{ pnet:{ in_channel:3, out_channel:128 } } } . I’ve already included object_pcd in my previous observation notes. ",
    "url": "/blog/code-notes/isaac/gymenv-network.html#31-how-does-input-work",
    
    "relUrl": "/code-notes/isaac/gymenv-network.html#31-how-does-input-work"
  },"145": {
    "doc": "IsaacGymEnv Network Notes",
    "title": "IsaacGymEnv Network Notes",
    "content": " ",
    "url": "/blog/code-notes/isaac/gymenv-network.html",
    
    "relUrl": "/code-notes/isaac/gymenv-network.html"
  },"146": {
    "doc": "IsaacGymEnv Obs Notes",
    "title": "1. Understanding TacSL Training Script",
    "content": "Currently, I can run the training script for TacSL peg insertion. My first goal is to try to run the training script and see if it works! Then I have to answer the following questions using the codebase: . | What is observation space? | What is the action space? | What parts are being randomized between environments? | What is the reward function? | How does this connect with WandB? | Where is policy gradient calculated? | What is the policy architecture? | How is GelSight included in the environment? | . ",
    "url": "/blog/code-notes/isaac/gymenv-obs.html#1-understanding-tacsl-training-script",
    
    "relUrl": "/code-notes/isaac/gymenv-obs.html#1-understanding-tacsl-training-script"
  },"147": {
    "doc": "IsaacGymEnv Obs Notes",
    "title": "1.1 Training Script Arguments",
    "content": "These are the arguments: . | checkpoint: start training from specified checkpoint otherwise from scratch | task.env.numEnvs: (int) number of environments to run in parallel | test: (bool) whether to run eval or not | seed: (int) random seed | max_iterations: (int) number of training iterations | experiment: (str) experiment name | task: (str) task name to get environment from | task.env.task_type: (str) task type to choose in environment (exclusive to tacsl) | task.rl.asymmetric_observations: (bool) whether to use asymmetric observations | task.rl.add_contact_info_to_aac_states: (bool) whether to add contact info to AAC states | . ",
    "url": "/blog/code-notes/isaac/gymenv-obs.html#11-training-script-arguments",
    
    "relUrl": "/code-notes/isaac/gymenv-obs.html#11-training-script-arguments"
  },"148": {
    "doc": "IsaacGymEnv Obs Notes",
    "title": "1.2 Getting Observation",
    "content": "It seems that the observations are taken as a dictionary called self.obs_dict. We call observations in post_physics_step in tacsl_task_insertion.py: . def post_physics_step(self): ... self.refresh_all_tensors() self.compute_observations() self.compute_reward() ... Observations are taken by calling compute_observations in tacsl_task_insertion.py: . def compute_observations(self): \"\"\"Compute observations.\"\"\" if self.cfg_task.env.use_dict_obs: return self.compute_observations_dict_obs() return self.obs_buf # shape = (num_envs, num_observations) . which calls compute_observations_dict_obs. This is the meat of the observation gathering code. 1.2.1 Tactile sensor force observation . You’ll notice that we get contact observations implicitly through plug_left_elastomer_force which is the force sensor measured on the left elastomer on the jaw gripper. In tacsl_task_insertion.py, we see that: . self.obs_dict['plug_left_elastomer_force'][:] = self.contact_force_pairwise[:, self.plug_body_id_env, self.franka_body_ids_env['panda_leftfinger']] . where self.contact_force_pairwise is assigned a value by the gym api: . _contact_force_pairwise = self.gym.acquire_pairwise_contact_force_tensor(self.sim) # shape = (num_envs * num_bodies * num_bodies, 3) ... self.contact_force_pairwise = gymtorch.wrap_tensor(_contact_force_pairwise) . since this gym call wasn’t reviewed in the tutorials we give a description from the documentation: . Retrieves buffer for pairwise contact forces between all bodies in the sim. The buffer has shape (num_rigid_bodies, num_rigid_bodies, 3). Each contact force state contains one value for each X, Y, Z axis. 1.2.2 Pose observations . There are several poses observed in the observation space. These are a list of them: . | plug: thing to be inserted | ee: end effector | socket: the thing to be inserted into | dof_pos: joint positions (revolute) + gripper position (prismatic) | . Lets dive into how ee_pos is calculated. In tacsl_task_insertion.py, we see that: . def compute_observations_dict_obs(self): self.obs_dict['ee_pos'][:] = self.fingertip_midpoint_pos ... where self.fingertip_midpoint_pos is calculated in tacsl_base.py: . def acquire_base_tensors(self): _body_state = self.gym.acquire_rigid_body_state_tensor(self.sim) # shape = (num_envs * num_bodies, 13) ... self.body_state = gymtorch.wrap_tensor(_body_state) ... self.body_pos = self.body_state.view(self.num_envs, self.num_bodies, 13)[..., 0:3] ... self.fingertip_centered_pos = self.body_pos[:, self.fingertip_centered_body_id_env, 0:3] ... self.fingertip_midpoint_pos = self.fingertip_centered_pos.detach().clone() ... And this is the gym documentaiton description of self.gym.acquire_rigid_body_state_tensor: . Retrieves buffer for Rigid body states. The buffer has shape (num_rigid_bodies, 13). State for each rigid body contains position([0:3]), rotation([3:7]), linear velocity([7:10]), and angular velocity([10:13]). This is the same way that ee_quat is calculated. Everything else is just indexing into the self.body_state tensor. 1.2.3 Joint observations . We get the dof_pos in tacsl_task_insertion.py: . def compute_observations_dict_obs(self): ... if 'dof_pos' in self.cfg_task.env.obsDims or 'dof_pos' in self.cfg_task.env.stateDims: self.obs_dict['dof_pos'][:] = self.dof_pos ... where self.dof_pos is calculated in tacsl_base.py: . def acquire_base_tensors(self): ... _dof_state = self.gym.acquire_dof_state_tensor(self.sim) # shape = (num_envs * num_dofs, 2) ... self.dof_state = gymtorch.wrap_tensor(_dof_state) ... self.dof_pos = self.dof_state.view(self.num_envs, self.num_dofs, 2)[..., 0] . Pretty straightforward. This is all for the teacher training. We’ll now move towards observations for the student training. 1.2.4 RGB observations . These observations are listed as left_tactile_camera_taxim and right_tactile_camera_taxim in the observation space. These are the RGB images from the GelSight sensors with the shape (H,W,3) and are calculated in tacsl_task_insertion.py: . def compute_observations_dict_obs(self): ... if self.cfg_task.env.use_camera_obs: images = self.get_camera_image_tensors_dict() if self.cfg_task.env.saveVideo and not self.cfg_task.env.use_tactile_field_obs: video_dict = { k: v for k, v in images.items() if k in [ 'left_tactile_camera_taxim', 'right_tactile_camera_taxim', 'wrist', 'front', 'side' ] } frame = make_frame_from_obs(video_dict) self.frames.append(np.array(frame)) ... if self.cfg_task.env.use_isaac_gym_tactile: # Optionally subsample tactile image ssr = self.cfg_task.env.tactile_subsample_ratio for k in self.tactile_ig_keys: images[k] = images[k][:, ::ssr, ::ssr] for cam in images: if cam in self.cfg_task.env.obsDims: if images[cam].dtype == torch.uint8: self.obs_dict[cam][..., :3] = images[cam] / 255. else: self.obs_dict[cam][..., :3] = images[cam] self.apply_image_augmentation_to_obs_dict() . So self.obs_dict takes in images[cam] which is filled in by get_camera_image_tensors_dict which is defined in tacsl_base.py: . def get_camera_image_tensors_dict(self): \"\"\" Get the dictionary of camera image tensors, including tactile RGB images. Returns: dict: Dictionary of camera image tensors. \"\"\" camera_image_tensors_dict = super().get_camera_image_tensors_dict() # Compute tactile RGB from tactile depth if hasattr(self, 'has_tactile_rgb') and self.nominal_tactile: for k in self.nominal_tactile: depth_image = self.nominal_tactile[k] - camera_image_tensors_dict[k] # depth_image_delta taxim_render_all = self.taxim_gelsight.render_tensorized(depth_image) camera_image_tensors_dict[f'{k}_taxim'] = taxim_render_all return camera_image_tensors_dict . where camera_images_tensors_dict is filled by super().get_camera_image_tensors_dict() which is defined in tacsl_base.py: . def get_camera_image_tensors_dict(self): \"\"\" Get the dictionary of camera image tensors. Returns: dict: Dictionary of camera image tensors. \"\"\" # transforms and information must be communicated from the physics simulation into the graphics system if self.device != 'cpu': self.gym.fetch_results(self.sim, True) self.gym.step_graphics(self.sim) self.gym.render_all_camera_sensors(self.sim) self.gym.start_access_image_tensors(self.sim) camera_image_tensors_dict = dict() for name in self.camera_spec_dict: camera_spec = self.camera_spec_dict[name] if camera_spec['image_type'] == 'rgb': num_channels = 3 camera_images = torch.zeros( (self.num_envs, camera_spec.image_size[0], camera_spec.image_size[1], num_channels), device=self.device, dtype=torch.uint8) for id in np.arange(self.num_envs): camera_images[id] = self.camera_tensors_list[id][name][:, :, :num_channels].clone() elif camera_spec['image_type'] == 'depth': num_channels = 1 camera_images = torch.zeros( (self.num_envs, camera_spec.image_size[0], camera_spec.image_size[1]), device=self.device, dtype=torch.float) for id in np.arange(self.num_envs): # Note that isaac gym returns negative depth # See: https://carbon-gym.gitlab-master-pages.nvidia.com/carbgym/graphics.html?highlight=image_depth#camera-image-types camera_images[id] = self.camera_tensors_list[id][name][:, :].clone() * -1. camera_images[id][camera_images[id] == np.inf] = 0.0 else: raise NotImplementedError(f'Image type {camera_spec[\"image_type\"]} not supported!') camera_image_tensors_dict[name] = camera_images return camera_image_tensors_dict . where camera_image_tensors_dict is filled by self.camera_tensors_list[id][name] which monitors the images in the environment and fills in accordingly. This process is set up in the following code in tacsl_sensors.py: . class CameraSensor: \"\"\" Class for managing camera sensors. Provides methods for creating and managing camera actors and tensors. \"\"\" def create_camera_actors(self, camera_spec_dict): \"\"\" Create camera actors based on the camera specification dictionary. # Note: This should be called once, as IsaacGym's global camera indexing expects all cameras of env 0 be created before env 1, and so on. Args: camera_spec_dict (dict): Dictionary of camera specifications. Returns: tuple: List of camera handles and list of camera tensors. \"\"\" camera_handles_list = [] camera_tensors_list = [] for i in range(self.num_envs): env_ptr = self.env_ptrs[i] env_camera_handles = self.setup_env_cameras(env_ptr, camera_spec_dict) camera_handles_list.append(env_camera_handles) env_camera_tensors = self.create_tensors_for_env_cameras(env_ptr, env_camera_handles, camera_spec_dict) camera_tensors_list.append(env_camera_tensors) return camera_handles_list, camera_tensors_list def create_tensors_for_env_cameras(self, env_ptr, env_camera_handles, camera_spec_dict): \"\"\" Create tensors for environment cameras. Args: env_ptr: Pointer to the environment. env_camera_handles (dict): Dictionary of camera handles. camera_spec_dict (dict): Dictionary of camera specifications. Returns: dict: Dictionary of environment camera tensors. \"\"\" env_camera_tensors = {} for name in env_camera_handles: camera_handle = env_camera_handles[name] if camera_spec_dict[name].image_type == 'rgb': # obtain camera tensor camera_tensor = self.gym.get_camera_image_gpu_tensor(self.sim, env_ptr, camera_handle, gymapi.IMAGE_COLOR) elif camera_spec_dict[name].image_type == 'depth': # obtain camera tensor camera_tensor = self.gym.get_camera_image_gpu_tensor(self.sim, env_ptr, camera_handle, gymapi.IMAGE_DEPTH) else: raise NotImplementedError(f'Camera type {camera_spec_dict[name].image_type} not supported') # wrap camera tensor in a pytorch tensor torch_camera_tensor = gymtorch.wrap_tensor(camera_tensor) # store references to the tensor that gets updated when render_all_camera_sensors env_camera_tensors[name] = torch_camera_tensor return env_camera_tensors . We can see that env_camera_tensors are filled automatically when render_all_camera_sensors is called. NOTE: Now, we move onto how to get shear fields. This has already been implemented in the codebase, we are just finding where it is located and how to use it. 1.2.5 Shear field observations . We can find a way to include shear fields in the observation space by looking at tacsl_task_insertion.py in the compute_observations_dict_obs function: . def compute_observations_dict_obs(self): ... if self.cfg_task.env.use_shear_force: tactile_force_field_dict = self.get_tactile_force_field_tensors_dict(debug=True if self.cfg_task.env.use_tactile_field_obs else False) # import ipdb; ipdb.set_trace() if self.cfg_task.env.use_tactile_field_obs: for k in ['tactile_force_field_left', 'tactile_force_field_right']: self.obs_dict[k][:] = tactile_force_field_dict[k] if self.cfg_task.env.zero_out_normal_force_field_obs: self.obs_dict[k][..., 0] *= 0.0 ... The two arguments we want for observation space are tactile_force_field_left and tactile_force_field_right. Of course, all of these values are computed in self.get_tactile_force_field_tensors_dict which is defined in tacsl_env_insertion.py: . def get_tactile_force_field_tensors_dict(self, debug=False): tactile_force_field_dict_raw = self.get_tactile_shear_force_fields() tactile_force_field_dict_processed = dict() ... #filling in tactile_force_field_dict_processed return tactile_force_field_dict_processed . This depends on self.get_tactile_shear_force_fields() which is defined in tacsl_sensors.py: . def get_tactile_shear_force_fields(self): tactile_force_field = dict() for key, config in self.tactile_shear_field_configs_dict.items(): indenter_link_id = config['indenter_link_rb_id'] elastomer_link_id = config['elastomer_link_rb_id'] result = self.get_penalty_based_tactile_forces(indenter_link_id, elastomer_link_id) tactile_force_field[key] = result return tactile_force_field . This calculates the shear fields which are not by default calculated in isaacgym. ** TLDR **: . | Use tactile_force_field_left and tactile_force_field_right in the observation space to get shear fields. | . ",
    "url": "/blog/code-notes/isaac/gymenv-obs.html#12-getting-observation",
    
    "relUrl": "/code-notes/isaac/gymenv-obs.html#12-getting-observation"
  },"149": {
    "doc": "IsaacGymEnv Obs Notes",
    "title": "IsaacGymEnv Obs Notes",
    "content": "Now that we have a rudimentary understanding of the Isaac Gym API, we’re going to dive into how IsaacGymEnv works. I’ll do a backtracking approach. As in, I’ll run code snippets and then figure out how each part works. The codebase is way too large, that I can only understand enough to get things working. Current Issues with TacSLIsaacGymEnv: . | Wrist Wrench doesn’t work (returns zeros) | . ",
    "url": "/blog/code-notes/isaac/gymenv-obs.html",
    
    "relUrl": "/code-notes/isaac/gymenv-obs.html"
  },"150": {
    "doc": "IsaacGymEnv Tacsl Notes",
    "title": "4. Understanding how Tacsl sensors work in IsaacGymEnv",
    "content": "These are specially created sensors that provide tactile information of the environment. We use this to understand how the robot interacts with the environment. ",
    "url": "/blog/code-notes/isaac/gymenv-tacsl.html#4-understanding-how-tacsl-sensors-work-in-isaacgymenv",
    
    "relUrl": "/code-notes/isaac/gymenv-tacsl.html#4-understanding-how-tacsl-sensors-work-in-isaacgymenv"
  },"151": {
    "doc": "IsaacGymEnv Tacsl Notes",
    "title": "4.1 Tacsl Sensor",
    "content": "So we can get the raw tactile rgb data or the “force field” data from the Tacsl Sensors in IsaacGymEnv. The “force field” data is just the deformation field of the tactile sensors in the xy direction. This is normally done with optical flow on dotted patterns on the tactile sensors in real life. This is done naturally in IsaacGymEnv. Here is an example of a shear field we get from visualizing tactile_force_field_left and tactile_force_field_right sensors in IsaacGymEnv when robot is holding an object. The yellow arrow represents the direction of the deformation. We can also look at the tactile data along with the actual scene where the robot is interacting with the object. As we can see, the red pads represent the tactile sensor on the robot. The robot is holding onto a peg at the very tip of the sensor. We can see that the sensors pads are inverted from each other and their relative direction for the right sensor is also inverted. NOTE: the tactile_force_field_left and tactile_force_field_right data structures are such that tactile_force_field_left[..., 0] is the normal direction (which is set to 0) and the same for the right sensor. ",
    "url": "/blog/code-notes/isaac/gymenv-tacsl.html#41-tacsl-sensor",
    
    "relUrl": "/code-notes/isaac/gymenv-tacsl.html#41-tacsl-sensor"
  },"152": {
    "doc": "IsaacGymEnv Tacsl Notes",
    "title": "4.2 Understanding Tacsl code",
    "content": "It’s good to know where all of the code related to Tacsl is, so we present how the Tacsl class inheritance works in IsaacGymEnv. flowchart TD A(CameraSensor) --&gt; C(TactileRGBSensor) B(TactileBase) --&gt; C(TactileRGBSensor) B(TactileBase) --&gt; D(TactileFieldSensor) C(TactileRGBSensor) --&gt; E(TacSLSensors) D(TactileFieldSensor) --&gt; E(TacSLSensors) A(CameraSensor) --&gt; E(TacSLSensors) E(TacSLSensors) --&gt; F(TacSLEnvInsertion) . TacSLEnvInsertion is the environment class which we don’t need much more detail about. We just need to know that TacSLSensors is the last abstraction we need to understand since it’ll be accessed by the environment class. We can see in TactileFieldSensor that it can run generate_tactile_points which if we visualize using Trimesh, we can see the points in 3d space. This shows that we can definitely access the 3d tactile points in the environment . 4.3 Tracking shear field calculation . The force field data we visualized was first calculated in tacsl_task_insertion.py using TacSLSensors.get_tactile_force_field_tensors_dict. This function merely processes the raw data from TactileFieldSensor.get_tactile_shear_force_fields. The meat of this function is described here: . def get_tactile_shear_force_fields(self): tactile_force_field = dict() for key, config in self.tactile_shear_field_configs_dict.items(): indenter_link_id = config['indenter_link_rb_id'] elastomer_link_id = config['elastomer_link_rb_id'] ## AN NOTE: forces are here result = self.get_penalty_based_tactile_forces(indenter_link_id, elastomer_link_id) tactile_force_field[key] = result return tactile_force_field . so this relies on TactileFieldSensor.get_penalty_based_tactile_forces. we got depth + shear and colored it like depth is visualized also added object too. ",
    "url": "/blog/code-notes/isaac/gymenv-tacsl.html#42-understanding-tacsl-code",
    
    "relUrl": "/code-notes/isaac/gymenv-tacsl.html#42-understanding-tacsl-code"
  },"153": {
    "doc": "IsaacGymEnv Tacsl Notes",
    "title": "IsaacGymEnv Tacsl Notes",
    "content": " ",
    "url": "/blog/code-notes/isaac/gymenv-tacsl.html",
    
    "relUrl": "/code-notes/isaac/gymenv-tacsl.html"
  },"154": {
    "doc": "IsaacGymEnv Task Notes",
    "title": "7. Creating your own Task in IsaacGymEnv",
    "content": "All tasks in IsaacGymEnv inherit from an abstract base class FactoryABCTask. This class defines the structure and methods that any task must implement. Here’s a breakdown of the key methods: . class FactoryABCTask(ABC): @abstractmethod def __init__(self): \"\"\"Initialize instance variables. Initialize environment superclass.\"\"\" @abstractmethod def _get_task_yaml_params(self): \"\"\"Initialize instance variables from YAML files.\"\"\" @abstractmethod def _acquire_task_tensors(self): \"\"\"Acquire tensors.\"\"\" @abstractmethod def _refresh_task_tensors(self): \"\"\"Refresh tensors.\"\"\" @abstractmethod def pre_physics_step(self): \"\"\"Reset environments. Apply actions from policy as controller targets. Simulation step called after this method.\"\"\" @abstractmethod def post_physics_step(self): \"\"\"Step buffers. Refresh tensors. Compute observations and reward.\"\"\" @abstractmethod def compute_observations(self): \"\"\"Compute observations.\"\"\" @abstractmethod def compute_reward(self): \"\"\"Detect successes and failures. Update reward and reset buffers.\"\"\" @abstractmethod def _update_rew_buf(self): \"\"\"Compute reward at current timestep.\"\"\" @abstractmethod def _update_reset_buf(self): \"\"\"Assign environments for reset if successful or failed.\"\"\" @abstractmethod def reset_idx(self): \"\"\"Reset specified environments.\"\"\" @abstractmethod def _reset_franka(self): \"\"\"Reset DOF states and DOF targets of Franka.\"\"\" @abstractmethod def _reset_object(self): \"\"\"Reset root state of object.\"\"\" @abstractmethod def _reset_buffers(self): \"\"\"Reset buffers.\"\"\" @abstractmethod def _set_viewer_params(self): \"\"\"Set viewer parameters.\"\"\" . There is a lot more we need to provide to this task class compared to the environment class. All of the _method methods are all called within the task class. The other methods are called by the script when it runs the task. For example, post_physics_step is called by the script extensively which computes observations and rewards every step. Most of the code past the task is abstracted which makes it difficult to understand how this codebase works. ",
    "url": "/blog/code-notes/isaac/gymenv-task.html#7-creating-your-own-task-in-isaacgymenv",
    
    "relUrl": "/code-notes/isaac/gymenv-task.html#7-creating-your-own-task-in-isaacgymenv"
  },"155": {
    "doc": "IsaacGymEnv Task Notes",
    "title": "IsaacGymEnv Task Notes",
    "content": " ",
    "url": "/blog/code-notes/isaac/gymenv-task.html",
    
    "relUrl": "/code-notes/isaac/gymenv-task.html"
  },"156": {
    "doc": "Hand Notes",
    "title": "Hand Discussion",
    "content": " ",
    "url": "/blog/project-notes/hand-notes.html#hand-discussion",
    
    "relUrl": "/project-notes/hand-notes.html#hand-discussion"
  },"157": {
    "doc": "Hand Notes",
    "title": "1. Why do we want dexterous hands?",
    "content": "Questions: . | What are core capabilities we are missing? | What are some tasks we cannot achieve with parallel jaw? | Can capabilities be achieved with bimanual? | . 1.1 . Currently with a single-arm jaw gripper setup, we are limited to a humble amount of tasks. | Lots of things are just being done with BC or RL. | . What can’t a jaw do? . | only one way to grasp… is that bad? . | can’t roll | . | . What’s the utility of more ways to grasp? . 1.3 . With bimanual, it’s just unconstrained parallel jaw. | It’s pretty tough to do anything past parallel jaw capabilities. | You could say extra fingers are to make grasped object more stable. | . | . What about deformable manipulation? . NOTE: ML argument is that we want to learn from data, and to mimic human form factor. bimanual benefits: . | transfer grasps. | . Difference between hand and jaw: . | super high dimensional . | lots of behavior that can be generated with multi-finger setup (idk about current sota). | . | . ",
    "url": "/blog/project-notes/hand-notes.html#1-why-do-we-want-dexterous-hands",
    
    "relUrl": "/project-notes/hand-notes.html#1-why-do-we-want-dexterous-hands"
  },"158": {
    "doc": "Hand Notes",
    "title": "2. Integration with existing major directions?",
    "content": ". | VLAs? | Large scale RL with sim2real? | Model-based stuff? | . ",
    "url": "/blog/project-notes/hand-notes.html#2-integration-with-existing-major-directions",
    
    "relUrl": "/project-notes/hand-notes.html#2-integration-with-existing-major-directions"
  },"159": {
    "doc": "Hand Notes",
    "title": "Hand Notes",
    "content": " ",
    "url": "/blog/project-notes/hand-notes.html",
    
    "relUrl": "/project-notes/hand-notes.html"
  },"160": {
    "doc": "Handy Workshop",
    "title": "Handy Workshop (5/19/2025)",
    "content": " ",
    "url": "/blog/conference-notes/icra-2025/workshop/handy.html#handy-workshop-5192025",
    
    "relUrl": "/conference-notes/icra-2025/workshop/handy.html#handy-workshop-5192025"
  },"161": {
    "doc": "Handy Workshop",
    "title": "Shuran Song",
    "content": "Research tries to tackle the cross-embodiment problem. Cross-embodiment . 2 ways they’re approaching this: . | Hardware design an interface to get around embodiment gap. (DexUMI) | Train a policy that accounts for this variation (GET-Zero). | . Note for (2), this is still limited to a certain setup. Shuran wants to extend the capabilities of (2). ",
    "url": "/blog/conference-notes/icra-2025/workshop/handy.html#shuran-song",
    
    "relUrl": "/conference-notes/icra-2025/workshop/handy.html#shuran-song"
  },"162": {
    "doc": "Handy Workshop",
    "title": "NVIDIA: Ankur Handa",
    "content": "These guys I’ve seen in terms of Ratliff’s work. In summary, it is interesting to check out their work. Their papers: . | Dextreme | DextremeFGP | DextrAH | DextrAH-RGB | DextrAH-MTS | DexPBT | . Concepts that are re-occurring: . | Automatic Domain Randomization | Teacher-Student | . ",
    "url": "/blog/conference-notes/icra-2025/workshop/handy.html#nvidia-ankur-handa",
    
    "relUrl": "/conference-notes/icra-2025/workshop/handy.html#nvidia-ankur-handa"
  },"163": {
    "doc": "Handy Workshop",
    "title": "Tengyu Liu",
    "content": "NOTE: Didn’t pay enough attention. Use Mocap with hand tracking to get reference traj. Use that to learn a policy? . Papers that they presented: . | ManipTrans | DexManipNet | . ",
    "url": "/blog/conference-notes/icra-2025/workshop/handy.html#tengyu-liu",
    
    "relUrl": "/conference-notes/icra-2025/workshop/handy.html#tengyu-liu"
  },"164": {
    "doc": "Handy Workshop",
    "title": "Spotlight",
    "content": "SeqGrasp: Sequential Multi-object GRASP . Dataset Generation: Initialize random robot hand poses around object. Run energy-based optimization to get a grasp. Multi-object grasping is about grasping multiple objects in one go. Dexonomy . Another way to grasp with multi-finger hand. NormalFlow . This is a tracking paper using visuotactile. They minimize Normal Map difference instead of doing point cloud matching (ICP). This is done with unconstrained optimization (Gauss-Newton Method). DOGlove . It’s a teleop glove. Kili what’re your thoughts?: Shit - Kili 2025 . | Under 600 | . ",
    "url": "/blog/conference-notes/icra-2025/workshop/handy.html#spotlight",
    
    "relUrl": "/conference-notes/icra-2025/workshop/handy.html#spotlight"
  },"165": {
    "doc": "Handy Workshop",
    "title": "Xiaolong Wang",
    "content": "Humanoid policy \\(\\sim\\) Human Policy. Does some collection with Egocentric data. Sleeper guy. ",
    "url": "/blog/conference-notes/icra-2025/workshop/handy.html#xiaolong-wang",
    
    "relUrl": "/conference-notes/icra-2025/workshop/handy.html#xiaolong-wang"
  },"166": {
    "doc": "Handy Workshop",
    "title": "Handy Workshop",
    "content": " ",
    "url": "/blog/conference-notes/icra-2025/workshop/handy.html",
    
    "relUrl": "/conference-notes/icra-2025/workshop/handy.html"
  },"167": {
    "doc": "Homework",
    "title": "Homework",
    "content": " ",
    "url": "/blog/conference-notes/icra-2025/homework.html",
    
    "relUrl": "/conference-notes/icra-2025/homework.html"
  },"168": {
    "doc": "Homework",
    "title": "Deformable Workshop",
    "content": "check these links: . | Jeannette Bohg (object-centric or not) | Shuran Song (lessons learned on deformable) | Ken Goldberg (AI + GOFE for Manipulating Deformables) | Graph-based RL for deformable | . ",
    "url": "/blog/conference-notes/icra-2025/homework.html#deformable-workshop",
    
    "relUrl": "/conference-notes/icra-2025/homework.html#deformable-workshop"
  },"169": {
    "doc": "Homework",
    "title": "Beyond Pick and Place Workshop",
    "content": ". | . ",
    "url": "/blog/conference-notes/icra-2025/homework.html#beyond-pick-and-place-workshop",
    
    "relUrl": "/conference-notes/icra-2025/homework.html#beyond-pick-and-place-workshop"
  },"170": {
    "doc": "How-to PDB",
    "title": "How to PDB",
    "content": " ",
    "url": "/blog/code-notes/how-to-pdb.html#how-to-pdb",
    
    "relUrl": "/code-notes/how-to-pdb.html#how-to-pdb"
  },"171": {
    "doc": "How-to PDB",
    "title": "Using PDB for debugging in python",
    "content": "Insert the following lines of code where you want to start debugging: . import pdb; pdb.set_trace() . This will pause the execution and drop you into the PDB interactive console. This is sketchier than just breakpoints in an IDE, but it works in any environment where you can run Python code. ",
    "url": "/blog/code-notes/how-to-pdb.html#using-pdb-for-debugging-in-python",
    
    "relUrl": "/code-notes/how-to-pdb.html#using-pdb-for-debugging-in-python"
  },"172": {
    "doc": "How-to PDB",
    "title": "PDB Commands",
    "content": "Short list of useful PDB commands: . | p: Print the value of an expression. (p x prints the value of x) | continue: Continue execution until the next breakpoint. | c: short for continue. | s: Step into the next line of code. | n: Step to the next line in the current function. | q: Quit the debugger and exit the program. | . NOTE: I found that the python documentation for PDB is not very helpful. Look at GDB documentation for a more comprehensive list of commands. ",
    "url": "/blog/code-notes/how-to-pdb.html#pdb-commands",
    
    "relUrl": "/code-notes/how-to-pdb.html#pdb-commands"
  },"173": {
    "doc": "How-to PDB",
    "title": "How-to PDB",
    "content": " ",
    "url": "/blog/code-notes/how-to-pdb.html",
    
    "relUrl": "/code-notes/how-to-pdb.html"
  },"174": {
    "doc": "How-to Tmux",
    "title": "How to Tmux",
    "content": " ",
    "url": "/blog/code-notes/how-to-tmux.html#how-to-tmux",
    
    "relUrl": "/code-notes/how-to-tmux.html#how-to-tmux"
  },"175": {
    "doc": "How-to Tmux",
    "title": "Using Tmux for terminal multiplexing",
    "content": "Create a new tmux session with a specific name: . tmux new -s [session-name] . Use Ctrl+b followed by d to detach from the session. To reattach to a session: . tmux attach -d -t [session-name] . To list all tmux sessions: . tmux ls . To kill a specific tmux session: . tmux kill-session -t [session-name] . Extra things for session management: . ctrl+b c # Create a new window ctrl+b , # Rename the current window ctrl+b s # List all windows . ",
    "url": "/blog/code-notes/how-to-tmux.html#using-tmux-for-terminal-multiplexing",
    
    "relUrl": "/code-notes/how-to-tmux.html#using-tmux-for-terminal-multiplexing"
  },"176": {
    "doc": "How-to Tmux",
    "title": "How-to Tmux",
    "content": " ",
    "url": "/blog/code-notes/how-to-tmux.html",
    
    "relUrl": "/code-notes/how-to-tmux.html"
  },"177": {
    "doc": "Image Augmentation (Torchvision)",
    "title": "Image Augmentation with Torchvision",
    "content": "https://docs.pytorch.org/vision/main/transforms.html . Torchvision now has v1 and v2 transforms. It is recommended to try v2 now. Here is an example of how to use v2 transforms for image augmentation: . from torchvision.transforms import v2 transforms = v2.Compose([ v2.ToImage(), # Convert to tensor, only needed if you had a PIL image v2.ToDtype(torch.uint8, scale=True), # optional, most input are already uint8 at this point # ... v2.RandomResizedCrop(size=(224, 224), antialias=True), # Or Resize(antialias=True) # ... v2.ToDtype(torch.float32, scale=True), # Normalize expects float input v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), ]) . reasons to use v2: . | faster | supports arbitrary inputs (dicts, list, tuples) | support is moved to v2 now. | can transform bbox masks and videos. | backwards compatible with v1 transforms (no loss). | . ",
    "url": "/blog/code-notes/image-augmentation.html#image-augmentation-with-torchvision",
    
    "relUrl": "/code-notes/image-augmentation.html#image-augmentation-with-torchvision"
  },"178": {
    "doc": "Image Augmentation (Torchvision)",
    "title": "Image Augmentation (Torchvision)",
    "content": " ",
    "url": "/blog/code-notes/image-augmentation.html",
    
    "relUrl": "/code-notes/image-augmentation.html"
  },"179": {
    "doc": "ML (Git Gud) Resources",
    "title": "1. Get started (Basics)",
    "content": ". | Andrej Karpathy Zero to Hero - YouTube . | Indirectly learn how pytorch works under the hood. | ML Concepts: Backprop, Gradient Descent, Weight Init, Train/Val, BatchNorm/LayerNorm, Attention …etc. | Training Concepts (gradient accumulation, mixed precision, distributed pytorch, ) | . | UvA DL - Notebook-Website . | Deep Learning 1 (PyTorch): exposes you to popular architectures | Training Models at Scale . | takes stuff you learned from (1.) and levels it up. | . | . | CS294 - Website . | Taught by Pieter Abbeel @ UC Berkeley. | Homeworks are from scratch mini-paper implementations per question. (most valuable) | Makes you more solid in your PyTorch and implementation intuition! | . | . ",
    "url": "/blog/ml-git-gud-notes/#1-get-started-basics",
    
    "relUrl": "/ml-git-gud-notes/#1-get-started-basics"
  },"180": {
    "doc": "ML (Git Gud) Resources",
    "title": "3. RL Stuff",
    "content": ". | CS285 - Website . | Taught by Sergey Levine @ UC Berkeley. | This is a course on RL and covers the basics of RL, policy gradients, actor-critic, PPO, DDPG, SAC, and more. | The homeworks are very well designed and will help you understand the concepts better. | . | UCL David Silver DeepMind - Youtube | Pulkit 6.8200 - Website . | This is a course on RL+IL but for robotics. | Well designed to help you setup code in this domain. | . | | . ",
    "url": "/blog/ml-git-gud-notes/#3-rl-stuff",
    
    "relUrl": "/ml-git-gud-notes/#3-rl-stuff"
  },"181": {
    "doc": "ML (Git Gud) Resources",
    "title": "2. Becoming a good engineer",
    "content": ". | Full-stack DL - Website . | This is a course that tries to make you become a better ML Engineer. | These are my takeaways . | PyTorch Lightning intro. | W&amp;B (Weights and Biases) for experiment tracking. | Gradio for model deployment. | . | . | Missing Semester of CS Education - Website . | If you don’t know stuff from this course, you’re missing out! | . | . ",
    "url": "/blog/ml-git-gud-notes/#2-becoming-a-good-engineer",
    
    "relUrl": "/ml-git-gud-notes/#2-becoming-a-good-engineer"
  },"182": {
    "doc": "ML (Git Gud) Resources",
    "title": "3. Learning more!",
    "content": ". | UvA DL - Notebook-Website . | Deep Learning 2: Learn some super niche stuff that will make you overall better. | . | Diffusion - Website . | Gives mathematical details of diffusion models and flow matching. | Labs help you train a diffusion model from the ground up. | . | Machine Learning &amp; Simulation - Youtube . | This guy does lots of ML Physics stuff and goes into classical techniques on dynamical systems (FEM from scratch). | He also is a huge fan of JAX and has a lot of videos on it. | . | . ",
    "url": "/blog/ml-git-gud-notes/#3-learning-more",
    
    "relUrl": "/ml-git-gud-notes/#3-learning-more"
  },"183": {
    "doc": "ML (Git Gud) Resources",
    "title": "4. Theoretical Stuff",
    "content": ". | CS274B (UCI) - Canvas . | This is a course taught by Erik Sudderth @ UC Irvine (I think better than Stanford) | It goes into probabilistic graphical models (just more bayesian probability) | Going through material + homework will get you very comfortable with probability (useful for research) | . | Matrix Calculus - Youtube | Transformer Circuits! - Website . | Listed are insightful posts I think are worth reading. | A mathematical framework for Transformer Circuits. | Comes with exercises | . | In-context Learning and Induction Heads | Circuit Tracking: Revealing Computational Graphs on LMs | Biology of LLMs | . | Interpreting GPT - Website | Vision Circuits! - Website | . ",
    "url": "/blog/ml-git-gud-notes/#4-theoretical-stuff",
    
    "relUrl": "/ml-git-gud-notes/#4-theoretical-stuff"
  },"184": {
    "doc": "ML (Git Gud) Resources",
    "title": "5. Learning more about ML Research",
    "content": ". | Yannic Kilcher - YouTube . | Better understanding of ML PhD life - Video | He has a lot of videos on ML research papers. He does a good job of explaining the concepts and the math behind them. | I read the same papers he does a video on and compare my analysis to his to see what I’m missing. | . | Aleksa Gordic - Youtube . | This guy does the same sort of content as Yannic Kilcher for papers. I like having another datapoint to compare. Also multiple sources help you keep up to date with current hype ML stuff. | Exclusive stuff from his channel: . | Deep dives into codebases from papers he also reviews! (DINO, DETR, Diffusion,…etc.) | Livestreams himself implementing super large models. | . | . | . ",
    "url": "/blog/ml-git-gud-notes/#5-learning-more-about-ml-research",
    
    "relUrl": "/ml-git-gud-notes/#5-learning-more-about-ml-research"
  },"185": {
    "doc": "ML (Git Gud) Resources",
    "title": "ML (Git Gud) Resources",
    "content": "This is a collection of resources I’ve used to try and git gud in ML. Hopefully, I can come back to these or someone will find use in it. I’m not going to list papers I’ve read since I think it’s much better to go through compiled information first. This isn’t a sufficient condition to get started in ML Research, but I think it’s necessary to have gathered skills from these resources to be able to do research. ",
    "url": "/blog/ml-git-gud-notes/",
    
    "relUrl": "/ml-git-gud-notes/"
  },"186": {
    "doc": "Deep Dive Papers",
    "title": "Deep Dive Papers",
    "content": "This is a collection of notes on papers I am interested in. ",
    "url": "/blog/paper-notes/",
    
    "relUrl": "/paper-notes/"
  },"187": {
    "doc": "GROOT",
    "title": "GROOT",
    "content": " ",
    "url": "/blog/code-notes/groot/",
    
    "relUrl": "/code-notes/groot/"
  },"188": {
    "doc": "Cluster Notes",
    "title": "Cluster Notes",
    "content": " ",
    "url": "/blog/code-notes/cluster-notes/",
    
    "relUrl": "/code-notes/cluster-notes/"
  },"189": {
    "doc": "Isaac Lab",
    "title": "Isaac Lab",
    "content": "I am currentlyy migrating from Isaac Gym to Isaac Lab. These notes will document the process of using Isaac Lab. ",
    "url": "/blog/code-notes/isaaclab/",
    
    "relUrl": "/code-notes/isaaclab/"
  },"190": {
    "doc": "Isaac Lab",
    "title": "Isaac Ecosystem",
    "content": "Isaac Gym is GPU-based physics simulation for robot learning. It is really old and terrible. It has been deprecated since. This current setup does not support ROS, deformable object interactions, and high-fidelity rendering. Isaac Sim is the new shit. OmniIsaacGymEnvs is the replacement for IsaacGymEnvs when using Isaac Sim. Isaac Lab is the replacement for OmniIsaacGymEnvs and is the single robot learning framework for Isaac Sim. ",
    "url": "/blog/code-notes/isaaclab/#isaac-ecosystem",
    
    "relUrl": "/code-notes/isaaclab/#isaac-ecosystem"
  },"191": {
    "doc": "Isaac",
    "title": "Isaac",
    "content": " ",
    "url": "/blog/code-notes/isaac/",
    
    "relUrl": "/code-notes/isaac/"
  },"192": {
    "doc": "Robocasa",
    "title": "Robocasa",
    "content": " ",
    "url": "/blog/code-notes/benchmark-notes/robocasa/",
    
    "relUrl": "/code-notes/benchmark-notes/robocasa/"
  },"193": {
    "doc": "Libero",
    "title": "Libero",
    "content": " ",
    "url": "/blog/code-notes/benchmark-notes/libero/",
    
    "relUrl": "/code-notes/benchmark-notes/libero/"
  },"194": {
    "doc": "Benchmarks",
    "title": "Benchmarks",
    "content": " ",
    "url": "/blog/code-notes/benchmark-notes/",
    
    "relUrl": "/code-notes/benchmark-notes/"
  },"195": {
    "doc": "Code Notes",
    "title": "Code Notes",
    "content": " ",
    "url": "/blog/code-notes/",
    
    "relUrl": "/code-notes/"
  },"196": {
    "doc": "Quick Notes on Graphs",
    "title": "Quick Notes on Graphs",
    "content": "My quick notes on graph stuff based on some articles. ",
    "url": "/blog/graph-notes/",
    
    "relUrl": "/graph-notes/"
  },"197": {
    "doc": "Judo Notes",
    "title": "Judo Notes",
    "content": "My notes on Judo, including techniques, training methods, and strategies. ",
    "url": "/blog/judo-notes/",
    
    "relUrl": "/judo-notes/"
  },"198": {
    "doc": "Limit Surface Planner",
    "title": "Limit Surface Planner",
    "content": " ",
    "url": "/blog/deep-dive-notes/limit-surface/",
    
    "relUrl": "/deep-dive-notes/limit-surface/"
  },"199": {
    "doc": "Limit Surface Planner",
    "title": "Understanding Dual Limit Surfaces",
    "content": "Dual limit surfaces help encode the relationship between rotary motion and translational motion during planar sliding. A limit surface is described as: . \\[\\begin{align} \\mathbf{w}^T \\mathbf{A}+ \\mathbf{w} = 1 \\\\ \\mathbf{A} = \\begin{bmatrix} \\frac{1}{(\\mu F_N)^2} &amp; 0 &amp; 0 \\\\ 0 &amp; \\frac{1}{(\\mu F_N)^2} &amp; 0 \\\\ 0 &amp; 0 &amp; \\frac{1}{(\\mu r c F_N)^2} \\end{bmatrix} \\end{align}\\] where \\(\\mathbf{w} = [F_x, F_y, \\tau]^T\\) represents the generalized friction force (with \\(F_x, F_y\\) being the frictional forces in the x and y directions, and \\(\\tau\\) being the torque about the vertical axis), and \\(A\\) is a positive definite matrix that defines the shape of the limit surface. \\(c \\approx 0.6\\). Using maximum dissipation principle, we can relate the generalized velocity \\(\\mathbf{v} = [V_x, V_y, \\omega]^T\\) to the generalized friction force \\(\\mathbf{w}\\) as follows: . \\[\\begin{align*} \\mathbf{w}^T \\mathbf{A} \\mathbf{w} &amp;= 1 \\\\ \\lambda \\frac{\\partial}{\\partial \\mathbf{w}} (\\mathbf{w}^T \\mathbf{A} \\mathbf{w}) &amp;= \\mathbf{v} \\\\ \\mathbf{A} = \\mathbf{A}^T \\space &amp;\\text{by definition.} \\\\ 2 \\lambda \\mathbf{A} \\mathbf{w} &amp;= \\mathbf{v} \\\\ \\mathbf{w} &amp;= \\frac{1}{2 \\lambda} \\mathbf{A}^{-1} \\mathbf{v} \\\\ \\text{Substituting into } \\mathbf{w}^T \\mathbf{A} \\mathbf{w} &amp;= 1 \\\\ \\left(\\frac{1}{2 \\lambda} \\mathbf{A}^{-1} \\mathbf{v}\\right)^T \\mathbf{A} \\left(\\frac{1}{2 \\lambda} \\mathbf{A}^{-1} \\mathbf{v}\\right) &amp;= 1 \\\\ \\frac{1}{4 \\lambda^2} \\mathbf{v}^T \\mathbf{A}^{-1} \\mathbf{v} &amp;= 1 \\\\ \\lambda &amp;= \\frac{1}{2} \\sqrt{\\mathbf{v}^T \\mathbf{A}^{-1} \\mathbf{v}} \\\\ \\mathbf{w} &amp;= \\frac{\\mathbf{A}^{-1} \\mathbf{v}}{\\sqrt{\\mathbf{v}^T \\mathbf{A}^{-1} \\mathbf{v}}} \\end{align*}\\] The final equation is now: \\(\\begin{equation} \\mathbf{w} = \\frac{\\mathbf{A}^{-1} \\mathbf{v}}{\\sqrt{\\mathbf{v}^T \\mathbf{A}^{-1} \\mathbf{v}}} \\end{equation}\\) . For dual limit surfaces, we have two limit surfaces (two patch contacts). We use this to compute a velocity constraint for no-slippage planning. \\[\\begin{equation} \\mathbf{w_1}^T \\mathbf{A}_1 \\mathbf{w_1} = 1 \\\\ \\mathbf{w_2}^T \\mathbf{A}_2 \\mathbf{w_2} &lt; 1 \\\\ \\text{Force balance gives us} \\space \\mathbf{w}_1 + \\mathbf{w}_2 = 0 \\end{equation}\\] where you want to slip on \\(A_1\\) and not slip on \\(A_2\\). This gives you a velocity constraint if you plug in the relationship between \\(\\mathbf{w}\\) and \\(\\mathbf{v}\\). ",
    "url": "/blog/deep-dive-notes/limit-surface/#understanding-dual-limit-surfaces",
    
    "relUrl": "/deep-dive-notes/limit-surface/#understanding-dual-limit-surfaces"
  },"200": {
    "doc": "Limit Surface Planner",
    "title": "Bimanual Dual Limit Surfaces",
    "content": "So the difference here is our force balance equation will have gravity affect the object. \\[\\begin{align*} \\mathbf{w}_a \\mathbf{A}_a \\mathbf{w}_a &amp;= 1 \\\\ \\mathbf{w}_b \\mathbf{B}_b \\mathbf{w}_b &amp;&lt; 1 \\\\ \\mathbf{w}_a + \\mathbf{w}_b + \\mathbf{g}_f = 0 \\end{align*}\\] where \\(\\mathbf{g}_f\\) is projection of gravity onto the plane of contact. \\[\\begin{align*} \\mathbf{w}_a \\mathbf{B} \\mathbf{w}_a + 2 \\mathbf{g}_f^T \\mathbf{B} \\mathbf{w}_a + \\mathbf{g}_f^T \\mathbf{B} \\mathbf{g}_f &lt; 1 \\\\ \\mathbf{w}_a^T (\\mathbf{B} - \\mathbf{A}) \\mathbf{w}_a + 2 \\mathbf{g}_f^T \\mathbf{B} \\mathbf{w}_a + \\mathbf{g}_f^T \\mathbf{B} \\mathbf{g}_f &lt; 0 \\\\ \\mathbf{v}^T (\\mathbf{A}^{-1} - \\mathbf{B} \\mathbf{A}^{-1} - \\mathbf{A}^{-1}) \\mathbf{v} - 2 (\\sqrt{\\mathbf{v}^T \\mathbf{A}^{-1} \\mathbf{v}}) \\mathbf{v}^T \\mathbf{A}^{-1} \\mathbf{B} \\mathbf{g}_f + (\\mathbf{g}_f^T \\mathbf{B} \\mathbf{g}_f) (\\mathbf{v}^T \\mathbf{A}^{-1} \\mathbf{v}) &lt; 0 \\end{align*}\\] By considering special cases (like patch contact radius is the same, or friction coefficients are the same), we can simplify the above equation. However, people have found this use case too limiting. This is where the project died. ",
    "url": "/blog/deep-dive-notes/limit-surface/#bimanual-dual-limit-surfaces",
    
    "relUrl": "/deep-dive-notes/limit-surface/#bimanual-dual-limit-surfaces"
  },"201": {
    "doc": "Tactile RL Notes",
    "title": "Tactile RL Notes",
    "content": "The core focus of this topic is using tactile sensors for policy learning. Tactile policy learning is currently a hot topic in robotic manipulation, and there have been many strides made to leverage tactile feedback in robotics. Gelsight Mini is 9x7 . Lets start off with a simple . | Manipulation via Membranes: CoRL 2023 | VT-Refine CoRL 2025 | . ",
    "url": "/blog/deep-dive-notes/tactile-rl-notes/",
    
    "relUrl": "/deep-dive-notes/tactile-rl-notes/"
  },"202": {
    "doc": "Debug Point ML",
    "title": "Debugging Point ML",
    "content": "Currently we are using point policies for our project. We are plugging it in raw and are finding issues just in the inclusion of the pointnet. ",
    "url": "/blog/deep-dive-notes/debug-point-ml/#debugging-point-ml",
    
    "relUrl": "/deep-dive-notes/debug-point-ml/#debugging-point-ml"
  },"203": {
    "doc": "Debug Point ML",
    "title": "Debug Point ML",
    "content": " ",
    "url": "/blog/deep-dive-notes/debug-point-ml/",
    
    "relUrl": "/deep-dive-notes/debug-point-ml/"
  },"204": {
    "doc": "Deep Dive Ideas",
    "title": "Deep Dive Ideas",
    "content": "This is a collection of deep dives on concepts. Normally I do this when getting into concepts or debugging things I don’t have a strong understanding in. ",
    "url": "/blog/deep-dive-notes/",
    
    "relUrl": "/deep-dive-notes/"
  },"205": {
    "doc": "AEROSP 740: Online Learning for Control",
    "title": "AEROSP 740: Online Learning for Control",
    "content": " ",
    "url": "/blog/class-notes/02-Online-Learning-Control/",
    
    "relUrl": "/class-notes/02-Online-Learning-Control/"
  },"206": {
    "doc": "Flow Matching and Diffusion",
    "title": "6.S184 Flow Matching",
    "content": " ",
    "url": "/blog/class-notes/06-Flow-Matching/#6s184-flow-matching",
    
    "relUrl": "/class-notes/06-Flow-Matching/#6s184-flow-matching"
  },"207": {
    "doc": "Flow Matching and Diffusion",
    "title": "Flow Matching and Diffusion",
    "content": " ",
    "url": "/blog/class-notes/06-Flow-Matching/",
    
    "relUrl": "/class-notes/06-Flow-Matching/"
  },"208": {
    "doc": "Deep Multi-Task and Meta Learning",
    "title": "CS 330",
    "content": " ",
    "url": "/blog/class-notes/08-Deep-Multitask-Learning/#cs-330",
    
    "relUrl": "/class-notes/08-Deep-Multitask-Learning/#cs-330"
  },"209": {
    "doc": "Deep Multi-Task and Meta Learning",
    "title": "Deep Multi-Task and Meta Learning",
    "content": " ",
    "url": "/blog/class-notes/08-Deep-Multitask-Learning/",
    
    "relUrl": "/class-notes/08-Deep-Multitask-Learning/"
  },"210": {
    "doc": "658-Lectures",
    "title": "658-Lectures",
    "content": " ",
    "url": "/blog/class-notes/00-Geometric-Mechanics/Lectures/",
    
    "relUrl": "/class-notes/00-Geometric-Mechanics/Lectures/"
  },"211": {
    "doc": "Textbook",
    "title": "Textbook",
    "content": " ",
    "url": "/blog/class-notes/00-Geometric-Mechanics/Textbook/",
    
    "relUrl": "/class-notes/00-Geometric-Mechanics/Textbook/"
  },"212": {
    "doc": "MATH 658: Geometric Mechanics",
    "title": "MATH 658: Geometric Mechanics",
    "content": " ",
    "url": "/blog/class-notes/00-Geometric-Mechanics/",
    
    "relUrl": "/class-notes/00-Geometric-Mechanics/"
  },"213": {
    "doc": "MATH 540: Mathematics for Biological Networks",
    "title": "MATH 540: Mathematics for Biological Networks",
    "content": " ",
    "url": "/blog/class-notes/01-Math-of-Bio-Networks/",
    
    "relUrl": "/class-notes/01-Math-of-Bio-Networks/"
  },"214": {
    "doc": "Deep Reinforcement Learning",
    "title": "CS 285",
    "content": " ",
    "url": "/blog/class-notes/07-Deep-RL/#cs-285",
    
    "relUrl": "/class-notes/07-Deep-RL/#cs-285"
  },"215": {
    "doc": "Deep Reinforcement Learning",
    "title": "Deep Reinforcement Learning",
    "content": " ",
    "url": "/blog/class-notes/07-Deep-RL/",
    
    "relUrl": "/class-notes/07-Deep-RL/"
  },"216": {
    "doc": "Sensorimotor Learning",
    "title": "6.484 Computational Sensorimotor Learning",
    "content": " ",
    "url": "/blog/class-notes/04-Computational-Sensorimotor-Learning/#6484-computational-sensorimotor-learning",
    
    "relUrl": "/class-notes/04-Computational-Sensorimotor-Learning/#6484-computational-sensorimotor-learning"
  },"217": {
    "doc": "Sensorimotor Learning",
    "title": "Sensorimotor Learning",
    "content": " ",
    "url": "/blog/class-notes/04-Computational-Sensorimotor-Learning/",
    
    "relUrl": "/class-notes/04-Computational-Sensorimotor-Learning/"
  },"218": {
    "doc": "Algebraic Techniques for Optimization",
    "title": "6.256",
    "content": " ",
    "url": "/blog/class-notes/09-Algebraic-Techniques-Optimization/#6256",
    
    "relUrl": "/class-notes/09-Algebraic-Techniques-Optimization/#6256"
  },"219": {
    "doc": "Algebraic Techniques for Optimization",
    "title": "Algebraic Techniques for Optimization",
    "content": " ",
    "url": "/blog/class-notes/09-Algebraic-Techniques-Optimization/",
    
    "relUrl": "/class-notes/09-Algebraic-Techniques-Optimization/"
  },"220": {
    "doc": "Unsupervised Learning",
    "title": "CS-294: Unsupervised Learning",
    "content": " ",
    "url": "/blog/class-notes/05-Unsupervised-Learning/#cs-294-unsupervised-learning",
    
    "relUrl": "/class-notes/05-Unsupervised-Learning/#cs-294-unsupervised-learning"
  },"221": {
    "doc": "Unsupervised Learning",
    "title": "Unsupervised Learning",
    "content": " ",
    "url": "/blog/class-notes/05-Unsupervised-Learning/",
    
    "relUrl": "/class-notes/05-Unsupervised-Learning/"
  },"222": {
    "doc": "ECE 598: Convex Optimization Methods in Control",
    "title": "ECE 598: Convex Optimization Methods in Control",
    "content": " ",
    "url": "/blog/class-notes/03-CVOPT-Control/",
    
    "relUrl": "/class-notes/03-CVOPT-Control/"
  },"223": {
    "doc": "Class Notes",
    "title": "Class Notes",
    "content": "Collection of class notes that I go through. ",
    "url": "/blog/class-notes/",
    
    "relUrl": "/class-notes/"
  },"224": {
    "doc": "Lit Review (Graph Insertion)",
    "title": "Lit Review (Graph Insertion)",
    "content": " ",
    "url": "/blog/project-notes/project1/lit_review/",
    
    "relUrl": "/project-notes/project1/lit_review/"
  },"225": {
    "doc": "Graph Insertion",
    "title": "Project 1: Graph Insertion",
    "content": "Everything I do in this project must satisfy Amazon’s requirements for dense insertion. ",
    "url": "/blog/project-notes/project1/#project-1-graph-insertion",
    
    "relUrl": "/project-notes/project1/#project-1-graph-insertion"
  },"226": {
    "doc": "Graph Insertion",
    "title": "Dense Insertion",
    "content": "Dense Insertion is a task where a robot arm must autonomously insert a set of objects into a bin. As more objects are inserted into a bin, it gets harder to insert more objects. I don’t know what the proposal was for this project, but I think it involves using extrinsic contact to figure out how to best insert an object. ",
    "url": "/blog/project-notes/project1/#dense-insertion",
    
    "relUrl": "/project-notes/project1/#dense-insertion"
  },"227": {
    "doc": "Graph Insertion",
    "title": "Dynamics+MPC vs. RL vs. IL",
    "content": "As of the time of writing, we are heading towards using RL. I just want to make sure that this is the correct choice. Dynamics+MPC is a method we can try that involves learning the dynamics of the system and utilizing sampling-based MPC to solve the task by designing a cost function. This method was used in Robopack which seemed like it took forever to do dense insertion. This is most likely due to running the MPC for a long time then running the plan in open-loop and hoping it all works. We may run into the same issues if we try to do this. IL is a method we can try which involves curating demonstrations of us doing dense insertion and having a robot imitate us. This is the most popular method for other manipulation tasks, but it is not clear if it will work for dense insertion. IL has issues with generalization where even small changes in the environment can cause the robot to fail. Combine this with different object configurations inside one bin and it is not clear if this will work. RL by process of elimination, I guess RL should work. It doesn’t have the limitations of IL and Dynamics+MPC, but it is super hard to train and make work. ",
    "url": "/blog/project-notes/project1/#dynamicsmpc-vs-rl-vs-il",
    
    "relUrl": "/project-notes/project1/#dynamicsmpc-vs-rl-vs-il"
  },"228": {
    "doc": "Graph Insertion",
    "title": "More Dense Insertion",
    "content": "Simplification While the dense insertion task is actually all about inserting multiple objects into a bin and planning to figure out how to place them such that there is enough space for the next object, we can simplify this to just inserting one object into a bin. Lets consider a diagram of the dense insertion problem. We want to insert an object into a bin. It’s relatively large, so there is no direct space to insert the object. We have to make our own space by pushing other objects out of the way. There are two things to consider: . | We can push objects out of the way if we know that there is space to push them into. | We can push deformable out of the way and hope that they will move out of the way. | . Both of these will create space for us to insert the object. Question: Can we zero-shot figure out the compliance of the scene? . | I don’t think you can figure this out by just looking at the scene. You need to poke around to see if you can insert the object. | Another way is to have this information as a prior. When we insert the objects one-by-one, we should be able to figure out which parts are stiff and which parts are compliant. | . ",
    "url": "/blog/project-notes/project1/#more-dense-insertion",
    
    "relUrl": "/project-notes/project1/#more-dense-insertion"
  },"229": {
    "doc": "Graph Insertion",
    "title": "Extrinsic Contact",
    "content": "If we did an end-to-end approach whether it is MPC+Dynamics, RL, or IL, there are too many configurations that are possible. Just think of the combinatorial enumeration of the object order in the bin. Making an e2e policy that can handle an arbitrary configuration is hard. Instead, we have to break the problem down into pieces. Just like how vision uses convolutions to exploit the symmetry in the data, how can we exploit the “symmetry” in dense insertion? . Simplification 2 , assume we know all of the object geometries and inertial properties. Even with these simplifications, the enumeration problem remains, but we have removed some of the complexity. Lets say we are holding a rigid object and trying to insert it into a bin by pushing away another object (rigid/compliant). Idea 1: We could make a tactile controller to ensure intrinsic object pose is stabilized while pushing our object into the bin. | we can represent the interaction between the end-effector, intrinsic object, and extrinsic object as a graph. | subidea 1.1: we can learn the graph dynamics and use that to stabilize. | subidea 1.2: we can just learn a general controller that can stabilize based on IL/RL. | . This idea might be too explicit and not general enough. Do we even need a tactile controller? . Idea 2: GNN as observation head We construct a graph \\(\\mathcal{G} = (\\mathcal{V}, \\mathcal{E})\\) from scene keypoints including objects, bin, and end-effector, and connect it up using knn-graph scheme. We plan to include extrinsic contact by adding it into the vertex features. We learn the contact forces for each vertex \\(v \\in \\mathcal{V}\\) and whether it’s in contact. \\[\\begin{equation*} v = \\begin{bmatrix} x \\\\ y \\\\ z \\\\ c \\\\ \\mathbf{f}_{c} \\end{bmatrix} \\end{equation*}\\] where \\(c \\in \\{0, 1\\}\\) is a binary variable indicating whether the vertex is in contact or not with the end-effector and \\(\\mathbf{f}_{c}\\) is the contact force. We then do some message-passing and then max-pool the graph to get a single embedding. We then pass this along for policy learning. Pooling Problem We want to utilize extrinsic contact as seen in miquel’s work. However, that work uses the object geometry and forces forces acting on the geometry to calculate the force controller output. Max-pooling takes away the geometrical structure of the graph by compressing it into a single embedding. My real question is whether a single embedding is enough to represent the extrinsic contact structure from miquel’s work (roughly). | Suggestion : Maybe we keep the graph structure and just output forces on each node, and have our robot move such to satisfy force-balance. | . Contact Var Problem I have some issues with this design choice: . | Using the closeness of the objects, could the knn-graph scene not be enough to represent contact? The connectivity between two nodes show that the features of one node (contact forces) will affect the other node. We can think of it as the vertex shooting a message to its neighbors. | If we don’t have the full object graph, the other unseen vertices might be in contact and affect the forces. A human doing dense packing can do shape completion and understand (relatively) which objects are in contact and thus act accordingly. I feel like contact is a sensitive problem (numerically) where one small change can lead to a large change in system behavior. I think we need to be careful about this. | . Main issue with Obs Head GNN We’re kind of just hoping that with forces and GNN, it’ll work itself out. I feel like it’s hard to convince ourselves that this is the right way to go. Key Question: How does explicit modelling extrinsic contact help? In the most abstract-level, extrinsic contact is all about object interaction through forces. Dense Insertion is all about object interaction through forces. In that sense, how should we model object interaction through forces? . Idea 3: Multi-object interaction We can give each object in the scene a full object graph. ",
    "url": "/blog/project-notes/project1/#extrinsic-contact",
    
    "relUrl": "/project-notes/project1/#extrinsic-contact"
  },"230": {
    "doc": "Graph Insertion",
    "title": "Graph Insertion",
    "content": " ",
    "url": "/blog/project-notes/project1/",
    
    "relUrl": "/project-notes/project1/"
  },"231": {
    "doc": "PhD Project Notes",
    "title": "PhD Project Notes",
    "content": "I love taking notes I guess. ",
    "url": "/blog/project-notes/",
    
    "relUrl": "/project-notes/"
  },"232": {
    "doc": "ICRA 2025",
    "title": "ICRA 2025",
    "content": " ",
    "url": "/blog/conference-notes/icra-2025/",
    
    "relUrl": "/conference-notes/icra-2025/"
  },"233": {
    "doc": "Conference Notes",
    "title": "Conference Notes",
    "content": " ",
    "url": "/blog/conference-notes/",
    
    "relUrl": "/conference-notes/"
  },"234": {
    "doc": "Prerequisites",
    "title": "Prerequisites",
    "content": "My notes on general mafs such that I can understand the graph mafs. Now I just want to understand things in general not just graph mafs. ",
    "url": "/blog/prereq-notes/",
    
    "relUrl": "/prereq-notes/"
  },"235": {
    "doc": "Kuka FRI Notes",
    "title": "Kuka FRI Notes",
    "content": "My notes on Kuka Iiwa robots, Drake, and FRI software. FRI stands for Fast Robot Interface. It is a software interface on the Kuka that allows for real-time control of the robot. It is a low-level backend accessible via the Kuka Sunrise. The Kuka FRI driver has different control modes that are accessible: . | Position Control: This is the internal PID that controls robot joint positions accurate up to 1mm of error. | Impedance Control: This is a control mode that allows for robot to be controlled in a compliant manner, where the dynamics for each joint joint act like a spring-damper system. You can feed feedforward joint torques and commanded joint positions to the robot. | Torque Control: This is the raw torque control mode that allows for direct control of the robot’s joint torques. It is the most low-level control mode and requires careful tuning to avoid damaging the robot. It already takes into account the gravity compensation of the Kuka Robot. | Cartesian Impedance: This is a control mode that allows for control of the robot’s end-effector in Cartesian space. It is similar to impedance control, but it allows for control of the end-effector position and orientation in Cartesian space and spring-damper is in Cartesian space. NO Implementation for it currently in Drake. | . ",
    "url": "/blog/kuka-notes/",
    
    "relUrl": "/kuka-notes/"
  },"236": {
    "doc": "IsaacSim Core (Hello World)",
    "title": "IsaacSim Core (Hello World)",
    "content": " ",
    "url": "/blog/code-notes/isaaclab/isaacsim-hello-world.html",
    
    "relUrl": "/code-notes/isaaclab/isaacsim-hello-world.html"
  },"237": {
    "doc": "Isaac Sim Python Start",
    "title": "IsaacSim Core",
    "content": "Understanding isaacsim core is important because that is what TacEX uses mostly. Adding objects using Core API is as simple as this… . import numpy as np from isaacsim.core.api.objects import DynamicCuboid DynamicCuboid( prim_path=\"/new_cube_2\", name=\"cube_1\", position=np.array([0, 0, 1.0]), scale=np.array([0.6, 0.5, 0.2]), size=1.0, color=np.array([255, 0, 0]), ) . Actually, this is a wrapper around the physics engine APIs and raw USD APIs. ",
    "url": "/blog/code-notes/isaaclab/isaacsim-start.html#isaacsim-core",
    
    "relUrl": "/code-notes/isaaclab/isaacsim-start.html#isaacsim-core"
  },"238": {
    "doc": "Isaac Sim Python Start",
    "title": "Application vs. Simulation vs. World vs. Scene vs. Stage",
    "content": "Everything in USD is a primitive with attributes. Simulation: moves primitives forward in time. Application: manages gross aspects of simulation (rendering and interaction). Stage: (from USD) defines logical and relational context for primitives in simulation. | example: mug prim on table prim. relative location of mug on table is defined in stage. | . Scene: manages collection of primitives in stage. World: provides context to simulation and manages user-important aspects of simulation. Analogy from isaacsim omniverse is that we are in a theater: . | application is theater | simulation is the play | we take a seat and see the stage. | when play starts, curtain reveals a scene. | when curtain falls, scene resets. | stage crew and mechanical devices behind the scene that manages the curtain and props is the world. | . ",
    "url": "/blog/code-notes/isaaclab/isaacsim-start.html#application-vs-simulation-vs-world-vs-scene-vs-stage",
    
    "relUrl": "/code-notes/isaaclab/isaacsim-start.html#application-vs-simulation-vs-world-vs-scene-vs-stage"
  },"239": {
    "doc": "Isaac Sim Python Start",
    "title": "Python",
    "content": " ",
    "url": "/blog/code-notes/isaaclab/isaacsim-start.html#python",
    
    "relUrl": "/code-notes/isaaclab/isaacsim-start.html#python"
  },"240": {
    "doc": "Isaac Sim Python Start",
    "title": "Quickly running python scripts",
    "content": "In the Isaac Sim codebase, there is python.sh file located in source/scripts/python/linux-x86_64. Here’s a quick breakdown on how to get it running. SCRIPT_DIR=\"$( cd \"$( dirname \"${BASH_SOURCE[0]}\" )\" &amp;&amp; pwd )\" # MY_DIR=\"$(realpath -s \"$SCRIPT_DIR\")\" # Setup python env from generated file (generated by tools/repoman/build.py) export CARB_APP_PATH=$SCRIPT_DIR/kit export ISAAC_PATH=$SCRIPT_DIR export EXP_PATH=$SCRIPT_DIR/apps source ${SCRIPT_DIR}/setup_python_env.sh . Variables: . | EXP_PATH: apps folder | SCRIPT_DIR: main isaac folder | ISAAC_PATH: same as SCRIPT_DIR | PYTHONPATH: path to each extensions python interfaces | LD_LIBRARY_PATH: path to binary interfaces so that we can find binary symbols at runtime. | CARB_APP_PATH: path to core omniverse kit executable. | . python_exe=${PYTHONEXE:-\"${SCRIPT_DIR}/kit/python/bin/python3\"} # this means if you ran source python.sh arg1 arg2 arg2, it'll pop up in $@.... # in other words this takes your arguments (script.py arg2 arg3 ...) and passes it to the python $python_exe $@ . In other words, running ./python.sh script.py arg1 arg2 will execute python3 script.py arg1 arg2 with the python interpreter located in kit/python/bin/python3. Before that, it sets up all the appropriate environment variables so that the python interpreter can find all the necessary libraries. ",
    "url": "/blog/code-notes/isaaclab/isaacsim-start.html#quickly-running-python-scripts",
    
    "relUrl": "/code-notes/isaaclab/isaacsim-start.html#quickly-running-python-scripts"
  },"241": {
    "doc": "Isaac Sim Python Start",
    "title": "Simulation App",
    "content": "Now that we can run python scripts, we can start doing some coding. from isaacsim import SimulationApp # Simple example showing how to start and stop the helper simulation_app = SimulationApp({\"headless\": True}) ### Perform any omniverse imports here after the helper loads ### simulation_app.update() # Render a single frame simulation_app.close() # Cleanup application . This is a really quick and easy way to get an app running. ",
    "url": "/blog/code-notes/isaaclab/isaacsim-start.html#simulation-app",
    
    "relUrl": "/code-notes/isaaclab/isaacsim-start.html#simulation-app"
  },"242": {
    "doc": "Isaac Sim Python Start",
    "title": "How SimulationApp works",
    "content": "import carb import omni.kit.app # start up carbonite framework. framework = carb.get_framework() framework.load_plugins( loaded_file_wildcards=[\"omni.kit.app.plugin\"], search_paths=[os.path.abspath(f'{os.environ[\"CARB_APP_PATH\"]}/kernel/plugins')], ) # Inject a experience config sys.argv.insert(1, f'{os.environ[\"EXP_PATH\"]}/isaacsim.exp.base.python.kit') # Add paths to extensions sys.argv.append(f\"--ext-folder\") sys.argv.append(f'{os.path.abspath(os.environ[\"ISAAC_PATH\"])}/exts') # Run headless sys.argv.append(\"--no-window\") # start application app = omni.kit.app.get_app() app.startup(\"Isaac-Sim\", os.environ[\"CARB_APP_PATH\"], sys.argv) # shutting down application app.shutdown() framework.unload_all_plugins() . ",
    "url": "/blog/code-notes/isaaclab/isaacsim-start.html#how-simulationapp-works",
    
    "relUrl": "/code-notes/isaaclab/isaacsim-start.html#how-simulationapp-works"
  },"243": {
    "doc": "Isaac Sim Python Start",
    "title": "Enabling additional extensions",
    "content": "from isaacsim import SimulationApp # Start the application simulation_app = SimulationApp({\"headless\": False}) # Get the utility to enable extensions from isaacsim.core.utils.extensions import enable_extension # Enable the layers and stage windows in the UI enable_extension(\"omni.kit.widget.stage\") enable_extension(\"omni.kit.widget.layers\") simulation_app.update() . ",
    "url": "/blog/code-notes/isaaclab/isaacsim-start.html#enabling-additional-extensions",
    
    "relUrl": "/code-notes/isaaclab/isaacsim-start.html#enabling-additional-extensions"
  },"244": {
    "doc": "Isaac Sim Python Start",
    "title": "Isaac Sim Python Start",
    "content": " ",
    "url": "/blog/code-notes/isaaclab/isaacsim-start.html",
    
    "relUrl": "/code-notes/isaaclab/isaacsim-start.html"
  },"245": {
    "doc": "Manipulation Keynote",
    "title": "Manipulation Keynote",
    "content": " ",
    "url": "/blog/conference-notes/icra-2025/keynote-manipulation.html",
    
    "relUrl": "/conference-notes/icra-2025/keynote-manipulation.html"
  },"246": {
    "doc": "Manipulation Keynote",
    "title": "Tamim Asfour - Bimanual Taxonomy",
    "content": "He breaks down taxonomy as: . | noncoupled . | left arm | right arm | . | noncoupled . | loosely coupled | tighlty coupled | . | . Just a way to categorize manipulation. He has also used object graphs to relate objects. ",
    "url": "/blog/conference-notes/icra-2025/keynote-manipulation.html#tamim-asfour---bimanual-taxonomy",
    
    "relUrl": "/conference-notes/icra-2025/keynote-manipulation.html#tamim-asfour---bimanual-taxonomy"
  },"247": {
    "doc": "Manipulation Keynote",
    "title": "Yasuhisa Hasegawa - Robot Mobility Support",
    "content": " ",
    "url": "/blog/conference-notes/icra-2025/keynote-manipulation.html#yasuhisa-hasegawa---robot-mobility-support",
    
    "relUrl": "/conference-notes/icra-2025/keynote-manipulation.html#yasuhisa-hasegawa---robot-mobility-support"
  },"248": {
    "doc": "Manipulation Keynote",
    "title": "Alberto Rodriguez - Humanoids",
    "content": "Revelations: . | Factory work is really hard (whole-body tasks) and haptic driven. | Many of these factory tasks aren’t automated | . flowchart TD A(factory tasks) --&gt; B(material handling) A(factory tasks) --&gt; C(General Assembly) . material handling: . | sequencing | kitting | racking | placing part in jigs | . general assembly . | tight fit insertions | connecting wires | fastening bolts | . Tough requirements: . | 99.x% reliability | high precision (work dominated by haptic tasks) | path to simple re-tasking | path for early deployments in assembly lines | . Journey: . | perception model -&gt; VFM | mission tasking -&gt; LLM -&gt; VLM + VLA | manipulation skills -&gt; BC or RL | whole body control -&gt; whole-body RL | . example eAtlas doing RL pick and place using RGB: . | distill to RGB stereo | works pretty well | . whole-body control and mobility in RL . | robust sim-2-real . | many specialist policies starting from human motion data | . | scale sim training to build generalist body policy. | . note on teleoperation: . | behaviors are inefficient. | lacking dynamic content | unnecessarily sequential | . ",
    "url": "/blog/conference-notes/icra-2025/keynote-manipulation.html#alberto-rodriguez---humanoids",
    
    "relUrl": "/conference-notes/icra-2025/keynote-manipulation.html#alberto-rodriguez---humanoids"
  },"249": {
    "doc": "Manipulation Keynote",
    "title": "Shuran Song - Making manipulation as simple as possible but not simpler",
    "content": "Learn from natural human demos So innovation is on teleop (noice). | have to think like a robot to teleop. | disconnects operator from contact (no haptic feedback) | requires robot-in-the-loop | . In the natural setting, we use our human hands to do a task, but that is not transferable to robots; . | this is the embodiment gap | . UMI is becoming very universal (as the name entails). ",
    "url": "/blog/conference-notes/icra-2025/keynote-manipulation.html#shuran-song---making-manipulation-as-simple-as-possible-but-not-simpler",
    
    "relUrl": "/conference-notes/icra-2025/keynote-manipulation.html#shuran-song---making-manipulation-as-simple-as-possible-but-not-simpler"
  },"250": {
    "doc": "Optimization Control Keynote",
    "title": "Optimization and Control Keynote",
    "content": " ",
    "url": "/blog/conference-notes/icra-2025/keynote-opt.html#optimization-and-control-keynote",
    
    "relUrl": "/conference-notes/icra-2025/keynote-opt.html#optimization-and-control-keynote"
  },"251": {
    "doc": "Optimization Control Keynote",
    "title": "Ram Vasudevan",
    "content": "In contrast to Vision-Language-Action (VLA) model. Requires a lot of data to train. | Physics intuition | . If you’re an optimization person, you should try to pivot. Current ways of using models may not be that useful. Using it as a “shield”. Things like peripherlas for the end2end model. Open Problems to beyond the demo: . | Quantifying error (conformal prediction) | Fuse Physics + Perception into Learning | Explain VLA generalization | . ",
    "url": "/blog/conference-notes/icra-2025/keynote-opt.html#ram-vasudevan",
    
    "relUrl": "/conference-notes/icra-2025/keynote-opt.html#ram-vasudevan"
  },"252": {
    "doc": "Optimization Control Keynote",
    "title": "Yana",
    "content": "Signal Temporal Logic for robust control and safety-based control. I think it’s too abstract and it doesn’t necessarily break down how STL creates an optimization program. Logic is like a 0/1 boolean. How is that relaxed and what does that translate to for optimization? . ",
    "url": "/blog/conference-notes/icra-2025/keynote-opt.html#yana",
    
    "relUrl": "/conference-notes/icra-2025/keynote-opt.html#yana"
  },"253": {
    "doc": "Optimization Control Keynote",
    "title": "Optimization Control Keynote",
    "content": " ",
    "url": "/blog/conference-notes/icra-2025/keynote-opt.html",
    
    "relUrl": "/conference-notes/icra-2025/keynote-opt.html"
  },"254": {
    "doc": "Latent GNNs",
    "title": "Latent GNNs (Manifold Learning?)",
    "content": " ",
    "url": "/blog/graph-notes/latent-gnn.html#latent-gnns-manifold-learning",
    
    "relUrl": "/graph-notes/latent-gnn.html#latent-gnns-manifold-learning"
  },"255": {
    "doc": "Latent GNNs",
    "title": "Relation with PointNet",
    "content": "Bronstein and Yue Wang both worked on using graphs as coarse representations of the object manifold instead of dense point clouds. With PointNet, you apply some shared learnable point-wise function to each point. Additionally, they applied message-passing on a graph. More can be explored in Dynamic Graph CNN (DGCNN). A key observation of Yue was that the graph structure didn’t have to be static throughout, and it could/should be updated dynamically. Example of DGCNN being used for point cloud segmentation in place of PointNet. The graph is updated dynamically, and the graph features are used to compute the final point-wise features. Limitations: . | This approach generated graph and graph features on the same latent embedding space. | . A followup work called Dynamic Graph Module (DGM) decouples graph and feature construction. This work has impressive results in medical domain like disease prediction on brain imaging data and other patient data features. DGM also beat DGCNN on point cloud classification/segmentation tasks. ",
    "url": "/blog/graph-notes/latent-gnn.html#relation-with-pointnet",
    
    "relUrl": "/graph-notes/latent-gnn.html#relation-with-pointnet"
  },"256": {
    "doc": "Latent GNNs",
    "title": "DGCNN",
    "content": ". ",
    "url": "/blog/graph-notes/latent-gnn.html#dgcnn",
    
    "relUrl": "/graph-notes/latent-gnn.html#dgcnn"
  },"257": {
    "doc": "Latent GNNs",
    "title": "DGM",
    "content": ". ",
    "url": "/blog/graph-notes/latent-gnn.html#dgm",
    
    "relUrl": "/graph-notes/latent-gnn.html#dgm"
  },"258": {
    "doc": "Latent GNNs",
    "title": "Connection to manifold learning",
    "content": "These two models bear similarity to manifold learning or non-linear dimensionality reduction algorithms. The underlying assumption is that the data lies on a low-dimensional manifold embedded in a high-dimensional space. The purpose of manifold learning is to capture these low-dim structures by reconstructing the manifold. This is unlike PCA which is a linear dimensionality reduction algorithm. Left uses linear projection which leaves clumps of data on the line. Right uses non-linear projection which captures the manifold structure of the data and the data is more evenly distributed on the line. ",
    "url": "/blog/graph-notes/latent-gnn.html#connection-to-manifold-learning",
    
    "relUrl": "/graph-notes/latent-gnn.html#connection-to-manifold-learning"
  },"259": {
    "doc": "Latent GNNs",
    "title": "Manifold Learning",
    "content": "Manifold learning vary in how they recover a “manifold”, but they share a blueprint. | Create representation of the data, typically done with a k-nearest neighbor graph. | (where methods differ) Compute low-dim representation (embedding) of data trying preserve structure of original data. Isomap tries to preserve graph geodesic distance, Locally Linear Embedding (LLE) finds local representation of adjacent points, and Laplacian eigenmaps uses eigenfunctions of the graph Laplacian operator as low-dim embedding. | Once representation is computed, an ML algorithm (clustering) is applied to it. | . ",
    "url": "/blog/graph-notes/latent-gnn.html#manifold-learning",
    
    "relUrl": "/graph-notes/latent-gnn.html#manifold-learning"
  },"260": {
    "doc": "Latent GNNs",
    "title": "Isomap",
    "content": ". ",
    "url": "/blog/graph-notes/latent-gnn.html#isomap",
    
    "relUrl": "/graph-notes/latent-gnn.html#isomap"
  },"261": {
    "doc": "Latent GNNs",
    "title": "Locally Linear Embedding (LLE)",
    "content": ". ",
    "url": "/blog/graph-notes/latent-gnn.html#locally-linear-embedding-lle",
    
    "relUrl": "/graph-notes/latent-gnn.html#locally-linear-embedding-lle"
  },"262": {
    "doc": "Latent GNNs",
    "title": "Laplacian Eigenmaps",
    "content": ". ",
    "url": "/blog/graph-notes/latent-gnn.html#laplacian-eigenmaps",
    
    "relUrl": "/graph-notes/latent-gnn.html#laplacian-eigenmaps"
  },"263": {
    "doc": "Latent GNNs",
    "title": "Graph Deep Learning Perspective on Manifold Learning",
    "content": "Graph Deep Learning combines the 3-steps into a single-step process which is the GNN. It’s a much more modern take and is end2end. The GNN is trained to learn the representation of the data and the graph structure simultaneously. ",
    "url": "/blog/graph-notes/latent-gnn.html#graph-deep-learning-perspective-on-manifold-learning",
    
    "relUrl": "/graph-notes/latent-gnn.html#graph-deep-learning-perspective-on-manifold-learning"
  },"264": {
    "doc": "Latent GNNs",
    "title": "Applications of Latent Graph Learning",
    "content": "List of applications: . | Few-shot learning (graphs can help generalize from a few examples). | Biology (molecules, proteins, … any interaction between two entities) | Analysis of physical systems (multi-object interactions, …). | . ",
    "url": "/blog/graph-notes/latent-gnn.html#applications-of-latent-graph-learning",
    
    "relUrl": "/graph-notes/latent-gnn.html#applications-of-latent-graph-learning"
  },"265": {
    "doc": "Latent GNNs",
    "title": "Latent GNNs",
    "content": " ",
    "url": "/blog/graph-notes/latent-gnn.html",
    
    "relUrl": "/graph-notes/latent-gnn.html"
  },"266": {
    "doc": "5. Learning Demos",
    "title": "5. Learning Demos",
    "content": "This method is fairly straightforward, we provide expert demonstrations of the task and supervise learn the policy. Goal is to learn a state-conditioned policy: \\(\\pi : \\mathcal{S} \\mapsto \\mathcal{A}\\). We are provided with trajectories/demonstrations \\(\\{ \\tau_{1:T}^i \\}_{i=1}^N\\). Assumption: We assume these policies are optimal or near-optimal. | Training time: Stage of training policy \\(\\pi_\\theta\\) using demos. | Testing time: We fix parameters and execute predicted actions. | Expert Policy \\(\\pi_d\\): Expert policy that achieves optimal performance on given task. | Demonstration dataset \\(\\mathcal{D} = \\{ \\tau^i_{1:T} \\}_{i=1}^N\\): constructed from rolling trajectories from the expert policy \\(\\pi_d\\). | . Typically the way we train in simulation is to run a few epochs, then test the policy. ",
    "url": "/blog/class-notes/04-Computational-Sensorimotor-Learning/learning-from-demo.html",
    
    "relUrl": "/class-notes/04-Computational-Sensorimotor-Learning/learning-from-demo.html"
  },"267": {
    "doc": "5. Learning Demos",
    "title": "5.1 Behavior Cloning",
    "content": "One way to learn policy is to match distributions of trajectories generated by \\(\\pi_\\theta\\) and \\(\\pi_d\\). This is known as Behavior Cloning and can be formulated by minimizing KL-divergence between the two distributions. \\[\\begin{equation} \\min_\\theta L(\\theta) = D_{KL}(P_D || P_\\theta) \\end{equation}\\] where \\(P_D\\) denote distributions of trajectories of \\(\\pi_d\\) and \\(\\pi_D\\), respectively. To see connection of gradient \\(L(\\theta)\\) and \\(\\pi_\\theta\\), we define probability of trajectory as \\(P(\\tau)\\): . \\[\\begin{align*} P(\\tau) &amp;= P(s_1, a_1, ..., s_T) \\\\ &amp;= P(s_1) P(a_1 \\mid s_1) ... P(a_{T-1} \\mid s_1, a_1, ..., s_{T-1}) P(s_T \\mid s_1, a_1, ..., a_{T-1}) \\\\ &amp;= P(s_1) \\prod_{t=1}^T P(s_t \\mid s_1, a_1, ..., a_{t-1})P(a_t \\mid s_1, a_1, ..., s_t) \\\\ &amp;= P(s_1) \\prod_{t=1}^T P(s_t \\mid s_{t-1}, a_{t-1}) P(a_t \\mid s_t) \\quad \\text{(Assume Markov)} \\\\ &amp;= \\rho(s_1) \\prod_{t=1}^T \\mathcal{T}(s_t \\mid s_{t-1}, a_{t-1})\\pi(a_t \\mid s_t) \\end{align*}\\] How to calculate gradient of \\(L(\\theta)\\)?: . \\[\\begin{align*} \\nabla_\\theta L(\\theta) &amp;= \\nabla_\\theta D_{KL}(P_D || P_\\theta) \\\\ &amp;= \\nabla_\\theta (\\sum_\\tau P_D(s_1, a_1, ..., s_T) \\log \\frac{P_D(s_1,a_1,...,s_T)}{P_\\theta(s_1,a_1,...,s_T)}) \\\\ &amp;= \\nabla_\\theta (\\sum_\\tau P_D(s_1, a_1, ..., s_T) (\\log P_D(s_1, a_1, ..., s_T) - \\log P_\\theta(s_1, a_1, ..., s_T))) \\\\ &amp;= -\\mathbb{E}_{\\tau \\sim D} \\left[ \\nabla_\\theta \\log P_\\theta(s_1,a_1,...,s_T) \\right] \\\\ &amp;= -\\mathbb{E}_{\\tau \\sim D} \\left[ \\sum_{t=1}^T \\nabla_\\theta \\log \\pi_\\theta (a_t \\mid s_t) \\right] \\\\ &amp;= -\\mathbb{E}_{s,a \\sim D} \\left[ \\nabla_\\theta \\log \\pi_\\theta(a \\mid s) \\right] \\end{align*}\\] which makes sense to what we’re doing. Thus, minimzing \\(L(\\theta)\\) is Maximum Likelihood Estimation (MLE) of the policy \\(\\pi_\\theta\\): . \\[\\begin{equation} \\theta^* = \\arg \\max_\\theta \\mathbb{E}_{s,a \\sim \\mathcal{D}} \\left[ \\log \\pi_\\theta (\\hat a = a \\mid s) \\right] \\end{equation}\\] One big difference between this and other supervised learning algorithms is the “drift” issue. If \\(\\pi_\\theta\\) makes a slightly incorrect step away from \\(\\pi_d\\) it’ll be slightly different which may or may not be in distribution, and this kind of error will just compound itself as time goes on. The timeseries aspect of this supervised learning perspective gives rise to the co-variate shift . One way to address co-variate shift is DAgger (Dataset Aggregation) . This performs the training process iteratively where DAgger will take data collected in eval time and re-run with the expert policy to get more data. This will reduce the covariate-shift as we are seeing the small errors and addressing them in the form of new data from the expert policy. There are still other issues with BC: . | Tedious Experts: DAgger requires access to expert to create new data. | Multimodality: Distribution of expert actions might have multiple “peaks”. We have to address which peak to choose for our policy. | Causal Confusion: Agent might learn “spurious” features. We might learn weird features and correlate them to a certain action instead of a feature that we want it to correlate to a certain action. For example, a car might brake because of the brake indicator in the car instead of visually seeing pedestrians on the road. That’s because brake indicator goes on every time we brake really hard. | Copy cat problem: BC might predict next action by previous actions instead of reacting to its sensing modalities. | Suboptimal Demonstrations: Our demonstrations may not be from a task expert which may train a policy to be sub-optimal at best. | . ",
    "url": "/blog/class-notes/04-Computational-Sensorimotor-Learning/learning-from-demo.html#51-behavior-cloning",
    
    "relUrl": "/class-notes/04-Computational-Sensorimotor-Learning/learning-from-demo.html#51-behavior-cloning"
  },"268": {
    "doc": "GDL 3: Manifolds",
    "title": "GDL 3: Manifolds",
    "content": " ",
    "url": "/blog/prereq-notes/manifolds.html",
    
    "relUrl": "/prereq-notes/manifolds.html"
  },"269": {
    "doc": "GDL 3: Manifolds",
    "title": "Topological Manifolds",
    "content": "Definition 1.1 (Topological Manifold) A topological space \\(M\\) is called a d-dimensional (topological) manifold if around every point \\(p \\in M\\) there exists a neighborhood \\(U \\subset M\\) and a homeomorphism . \\[x: U \\to x(U) := V \\subseteq \\mathbb{R}^d\\] The tuple \\((U,x)\\) is called a chart of \\(M\\) and the components of $x_i$$ are called coordinates. Atlas of a manifold \\(M\\) is a set of charts \\(\\mathcal{A} = \\{(U^l, x^l)\\}_{l \\in L}\\) such that \\(\\bigcup_{l \\in L} U^l = M\\). The homeomorphisms \\(x^B \\circ (x^A)^{-1}\\) are called transition maps. ",
    "url": "/blog/prereq-notes/manifolds.html#topological-manifolds",
    
    "relUrl": "/prereq-notes/manifolds.html#topological-manifolds"
  },"270": {
    "doc": "GDL 3: Manifolds",
    "title": "Smooth Manifolds",
    "content": "We want to differentiate functions \\(f: M \\to \\mathbb{R}\\) on the manifold. Define differentiation on \\(M\\) by pulling back functions via chart \\(x^A\\) to \\(\\mathbb{R}^d\\). \\[\\begin{equation} f \\circ (x^A)^{-1}: V^A \\to \\mathbb{R} \\end{equation}\\] ensure consistency among charts (that is charts need to be smoothly compatible) i.e. \\(x^B \\circ (x^A)^{-1}\\) is smooth. We obtain a smooth manifold by adding an atlas with smooth transition maps. Definition 1.2 (Smooth Atlas) An atlas \\(\\mathcal{A}\\) is called smooth (differentiable) if the transition maps between any two overlapping charts are smooth (differentiable). Definition 1.3 (Smooth Manifold) A smooth manifold is a topological manifold equipped with a smooth atlas. Definition 1.4 (Smooth map) Let \\(\\phi: M \\to N\\) be a map between smooth map at \\(p \\in M\\) if for some smooth charts \\((U_M, x_M)\\) and \\((U_N,x_N)\\) around \\(p\\) and \\(\\phi(p)\\), the map \\(x_N \\circ \\phi \\circ x_M^{-1}\\) is smooth in the usual sense on \\(\\mathbb{R}^d\\) . Definition 1.5 (Smooth curve) A smooth curve on \\(M\\) is a smooth map \\(\\gamma: \\mathbb{R} \\to M\\). Definition 1.6 (Tangent space (informal)) The tangent space \\(T_pM\\) at \\(p \\in M\\) is the vector space spanned by the tangent vectors \\(\\gamma'(0)\\) of smooth curves \\(\\gamma: \\mathbb{R} \\to M\\) with \\(\\gamma(0) = p\\). \\[\\begin{equation} T_pM := \\{ \\gamma'(0) \\mid \\gamma: \\mathbb{R} \\to M, \\quad \\gamma(0) = p, \\quad \\gamma \\in \\mathcal{C}^d, d \\geq 1 \\} \\end{equation}\\] . Disjoint union of tangent spaces forms the tangent bundle \\(TM\\) of \\(M\\). Despite tangent spaces being vector spaces, they do not have a canonical reference frame (basis), so we need to pick some frame to represent tangent vectors numerically - different choices possible! . The set of all reference frames form the frame bundle \\(FM\\) of \\(M\\). A chart induces a frame field . Definition 1.7 (Diffeomorphism) Let \\(\\phi: M \\to N\\) be a bijective map between smooth manifolds. if both \\(\\phi\\) and \\(\\phi^{-1}\\) are smooth, then \\(\\phi\\) is said to be a diffeomorphism. \\(M \\cong N\\) if they are diffeomorphic. \\[\\text{Diff}(M) := \\{ \\phi: M \\to M \\mid \\phi \\text{ is diffeomorphic} \\}\\] . \\(\\text{Diff}(M)\\) are the symmetries of a smooth manifold M - if we want isometries, we need to add a metric structure. ",
    "url": "/blog/prereq-notes/manifolds.html#smooth-manifolds",
    
    "relUrl": "/prereq-notes/manifolds.html#smooth-manifolds"
  },"271": {
    "doc": "GDL 3: Manifolds",
    "title": "Riemmanian Manifolds",
    "content": "Riemmanian metric tensor fields endow smooth manifolds with notion of distance. The metric tensor field \\(\\eta\\) is a smooth assignment of positive definite inner products . \\[\\begin{equation} \\eta_p: T_pM \\times T_pM \\to \\mathbb{R} \\text{ s.t. } \\eta_p(v,v) \\geq 0 \\end{equation}\\] norms of tangent vectors are defined as usual: \\(\\|v\\| := \\sqrt{\\eta_p(v,v)} \\space \\forall v \\in T_pM\\) . lengths of smooth curves \\(\\gamma: [a,b] \\to M\\) are defined by integrating their tangent vectors (“velocities”) along \\(\\gamma\\): . \\[\\begin{equation} l_\\gamma = \\int_a^b \\| \\frac{d}{dt} \\gamma(t) \\| dt \\end{equation}\\] . the distance \\(d_\\eta(p,q)\\) between two points \\(p\\) and \\(q\\) is the length of the shortest curve between them (geodesic): . \\[\\begin{align*} d_\\eta: M \\times M \\to \\mathbb{R}_{\\geq 0} \\\\ d_\\eta(p,q) := \\inf \\{ l_\\gamma \\mid \\gamma \\text{ is a piecewise smooth curve from p to q} \\} \\end{align*}\\] . isometries are distance preserving maps between Riemmanian manifolds: . Definition 1.8 (Isometry) An isometry \\(\\phi: M \\to N\\) for between Riemmanian manifolds \\((M,\\eta_M)\\) and \\((N,\\eta_N)\\) is a diffeomorphism such that \\(\\eta_N(\\phi_*v, \\phi_*w) = \\eta_M(v,w)\\) \\(\\forall v,w \\in T_pM\\) , where \\(\\phi_*\\) is the pushforward of the metric tensor field. the symmetries of a Riemmanian manifold form its isometry group . \\[\\text{Isom}(M) := \\{ \\phi: M \\to N \\mid \\phi \\text{ is an isometry} \\}\\] all metric derived properties (geodesics, curvature, Levi-Civita connection, …) are preserved by isometries. The metric induces a (unique) Levi-Civita connection = “infinitisimal transport” . finite parallel transporters along paths \\(\\gamma\\) follow by integration. parallel transport is in general path dependent . transport between mesh faces “unfold, shift, fold back” - this is the “holonomy” of the connection. the intrinsic (Riemann) curvature tensor measures the amount of rotation of a vector when transporting it in infiniesimal circle. contracted (“index-averaged”) versions: Ricci/scalar/Gaussian curvature . for embedded manifolds , we also have several measures for extrinsic . curvature at a vertex of a 2d-mesh, corresponds to its angle defect . the Riemmanian exponential maps \\(\\exp_p: T_pM \\to M\\) send vectors \\(v \\in T_pM\\) to points \\(\\exp_p(v) \\in M\\) by walking a distance \\(\\|v\\|\\) along the geodesic in direction of \\(v\\). useful for Riemannian gradient descent . in general only locally injective the inverse is locally given by the Riemmanian logarithmic map : . \\[\\begin{equation} \\log_p = \\exp^{-1}_p: M \\supseteq U_{p,\\text{injective}} \\to T_pM \\end{equation}\\] . ",
    "url": "/blog/prereq-notes/manifolds.html#riemmanian-manifolds",
    
    "relUrl": "/prereq-notes/manifolds.html#riemmanian-manifolds"
  },"272": {
    "doc": "GDL 3: Manifolds",
    "title": "Fiber Bundles",
    "content": "Globally trivial bundles are spaces that can be written as a product: . \\[E = M \\times F\\] M is manifold (base space) and F is the fiber. globally trivial bundles are in particular locally trivial over a neighborhood \\(U \\subseteq M\\) . general fiber bundles are locally trivializable, but not necessarily global products. Fiber bundles \\((E, M, \\pi, F)\\) are structures consisting of… . | E (total space) | M (base space) | F (fiber) | a continuous surjective bundle projection map \\(\\pi: E \\to M\\) | . such that aroundd every \\(p \\in M\\) there exists a trivializing neighborhood \\(U \\subseteq M\\) and a homeomorphism (local trivialization) \\(\\psi: \\pi^{-1}(U) \\to U \\times F\\) satisfying \\(\\text{proj}_1 \\circ \\psi = \\pi\\) . the global topological structure is recovered from an atlas of local trivializations. \\(\\mathcal{A} = \\{ (U^I, \\psi^I) \\}_{I \\in \\mathcal{J}}\\) . by gluing the trivializations together with transition maps . \\[\\begin{equation} \\text{id} \\times g^{BA} := \\psi^{B} \\circ (\\psi^A)^{-1} : (U^A \\cap U^B) \\times F \\to (U^A \\cap U^B) \\times F \\end{equation}\\] . The tangent spaces of a smooth manifold form the tangent bundle \\((TM, M, \\pi_{TM}, \\mathbb{R}^d)\\) . with total space \\(TM := \\bigcup_{p \\in M} T_pM\\) and projection map \\(\\pi_{TM}: TM \\to M\\) given by \\(\\pi_{TM}(v) = p\\) for \\(v \\in T_pM\\). local trivializations \\(\\psi^A\\) identify tangent spaces over \\(U^A\\) with \\(\\mathbb{R}^d\\) \\(\\implies\\) inducing reference frames on \\(T_pM\\) transition maps \\(\\psi^B \\circ (\\psi^A)^{-1}\\) glue the fibers \\(\\mathbb{R}^d\\) together over \\(U^A \\cap U^B\\) via structure group elements. additional structure on M allows to restrict to a G-atlas of trivializations with a reduced structure group G. the frame bundle \\((FM, M, \\pi_{FM}, GL(d))\\) is the bundle consisting of all reference frames of all tangent spaces . its fiber is the general linear group i.e. \\(F_pM \\cong_{\\text{top}} GL(d)\\) . G-structures correspond to sub-bundles of frames with fiber \\(G \\leq GL(d)\\) . they are in 1-to-1 relation with G-atlases of TM. ",
    "url": "/blog/prereq-notes/manifolds.html#fiber-bundles",
    
    "relUrl": "/prereq-notes/manifolds.html#fiber-bundles"
  },"273": {
    "doc": "Math Notes",
    "title": "Quick RL Math Notes",
    "content": " ",
    "url": "/blog/project-notes/project1/math.html#quick-rl-math-notes",
    
    "relUrl": "/project-notes/project1/math.html#quick-rl-math-notes"
  },"274": {
    "doc": "Math Notes",
    "title": "Advantage Actor Critic",
    "content": "\\[\\begin{equation} R^\\gamma(\\tau_{t:T}) = Q^\\pi(s_t,a_t) = \\mathbb{E} [ \\sum_{i=t}^T \\gamma^{i-t} r(s_i, a_i) \\mid s_t, a_t ] \\end{equation}\\] . | \\(R\\) is returns function (\\(\\gamma\\) is discount factor but not exponent of \\(R\\)) | \\(Q\\) is the action-value function | \\(r\\) is the reward function. | . When calculating policy gradients, we get them from solving the following optimization problem: . \\[\\begin{equation} \\max_{d} J(\\theta + d) - J(\\theta) \\\\ \\text{s.t.} \\quad \\mathbb{E}_s \\left[ D_{KL}(\\pi_\\theta(a \\mid s) || \\pi_{\\theta + d}(a \\mid s)) \\right] \\leq \\epsilon \\end{equation}\\] . | \\(J(\\theta)\\) = \\(\\sum_{i=1}^N \\sum_{t=1}^T \\nabla_\\theta \\log \\pi(a \\mid s_t^t) A(s_t,a_t)\\) | \\(A(s_t,a_t)\\) = \\(Q(s_t,a_t) - V(s_t)\\) | \\(\\mathbf{F}(\\theta)\\) = \\(\\mathbb{E}_{\\pi_\\theta (a \\mid s)} \\left[ \\nabla_\\theta \\log \\pi_\\theta(a \\mid s) \\nabla_\\theta \\log \\pi_\\theta(a \\mid s)^T \\right]\\) this is the Fisher information matrix (second-order approx of KL-divergence). | . Using our knowledge of Negative Log-Likelihood, without the advantage function, we are trying to use our neural network to match the action distribution from our rollouts (maximizing log-likelihood of actions from rollouts). And our cost function is the gradient of the log-likelihood (doing gradient ascent). We are just maximizing the cost boost without stepping too far away from the action distribution (KL-divergence constraint). Since our actions are delta-poses, we can almost think of the KL-divergence constraint on a rollout trajectory to create a funnel of admissible trajectories (actions). Using the insertion reward (keypoints), it’s easy to see how when we sample from the network which actions should be preferred (actions taken closer to the goal). If we use this other reward (packed object arrangement), there is no direct correlation between an action taken and the reward improving. | This motivates the case for curriculum learning, we generate trajectories close to the goal so that we can find pushing actions from the funnel of admissible actions to push occluding objects out of the way. | . ",
    "url": "/blog/project-notes/project1/math.html#advantage-actor-critic",
    
    "relUrl": "/project-notes/project1/math.html#advantage-actor-critic"
  },"275": {
    "doc": "Math Notes",
    "title": "Math Notes",
    "content": " ",
    "url": "/blog/project-notes/project1/math.html",
    
    "relUrl": "/project-notes/project1/math.html"
  },"276": {
    "doc": "Neural Sheaf Diffusion",
    "title": "Neural Sheaf Diffusion for Graphs",
    "content": "This is based on Neural Sheaf Diffusion for Graphs. ",
    "url": "/blog/graph-notes/neural-sheaf-diffusion.html#neural-sheaf-diffusion-for-graphs",
    
    "relUrl": "/graph-notes/neural-sheaf-diffusion.html#neural-sheaf-diffusion-for-graphs"
  },"277": {
    "doc": "Neural Sheaf Diffusion",
    "title": "Fresh perspective on GNN issues",
    "content": "GNNs have long suffered from performance on heterophilic graphs and oversmoothing. In the Neural Sheaf Diffusion paper, they argue that the root of both of these problems lies in the “geometry” of the graph. Graphs and manifolds are topological objects (have a notion of neighborhood) but not distance or direction. We can define some notion of geometry by decorating each node and edge with a vector space structure (these are called stalks) and a collection of linear transforms (restriction maps) between stalks of every node and edge. In this image \\(\\mathcal{F}(u)\\), \\(\\mathcal{F}(v)\\), and \\(\\mathcal{F}(e=(u,v))\\) are stalks. The others are restriction maps to go back and forth between the stalks. The graph with stalks and restriction maps is called a cellular sheaf. Sheaves are main objects of study in algebraic topology and are roughly thought of as a general way to describe data defined on a topological space. ",
    "url": "/blog/graph-notes/neural-sheaf-diffusion.html#fresh-perspective-on-gnn-issues",
    
    "relUrl": "/graph-notes/neural-sheaf-diffusion.html#fresh-perspective-on-gnn-issues"
  },"278": {
    "doc": "Neural Sheaf Diffusion",
    "title": "Sheaves",
    "content": "Sheaves are closely related to that of a connection in differential geometry. A connection “connects” tangent spaces of two nearby points on a smooth manifold. This connection is needed to perform parallel transport of vectors along a curve. The connection is defined by a linear map between the tangent spaces of two points. Defining the connection endows a manifold with geometric strucutre, different connections give rise to different geometric structures. ",
    "url": "/blog/graph-notes/neural-sheaf-diffusion.html#sheaves",
    
    "relUrl": "/graph-notes/neural-sheaf-diffusion.html#sheaves"
  },"279": {
    "doc": "Neural Sheaf Diffusion",
    "title": "Sheaf and Graph Diffusion",
    "content": "Sheaf diffusion equation described below: \\(\\begin{equation} \\mathbf{\\dot x}(t) = - \\Delta \\mathbf{x}(t) \\end{equation}\\) . Sheaf Laplacian \\(\\Delta\\) is an \\(nd \\times nd\\) matrix, where \\(nd\\) is dimension of vector \\(x\\). It acts on d-dimensional stalk. As we run diffusion with the sheaf laplacian, as the limit \\(t \\to \\infty\\), the diffusion converges to a solution which belongs to harmonic space of sheaf laplacian. Any node classification task can be posed as finding the right sheaf on which the limit of sheaf diffusion is able to linearly separate node features. This can be seen as an alternative test to the well known WL-test. Choosing \\(d=1\\) and non-zero restriction maps (non-negative weights) corresponds to standard diffusion on a graph (GCN). These architectures can only separate 2 classes of nodes under homophily assumptions. With asymmetric restriction maps, we can have negative weights which was shown to work in heterophilic graphs. ",
    "url": "/blog/graph-notes/neural-sheaf-diffusion.html#sheaf-and-graph-diffusion",
    
    "relUrl": "/graph-notes/neural-sheaf-diffusion.html#sheaf-and-graph-diffusion"
  },"280": {
    "doc": "Neural Sheaf Diffusion",
    "title": "Sheaf Convolutional Networks and Energy Minimisation",
    "content": "We try to connect sheaf theory to deep learning on graphs. Consider the following GNN layer: . \\[\\begin{equation} \\mathbf{Y} = \\sigma((\\mathbf{I} - \\Delta)(\\mathbf{I} \\otimes \\mathbf{W}_1) \\mathbf{X} \\mathbf{W}_2) \\end{equation}\\] where \\(\\mathbf{X} \\in \\mathbb{R}^{nd \\times f}\\) is input features with f channels, \\(\\mathbf{W}_1, \\mathbf{W}_2 \\in \\mathbb{R}^{f \\times f}\\) are learnable weights. \\(\\Delta \\in \\mathbb{R}^{nd \\times nd}\\) is sheaf laplacian. \\(\\otimes\\) is the Kronecker product. \\(\\sigma\\) is a non-linear activation function (ReLU or LeakyReLU). This is called a Sheaf Convolutional Network (SCN). We examine the sheaf dirichlet energy \\(\\mathcal{E}(\\mathbf{X}) = \\text{trace}(\\mathbf{X}^T \\Delta \\mathbf{X})\\). This is a quadratic form analogous to classical Dirichlet energy that measures distance of \\(\\mathbf{X}\\) from harmonic space of sheaf Laplacian. Diffusion equations are gradient flows of the Dirichlet energy, minimising it over time. Application of one GCN layer necessarily decreases the energy \\(\\mathcal{E}(\\mathbf{Y}) \\leq \\mathcal{E}(\\mathbf{X})\\). As a result, features tend to become smoother with increased depth. However, in the sheaf diffusion case, if we choose a sheaf, it is possible to increase the energy. ",
    "url": "/blog/graph-notes/neural-sheaf-diffusion.html#sheaf-convolutional-networks-and-energy-minimisation",
    
    "relUrl": "/graph-notes/neural-sheaf-diffusion.html#sheaf-convolutional-networks-and-energy-minimisation"
  },"281": {
    "doc": "Neural Sheaf Diffusion",
    "title": "Neural Sheaf Diffusion",
    "content": " ",
    "url": "/blog/graph-notes/neural-sheaf-diffusion.html",
    
    "relUrl": "/graph-notes/neural-sheaf-diffusion.html"
  },"282": {
    "doc": "Neuro-Symbolic Workshop",
    "title": "Neuro-Symbolic Workshop (5/19/2025)",
    "content": " ",
    "url": "/blog/conference-notes/icra-2025/workshop/neuro-symbolic.html#neuro-symbolic-workshop-5192025",
    
    "relUrl": "/conference-notes/icra-2025/workshop/neuro-symbolic.html#neuro-symbolic-workshop-5192025"
  },"283": {
    "doc": "Neuro-Symbolic Workshop",
    "title": "Luca Carlone",
    "content": "todo: watch . ",
    "url": "/blog/conference-notes/icra-2025/workshop/neuro-symbolic.html#luca-carlone",
    
    "relUrl": "/conference-notes/icra-2025/workshop/neuro-symbolic.html#luca-carlone"
  },"284": {
    "doc": "Neuro-Symbolic Workshop",
    "title": "Danfei Xu",
    "content": "todo: watch . ",
    "url": "/blog/conference-notes/icra-2025/workshop/neuro-symbolic.html#danfei-xu",
    
    "relUrl": "/conference-notes/icra-2025/workshop/neuro-symbolic.html#danfei-xu"
  },"285": {
    "doc": "Neuro-Symbolic Workshop",
    "title": "Jiayuan Mao",
    "content": "todo: watch . ",
    "url": "/blog/conference-notes/icra-2025/workshop/neuro-symbolic.html#jiayuan-mao",
    
    "relUrl": "/conference-notes/icra-2025/workshop/neuro-symbolic.html#jiayuan-mao"
  },"286": {
    "doc": "Neuro-Symbolic Workshop",
    "title": "Dinesh",
    "content": "todo: watch . ",
    "url": "/blog/conference-notes/icra-2025/workshop/neuro-symbolic.html#dinesh",
    
    "relUrl": "/conference-notes/icra-2025/workshop/neuro-symbolic.html#dinesh"
  },"287": {
    "doc": "Neuro-Symbolic Workshop",
    "title": "anirudha",
    "content": "todo: watch . ",
    "url": "/blog/conference-notes/icra-2025/workshop/neuro-symbolic.html#anirudha",
    
    "relUrl": "/conference-notes/icra-2025/workshop/neuro-symbolic.html#anirudha"
  },"288": {
    "doc": "Neuro-Symbolic Workshop",
    "title": "Neuro-Symbolic Workshop",
    "content": " ",
    "url": "/blog/conference-notes/icra-2025/workshop/neuro-symbolic.html",
    
    "relUrl": "/conference-notes/icra-2025/workshop/neuro-symbolic.html"
  },"289": {
    "doc": "Paper",
    "title": "Paper",
    "content": "Paper link . Accepted to RSS 2024. This works with Isaac to create photorealistic data. This is important for us. Pros: . | It has photorealistic rendering. (key property) | Cons: | No lab support (:&lt;). | Doesn’t have obvious support for fixed-base robots. | . ",
    "url": "/blog/code-notes/benchmark-notes/robocasa/paper.html",
    
    "relUrl": "/code-notes/benchmark-notes/robocasa/paper.html"
  },"290": {
    "doc": "Paper",
    "title": "Libero (paper)",
    "content": "Paper link. This paper concerns itself with LLDM (Life-long Learning in Decision-Making), and is a benchmark with 4 task suites (130 total tasks) with high-quality human-teleoperated demonstration data. LIBERO stands for (LIfelong learning BEnchmark on RObot manipulation tasks) . ",
    "url": "/blog/code-notes/benchmark-notes/libero/paper.html#libero-paper",
    
    "relUrl": "/code-notes/benchmark-notes/libero/paper.html#libero-paper"
  },"291": {
    "doc": "Paper",
    "title": "Lifelong learning",
    "content": "Life-long learning is a paradigm where a model learns from a stream of tasks, and is able to transfer knowledge from previously learned tasks to new tasks. This is important for robotics, as robots often need to learn new skills in dynamic environments. These new skills are learned along the agent’s lifetime hence the name “life-long learning”. This was submitted in 2023 and at that time, it is understudied how agents transfer learn from one task to another. One interesting scenario listed in the paper is where an agent initially learned to retrieve a juice from the fridge is trying to learn new tasks. After learning the new tasks, it could fail to do its original task due to forgetting the details of the original task like what the juice looked like or where the fridge is (declarative knowledge) or how to grab the juice (procedural knowledge). ",
    "url": "/blog/code-notes/benchmark-notes/libero/paper.html#lifelong-learning",
    
    "relUrl": "/code-notes/benchmark-notes/libero/paper.html#lifelong-learning"
  },"292": {
    "doc": "Paper",
    "title": "Observations with LIBERO",
    "content": "These are observations that LIBERO has made (Don’t fully trust): . | Policy architecture design is as crucial as lifelong learning algorithms. The transformer architecture is better at abstracting temporal information than a recurrent neural network. Vision transformers work well on tasks with rich visual information (e.g., a variety of objects). Convolution networks work well when tasks primarily need procedural knowledge. | While the lifelong learning algorithms we evaluated are effective at preventing forgetting, they generally perform worse than sequential finetuning in terms of forward transfer. | Our experiment shows that using pretrained language embeddings of semantically-rich task descriptions yields performance no better than using those of the task IDs. | Basic supervised pretraining on a large-scale offline dataset can have a negative impact on the learner’s downstream performance in LLDM. | . ",
    "url": "/blog/code-notes/benchmark-notes/libero/paper.html#observations-with-libero",
    
    "relUrl": "/code-notes/benchmark-notes/libero/paper.html#observations-with-libero"
  },"293": {
    "doc": "Paper",
    "title": "Background",
    "content": "Formulate Markov Decision Process (MDP) \\(\\mathcal{M} = (\\mathcal{S}, \\mathcal{A}, \\mathcal{T}, H, \\mu_0, R)\\). | \\(\\mathcal{S}\\) state space. | \\(\\mathcal{A}\\) action space. | \\(\\mu_0\\) initial state distribution. | \\(R: \\mathcal{S} \\times \\mathcal{A} \\to \\mathbb{R}\\) reward function. | \\(\\mathcal{T}: \\mathcal{S} \\times \\mathcal{A} \\to \\mathcal{S}\\) transition function. | \\(H\\) horizon | In the sparse-reward setting, \\(R\\) is \\(g: \\mathcal{S} \\to \\{0,1\\}\\) and is the goal predicate | . Agent objective is to learn a policy \\(\\pi\\) that maximizes the expected return . \\[\\begin{equation} \\max_{\\pi} J(\\pi) = \\mathbb{E}_{s_t,a_t \\sim \\pi, \\mu_0}[\\sum_{t=1}^H g(s_t)] \\end{equation}\\] For life-long learning, we can think of learning of \\(K\\) tasks \\(\\{T^1, ..., T^K \\}\\) sequentially (learn 1 task, then the next, and so on). Up to the k-th task, the learning problem is formulated as follows: . \\[\\begin{equation} \\underset{\\pi}{\\max} J_{LRL}(\\pi) = \\frac{1}{k} \\sum_{p=1}^k [\\underset{s_t^p, a_t^p \\sim \\pi(\\cdot ; T^p, \\mu_0^p)}{\\mathbb{E}}[\\sum_{t=1}^L g^p(s_t^p)] ] \\end{equation}\\] Note that this problem wants to do the best over all of these tasks on average. Note that the neural network is conditioned on \\(T^p\\) only and not any previous task. This means we do not have access to the previous task dataset only the weights of the current policy. This is for policy learning in general! If we just want to formulate the BC problem, here it is: . \\[\\begin{equation} \\underset{\\pi}{\\max} J_{BC}(\\pi) = \\frac{1}{k} \\sum_{p=1}^k \\underset{o_t,a_t \\sim D^p}{\\mathbb{E}}[\\sum_{t=0}^{\\ell^p} \\mathcal{L}(\\pi(o_{\\leq t}; T^p), a^p_t ) ] \\end{equation}\\] In other words, we sample out of the dataset and define a loss function to match the actions of the expert policy. In LIBERO, \\(\\pi\\) is a Gaussian Mixture Model (GMM) and thus also \\(\\mathcal{L}\\) is the negative-log-likelihood (NLL) loss. REMARK: Something that I’m starting to learn based from reading this paper is that policy learning (even supervised) is generally not that well explored. There isn’t a wealth of processed content on it. Instead, I have to read more supervised policy learning papers to understand how to make this work. ",
    "url": "/blog/code-notes/benchmark-notes/libero/paper.html#background",
    
    "relUrl": "/code-notes/benchmark-notes/libero/paper.html#background"
  },"294": {
    "doc": "Paper",
    "title": "Benchmark Details",
    "content": "These are the 4 suites: . | libero-spatial | libero-object | libero-goal | libero-100 | . It doesn’t seem that this codebase uses depth data for its experiments. It only uses RGB and language data. Pros: . | lots of tasks | LIBERO-OBJECT is a good benchmark (can try and vary objects in the scene). Cons: | no depth data used in experiments. | doesn’t look that good. | . ",
    "url": "/blog/code-notes/benchmark-notes/libero/paper.html#benchmark-details",
    
    "relUrl": "/code-notes/benchmark-notes/libero/paper.html#benchmark-details"
  },"295": {
    "doc": "Paper",
    "title": "Paper",
    "content": " ",
    "url": "/blog/code-notes/benchmark-notes/libero/paper.html",
    
    "relUrl": "/code-notes/benchmark-notes/libero/paper.html"
  },"296": {
    "doc": "PointNet",
    "title": "PointNet",
    "content": "paper link . KEY: Processing point clouds require us to be invariant to permutations of the input points. This is because on the computer, we must store the points in some order, but the order should not matter. ",
    "url": "/blog/deep-dive-notes/debug-point-ml/pointnet.html",
    
    "relUrl": "/deep-dive-notes/debug-point-ml/pointnet.html"
  },"297": {
    "doc": "PointNet",
    "title": "Related Works",
    "content": "Point Cloud Features: Lots of works that have done some handcrafted learning for point clouds. Doing feature engineering with intrinsic/extrinsic features and local/global features. Volumetric Data: This transfers from our previous understanding of processing image data (2d CNNs), where we can do 3D CNNs. | This is constrained by resolution (voxel size). | Also compute cost (lots of memory to process empty space) | . Multi-view CNNs: Lots of times we use multi-view data to get 3d point clouds. Instead of getting the 3d representation out of them, we just choose to process these images with 2D CNNs. It’s non-trivial to extend this to stuff like shape completion. Meshes: We can use Spectral CNNs to process meshes. However, this is constrained on manifold meshes like objects and not non-isometric shapes like furniture. Deep Learning on Unordered Sets: People have done unordered set processing for NLP. However, this does not utilize the geometric information that comes from an unordered point set. PointNet should try to utilize this geometric information. ",
    "url": "/blog/deep-dive-notes/debug-point-ml/pointnet.html#related-works",
    
    "relUrl": "/deep-dive-notes/debug-point-ml/pointnet.html#related-works"
  },"298": {
    "doc": "PointNet",
    "title": "Architecture",
    "content": ". this is the pointnet architecture. The PointNet architecture aggregates information through max pooling all the point embeddings into one descriptor. Let our batched input be \\(\\mathbf{X} \\in \\mathbb{R}^{N \\times D}\\) where \\(B\\) is batch, \\(N\\) is point set size, and \\(D\\) is point dim. Our points are first passed through a “T-Net” which is a spatial transformer network. This learns a transformation matrix and applies it the input points. T-Net creates a transformation matrix \\(\\mathbb{R}^{D \\times D}\\) and is repeated \\(N\\) times for each point in the point set (it isn’t unique per point to preserve permutation invariance). \\[\\begin{align*} \\text{TNet} &amp;: \\mathbb{R}^{N \\times D} \\to \\mathbb{R}^{N \\times D \\times D} \\\\ \\mathbf{X}' &amp;= \\text{TNet}(\\mathbf{X})\\mathbf{X} \\end{align*}\\] Ideally \\(D=3\\) such that we learn a rotation matrix (something like that). Since this doesn’t use projective geometry, do not think it will apply a translation. We now apply MLP to $$\\mathbf{X}’$ to get a point embedding. \\[\\begin{align*} \\mathbf{X}'' &amp;= \\text{MLP}(D,64)(\\mathbf{X}') \\\\ \\mathbf{X}''' &amp;= \\text{MLP}(64,64)(\\mathbf{X}'') \\\\ \\end{align*}\\] Now we do a second transform but it’s called a feature transform (64 dims). This will most likely do the translation reasoning in the PointNet. \\[\\begin{align*} \\mathbf{Y} &amp;= \\text{TNet2}(\\mathbf{X}''')\\mathbf{X}''' \\end{align*}\\] The rest is just more MLP layers and finally a max pooling layer to get the global descriptor. \\[\\begin{align*} \\mathbf{Y}' &amp;= \\text{MLP}(64,512)(\\mathbf{Y}) \\\\ \\mathbf{Z} &amp;= \\text{MAXPOOL}(\\mathbf{Y}') \\\\ \\mathbf{Z} &amp;\\in \\mathbb{R}^{512} \\end{align*}\\] Local/Global Info Aggregation: When we do max-pooling, we get global point cloud features. However, segmentation needs local features also. Therefore, we use intermediate values after the feature transform like \\(\\mathbf{Y}\\) to get local features. Why Input/Feature Transform?: For processing points, it’s not enough to use shared MLP implementation. We must use the input transform/feature transform idea to transform the points to a “canonical” form. | If we don’t put the object into the object frame, we can instead learn this canonical alignment through the T-Nets. This is the point of the input transform. | The feature transforms help align different point clouds, so we do this alignment in feature space. Since the feature space transform is high-dimensional, it is hard to optimize, so we add a regularization term to constrain feature matrix to be close to orthogonal matrix | . \\(\\begin{equation*} \\mathcal{L}_{\\text{reg}} = \\| \\mathbf{I} - \\mathbf{A}\\mathbf{A}^T \\|_F^2 \\end{equation*}\\) which is the frobenius norm (treat matrix as a vector and take the norm of that vector). ",
    "url": "/blog/deep-dive-notes/debug-point-ml/pointnet.html#architecture",
    
    "relUrl": "/deep-dive-notes/debug-point-ml/pointnet.html#architecture"
  },"299": {
    "doc": "PointNet",
    "title": "Training Details",
    "content": " ",
    "url": "/blog/deep-dive-notes/debug-point-ml/pointnet.html#training-details",
    
    "relUrl": "/deep-dive-notes/debug-point-ml/pointnet.html#training-details"
  },"300": {
    "doc": "PointNet",
    "title": "Architecture Analysis",
    "content": " ",
    "url": "/blog/deep-dive-notes/debug-point-ml/pointnet.html#architecture-analysis",
    
    "relUrl": "/deep-dive-notes/debug-point-ml/pointnet.html#architecture-analysis"
  },"301": {
    "doc": "3. Policy Gradients",
    "title": "3. Policy Gradients",
    "content": "Policy gradients are used to solve SDM problems. We take the gradient of the sum of rewards with respect to policy parameters. Using gradient ascent, we can find the optimal policy. ",
    "url": "/blog/class-notes/04-Computational-Sensorimotor-Learning/policy-grad.html",
    
    "relUrl": "/class-notes/04-Computational-Sensorimotor-Learning/policy-grad.html"
  },"302": {
    "doc": "3. Policy Gradients",
    "title": "3.1 Derivation and Properties of Policy Gradient",
    "content": "\\[\\begin{equation} \\max_{\\theta} J(\\theta) = \\mathbb{E}_{\\pi_{\\theta}} \\left[ R^{\\gamma}(\\tau) \\right] \\end{equation}\\] where \\(R^{\\gamma}(\\tau)\\) denotes the discounted return of a trajectory \\(\\tau\\) and \\(\\pi_{\\theta}\\) denotes policy parametrized by \\(\\theta\\). Let \\(p_{\\theta}(\\tau)\\) be probability of trajectory \\(\\tau\\) being sampled. We express gradient of expected return \\(\\nabla_{\\theta} J(\\theta)\\) as: . \\[\\begin{align*} \\nabla_{\\theta} J(\\theta) &amp;= \\nabla_{\\theta} \\mathbb{E}_{\\pi_{\\theta}} \\left[ R^{\\gamma}(\\tau) \\right] \\\\ &amp;= \\nabla_{\\theta} \\int_{\\tau} p_{\\theta}(\\tau) R^{\\gamma}(\\tau) d\\tau \\\\ &amp;= \\int_{\\tau} \\nabla_{\\theta} p_{\\theta}(\\tau) R^{\\gamma}(\\tau) d\\tau \\\\ &amp;= \\int_{\\tau} p_{\\theta}(\\tau) \\frac{\\nabla_{\\theta} p_{\\theta}(\\tau)}{p_{\\theta}(\\tau)} R^{\\gamma}(\\tau) d\\tau \\\\ &amp;= \\int_{\\tau} p_{\\theta}(\\tau) \\nabla_{\\theta} \\log p_{\\theta}(\\tau) R^{\\gamma}(\\tau) d\\tau \\\\ &amp;= \\mathbb{E}_{\\pi_{\\theta}} \\left[ \\nabla_{\\theta} \\log (p_\\theta (\\tau)) R^\\gamma (\\tau) \\right] \\end{align*}\\] using the log-derivative trick, we can move the gradient inside the expectation. Now the calculation of the gradient is reduced to calculating the gradient of the log-likelihood of the trajectory. Gradient ascent can be calculated as follows \\(\\theta \\gets \\theta + \\alpha \\nabla_{\\theta} J(\\theta)\\). Remark 3.1 (Continuous action spaces). Policy gradients make no assumption about action spaces. We can use any choice of probabilistic distribution for \\(\\pi\\) and sample \\(a_t\\) from \\(\\pi\\). Remark 3.2 (Model-free). We can rewrite \\(\\nabla_{\\theta} J(\\theta)\\) as: . \\[\\begin{equation} \\mathbb{E}_{\\pi_{\\theta}} \\left[ \\nabla_{\\theta} \\log (p_\\theta (\\tau)) R^\\gamma (\\tau) \\right] = \\mathbb{E}_{\\pi_{\\theta}} \\left[ \\nabla_{\\theta} \\sum_{t=1}^T \\log (\\pi_\\theta (a_t \\mid s_{1:t}, a_{1:t}))R^\\gamma(\\tau_{1:T}) \\right] \\end{equation}\\] We can see that this is independent of the state transition model \\(P\\). We say that this is model-free. This means that we estimate policy gradients by sampling trajectories from an environment. Remark 3.3 (No Markov Assumption). Markov assumption is described as follows: . \\[\\begin{equation} \\pi(a_t \\mid s_{1:t}, a_{1:t}) = \\pi(a_t \\mid s_t) \\end{equation}\\] a lot of algorithms assume that the policy is Markovian. However, this is not necessary in this case. We can make our policy gradient algorithm work with Markovian policies if we want to. ",
    "url": "/blog/class-notes/04-Computational-Sensorimotor-Learning/policy-grad.html#31-derivation-and-properties-of-policy-gradient",
    
    "relUrl": "/class-notes/04-Computational-Sensorimotor-Learning/policy-grad.html#31-derivation-and-properties-of-policy-gradient"
  },"303": {
    "doc": "3. Policy Gradients",
    "title": "3.2 REINFORCE",
    "content": "The above policy gradient derivation is also known as REINFORCE. However, we just described the update step. These are the exact steps of the algorithm: . | Roll out trajectory \\(\\tau^i\\) using policy \\(\\pi_{\\theta}\\). | Compute gradient of log-likelihood of all actions \\(g^i = \\nabla_\\theta \\log \\pi_\\theta (\\cdot \\mid \\cdot)\\). | Weigh gradient by corresponding returns \\(g^i R^\\gamma (\\tau^i)\\). | Update policy parameters \\(\\theta \\gets \\theta + \\alpha g^i R^\\gamma (\\tau^i)\\). | . We can rollout multiple trajectories and average the gradients. This is called the Monte Carlo estimate of the gradient. If we assume markov property, we can rewrite the steps as: . | Collect \\(N\\) trajectories \\(\\{ \\tau_{1:T}^i \\}_{i=1}^N\\) with policy \\(\\pi_{\\theta}\\). | Compute gradient \\(\\nabla_\\theta J(\\theta) = \\frac{1}{N} \\sum_{i=1}^N \\sum_{t=1}^T \\nabla_\\theta \\log \\pi (a \\mid s^i_t) R^\\gamma (\\tau^i_{1:t})\\). | update policy parameters \\(\\theta \\gets \\theta + \\alpha \\nabla_\\theta J(\\theta)\\). | . ",
    "url": "/blog/class-notes/04-Computational-Sensorimotor-Learning/policy-grad.html#32-reinforce",
    
    "relUrl": "/class-notes/04-Computational-Sensorimotor-Learning/policy-grad.html#32-reinforce"
  },"304": {
    "doc": "3. Policy Gradients",
    "title": "3.3 Credit Assignment",
    "content": "The idea of weighing the gradient of the policy by the return is called credit assignment. Unfortunately, credit assignment is not easy because of the high variance in sampled trajectory returns \\(R^\\gamma (\\tau)\\). The reason why this is the case is that it is challenging to distinguish contribution of eacha ction to the observed return. This is one of the issues that come with model-free approaches. Normally in model-based approaches, the gradient of the transition model can allow us to figure out the contribution of each action to the observed return. The issue still stands and it only gets worse as trajectory length increases. Discount We can use discount factor to reduce the variance of the return. A lower discount factor anneals the variance of the return since the total magnitude of the return is reduced. However, too small of a discount can lead to suboptimal policies. Baseline We can introduce a baseline to increase probability of trajectories with above-average returns and decrease that with below-average returns (baseline is average return). We increase probabilities of trajectories with “advantage”. Advantage quantifies gap between return of trajectory \\(R^\\gamma (\\tau)\\) to the average trajectory return \\(b(s_t) = \\mathbb{E}_{\\pi_{\\theta}} \\left[ R^\\gamma (\\tau) \\right]\\). With notion of advantage, we re-write policy gradient as: . \\[\\begin{equation} \\sum_{i=1}^N \\sum_{t=1}^T \\nabla_\\theta \\log \\pi (a \\mid s_t^i) (R^\\gamma (\\tau^i_{1:t}) - b(s_t^i)) \\end{equation}\\] where \\(R^\\gamma (\\tau^i_{1:t}) - b(s_t^i)\\) is the advantage of the action \\(a_t\\). A fundamental reason that baselines reduce variance is because of the following inequality: . \\[\\begin{equation} \\text{Var} \\left[ \\nabla_\\theta \\log(p_\\theta(\\tau))(R^\\gamma(\\tau) - b(\\tau)) \\right] \\leq \\text{Var} \\left[ \\nabla_\\theta \\log(p_\\theta(\\tau))R^\\gamma(\\tau) \\right] \\end{equation}\\] which only holds when \\(R^\\gamma(\\tau)\\) and \\(b(\\tau)\\) are correlated. If we assume both of these are random variables, then the variance of the difference of two random variables is described as: . \\[\\begin{equation} \\text{Var} \\left[ X - Y \\right] = \\text{Var} \\left[ X \\right] + \\text{Var} \\left[ Y \\right] - 2\\text{Cov} \\left[ X, Y \\right] \\end{equation}\\] if \\(2\\text{Cov} \\left[ X, Y \\right] &gt; \\text{Var} \\left[ Y \\right]\\) (i.e. X and Y correlate), then \\(\\text{Var} \\left[ X - Y \\right] &lt; \\text{Var} \\left[ X \\right]\\). This is because the term subtracts from the variance of \\(X\\) more than the contribution of the variance of \\(Y\\). Here are some common choices of baselines: . | Weighted average: \\(b(s_t) = \\frac{\\mathbb{E} \\left[ \\| \\nabla_\\theta \\log \\pi_\\theta(a \\mid s_t) \\|_2^2 R(\\tau_{1:T}) \\right]}{ \\| \\mathbb{E} \\left[ \\nabla_\\theta \\log \\pi_\\theta (a \\mid s_t) \\|_2^2 \\right] }\\) | Value function: \\(V^\\pi (s_t) = \\mathbb{E}_{\\pi_\\theta} \\left[ R^\\gamma (\\tau_{1:T}) \\right]\\) | . Calculating the value function is a bit tricky. We can parametrize the value function with another model (separate from policy model) \\(V^\\pi_\\psi\\). We then minimize the expected magnitude of advantage with respect to \\(\\psi\\): . \\[\\begin{equation} \\min_\\psi \\mathbb{E} \\left[ \\| R^\\gamma(\\tau_{t:T}) - V^\\pi_\\psi (s_t) \\|_2^2 \\right] \\end{equation}\\] where \\(R^\\gamma(\\tau_{t:T})\\) is the monte carlo estimation of expected return of a trajectory starting from \\(s_t\\). Sample size Another way to reduce variance is to increase number of trajectories sampled \\(N\\). However, we know that this is not always possible and can be too expensive, especially considering that we are sampling \\(N\\) trajectories for each gradient update. ",
    "url": "/blog/class-notes/04-Computational-Sensorimotor-Learning/policy-grad.html#33-credit-assignment",
    
    "relUrl": "/class-notes/04-Computational-Sensorimotor-Learning/policy-grad.html#33-credit-assignment"
  },"305": {
    "doc": "3. Policy Gradients",
    "title": "3.4 Actor-Critic",
    "content": "Actor-critic method uses the value function to replace MC estimates of trajectory returns \\(R^\\gamma(\\tau)\\) and reduce variance of the policy gradient. This section goes over the Advantage Actor-Critic method and the Generalized Advantage Estimation (GAE) method which balances bias-variance trade-off between MC estimation and value predictions. ",
    "url": "/blog/class-notes/04-Computational-Sensorimotor-Learning/policy-grad.html#34-actor-critic",
    
    "relUrl": "/class-notes/04-Computational-Sensorimotor-Learning/policy-grad.html#34-actor-critic"
  },"306": {
    "doc": "3. Policy Gradients",
    "title": "3.4.1 Advantage Actor-Critic (A2C)",
    "content": "A2C smooths estimates of traj return by averaging which reduces variance of policy gradient. The reason to smooth is because it is unlikely to obtain samples of trajectory returns from exactly the same state. As a result, we won’t get sufficient samples of trajectory returns starting from the same state. Taking average trajectory returns from similar states will reduce the variance of estimating the trajectory return starting from state \\(s\\). To smooth estimates of \\(R^\\gamma(\\tau)\\), A2C replaces the MC estimates of trajectory returns \\(R^\\gamma(\\tau)\\) starting from state \\(s_t\\) with: . \\[\\begin{equation} R^\\gamma(\\tau_{t:T}) = Q^\\pi(s_t,a_t) = \\mathbb{E} [ \\sum_{i=t}^T \\gamma^{i-t} r(s_i, a_i) \\mid s_t, a_t ] \\end{equation}\\] where \\(Q\\) is the expected return of taking action \\(a_t\\) in state \\(s_t\\). \\(\\mathbb{E}[\\cdot \\mid s_t, a_t]\\) means \\(s_t\\) and \\(a_t\\) are fixed. \\(Q\\) can be obtained from the value function baseline \\(V^\\pi\\) by constructing \\(Q^\\pi\\) in a recursive way: . \\[\\begin{align*} Q^\\pi(s_t,a_t) &amp;= \\mathbb{E} \\left[ \\sum_{i=t}^T \\gamma^{i-t} r(s_i,a_i) \\mid s_t, a_t \\right] \\\\ &amp;= \\mathbb{E} \\left[ r(s_t, a_t) + \\gamma V^\\pi(s_{t+1}) \\right] \\\\ &amp;\\approx \\mathbb{E} \\left[ r(s_t,a_t) + \\gamma V^\\pi_\\psi (s_{t+1}) \\right] \\end{align*}\\] this is assuming \\(V^\\pi \\approx V^\\pi_\\psi\\). Now we can write objective of policy gradient: . \\[\\begin{align*} &amp;\\sum_{i=1}^N \\sum_{t=1}^T \\nabla_\\theta \\log \\pi(a \\mid s_t^t) (R^\\gamma (\\tau_{t:T}) - V^\\pi_\\psi(s_t)) \\\\ &amp;= \\sum_{i=1}^N \\sum_{t=1}^T \\nabla_\\theta \\log \\pi(a \\mid s_t^t) (r(s_t,a_t) + \\gamma V^\\pi_\\psi (s_{t+1}) - V^\\pi_\\psi (s_t)) \\\\ &amp;= \\sum_{i=1}^N \\sum_{t=1}^T \\nabla_\\theta \\log \\pi(a \\mid s_t^t) A(s_t,a_t) \\end{align*}\\] where \\(A(s_t,a_t) = Q^\\pi(s_t,a_t) - V^\\pi_\\psi(s_t)\\) is the advantage. Reminder all of this is for calculating the policy gradient if you haven’t been following along. Using this calculation in our algorithm is called Advantage Actor-Critic (A2C). The MC estimates of \\(R^\\gamma(\\tau_{t:T})\\) has high randomness, while the randomness of \\(r(s_t,a_t) + \\gamma V^\\pi_\\psi(s_{t+1})\\) only comes from \\(r(s_t,a_t)\\) since the value function is deterministic. This eliminates variance but introduces bias since \\(V^\\pi_\\psi\\) is not the true value function and has underlying error. Formally, we can write out the bias-variance trade-off. Consider an unbiased estimate \\(h\\): . \\[\\begin{equation*} \\text{Bias}(h) = \\mathbb{E} [ X - h(X) ] = \\mathbb{E} [ X ] - \\mathbb{E} [ h(X) ] = 0 \\end{equation*}\\] where \\(X\\) is the random variable and \\(h(X)\\) is the estimate to \\(X\\). In this case, \\(X\\) is \\(R^\\gamma(\\tau_{t:T})\\) and \\(h(X)\\) is estimate of \\(R^\\gamma(\\tau_{t:T})\\). Let’s check out if MC estimation is unbiased estimate: . \\[\\begin{equation*} \\mathbb{E} \\left[ \\frac{1}{N} \\sum_{i=1}^N R^\\gamma (\\tau_{t:T}^i) \\right] = \\frac{1}{N} \\sum_{i=1}^N \\mathbb{E} [ R^\\gamma (\\tau_{t:T}^i)] = \\frac{1}{N} \\cdot N \\mathbb{E} [ R^\\gamma(\\tau^i_{t:T})] = \\mathbb{E} [ R^\\gamma(\\tau_{t:T})] \\end{equation*}\\] The MC estimation method above is unbiased. Now let’s check out our A2C method: . \\[\\require{cancel} \\begin{equation*} \\mathbb{E} \\left[ \\frac{1}{N} \\sum_{i=1}^N (r(s_t^i) + \\gamma V^\\pi_\\psi) (s^i_{t+1}) \\right] = \\frac{1}{N} \\sum_{i=1}^N \\mathbb{E} [ r(s_t^i, a_t^i)] + \\gamma \\frac{1}{N} \\mathbb{E} [ V^\\pi_\\psi (s^i_{t+1})] \\cancel{=} \\mathbb{E} [ R^\\gamma (\\tau_{t:T})] \\end{equation*}\\] as we can see, there is no way A2C is unbiased. ",
    "url": "/blog/class-notes/04-Computational-Sensorimotor-Learning/policy-grad.html#341-advantage-actor-critic-a2c",
    
    "relUrl": "/class-notes/04-Computational-Sensorimotor-Learning/policy-grad.html#341-advantage-actor-critic-a2c"
  },"307": {
    "doc": "3. Policy Gradients",
    "title": "3.4.2 Generalized Advantage Estimation (GAE)",
    "content": "GAE balances bias and variance by mixing MC and approximated estimation. Consider \\(r(s_t,a_t) + \\gamma V^\\pi_\\psi (s_{t+1})\\) which is a biased estimate. One way to suppress variance in advantage estimation is incorporating more steps of rewards sampled from invronment. This is what we mean if we incorporate one more step: . \\[\\begin{equation*} A^1(s_t,a_t) = r(s_t,a_t) + \\gamma r(s_{t+1}, a_{t+1}) + \\gamma^2 V^\\pi_\\psi (s_{t+2}) - V^\\pi_\\psi (s_t) \\end{equation*}\\] We can generalize this to \\(n\\) steps: . \\[\\begin{equation*} A^n(s_t,a_t) = \\sum_{i=0}^{n} \\gamma^{i-1} r(s_{t+i}, a_{t+i}) + \\gamma^{n+1} V^\\pi_\\psi (s_{t+n+1}) - V^\\pi_\\psi (s_t) \\end{equation*}\\] How to choose \\(n\\)? Instead of picking, GAE uses exponential weighted average to mix all \\(A^i\\) with a parameter \\(\\lambda\\) which gives us: . \\[\\begin{equation} A^{\\text{GAE}(\\gamma, \\lambda)}(s_t, a_t) = \\sum_{i=1}^T (\\gamma \\lambda)^i A^i(s_t, a_t) \\end{equation}\\] where \\(\\lambda\\) controls bias and variance. If \\(\\lambda = 0\\), then we only use 1-step advantage estimation. If \\(\\lambda = 1\\), then we are doing MC estimation. ",
    "url": "/blog/class-notes/04-Computational-Sensorimotor-Learning/policy-grad.html#342-generalized-advantage-estimation-gae",
    
    "relUrl": "/class-notes/04-Computational-Sensorimotor-Learning/policy-grad.html#342-generalized-advantage-estimation-gae"
  },"308": {
    "doc": "3. Policy Gradients",
    "title": "3.5 Exploration and Data Diversity",
    "content": "Asynchronous sampling A3C (Asynchronous Advantage Actor-Critic) is a variant of A2C that is asynchronous. It collects multiple trajectories in parallel using copies of the same policy. A3C consists of one master and multiple workers. This allows us to increase sample size. Each worker fetches master’s policy parameters asynchronously, computes policygradient using collected trajectories and sends gradient back to master thread. So what is the correctness of accumulating these gradients? Still leads to satisfactory performance. Entropy Regularization Regularize policy gradients with entropy to increase diversity of actions executed by agent. Regularized objective is: . \\[\\begin{align} \\sum_{i=1}^N \\sum_{t=1}^T \\nabla_\\theta \\log \\pi(a \\mid s_t^i) A(s_t, a_t) + \\mathcal{H}(\\pi(a \\mid s_t)) \\\\ \\mathcal{H}(\\pi(a \\mid s_t)) = -\\sum_{a \\in \\mathcal{A}} \\pi(a \\mid s_t) \\log \\pi(a \\mid s_t) \\end{align}\\] \\(\\mathcal{H}\\) is the entropy and we just take the entropy of the policy. This term encourages exploration by making parameters want to increase the entropy of the policy. ",
    "url": "/blog/class-notes/04-Computational-Sensorimotor-Learning/policy-grad.html#35-exploration-and-data-diversity",
    
    "relUrl": "/class-notes/04-Computational-Sensorimotor-Learning/policy-grad.html#35-exploration-and-data-diversity"
  },"309": {
    "doc": "3. Policy Gradients",
    "title": "3.6 Conservative Policy Optimization",
    "content": "Now lets think about our policy gradient problem. Since our data collection comes from the same policy, a bad policy may collect bad data. It’s a bad feedback loop where we may not be able to get a good policy. One way to get around this is to ensure our updates are conservative. We do this to make sure that we don’t get into this bad feedback loop. This conservative update correlates to finding a conservative gradient step. ",
    "url": "/blog/class-notes/04-Computational-Sensorimotor-Learning/policy-grad.html#36-conservative-policy-optimization",
    
    "relUrl": "/class-notes/04-Computational-Sensorimotor-Learning/policy-grad.html#36-conservative-policy-optimization"
  },"310": {
    "doc": "3. Policy Gradients",
    "title": "3.6.1 Parameter Space Constraint",
    "content": "One way to do this is to re-formulate our optimization problem: . \\[\\begin{equation} \\max_{\\theta} J(\\theta) \\\\ \\text{s.t.} \\quad \\| \\theta \\|_2^2 \\leq \\epsilon \\end{equation}\\] We can re-cast this problem as: . \\[\\begin{equation} \\max_{d} J(\\theta + d) \\\\ \\text{s.t.} \\quad \\| d \\|_2^2 \\leq \\epsilon \\end{equation}\\] we can get the Lagrangian of this problem: . \\[\\begin{equation} \\max_{d} J(\\theta + d) - \\lambda \\| d \\|_2^2 \\end{equation}\\] Since \\(J(\\theta + d)\\) is unknown, we can use a first-order approximation: . \\[\\begin{equation*} J(\\theta + d) \\approx J(\\theta) + d^T \\nabla J(\\theta) + O(d^2) \\end{equation*}\\] Using this approximation, we can write the Lagrangian as: . \\[\\begin{equation} \\max_{d} J(\\theta) + d^T \\nabla J(\\theta) - \\lambda \\| d \\|_2^2 \\end{equation}\\] if we set the gradient with respect to \\(d\\) to zero, we get: . \\[\\begin{align*} 0 &amp;= \\nabla J(\\theta) - 2\\lambda d \\\\ d &amp;= \\frac{1}{2\\lambda} \\nabla J(\\theta) \\end{align*}\\] which is just vanilla gradient ascent but we have a constraint on the step size or learning rate. Additionally, we want to make sure we follow this: . \\[\\begin{equation*} \\| \\beta d^* \\|_2^2 \\leq \\epsilon \\end{equation*}\\] which implies \\(\\beta = \\sqrt{\\frac{\\epsilon}{\\| d^* \\|_2^2}}\\). And thus our gradient step is: . \\[\\begin{equation} \\theta \\gets \\theta + \\beta d^* = \\theta + \\sqrt{\\frac{\\epsilon}{\\| d^* \\|_2^2}} d^* \\end{equation}\\] Another thing to note is how to pick \\(\\epsilon\\). We need to pick it such that the policy change is within a desired range. One thing to note is that the parameter difference depends on the current policy parametrizations and is not fixed. ",
    "url": "/blog/class-notes/04-Computational-Sensorimotor-Learning/policy-grad.html#361-parameter-space-constraint",
    
    "relUrl": "/class-notes/04-Computational-Sensorimotor-Learning/policy-grad.html#361-parameter-space-constraint"
  },"311": {
    "doc": "3. Policy Gradients",
    "title": "3.6.2 Parametrization-independent Constraints on Output Space",
    "content": "This is where Natural Policy Gradient (NPG) comes in. We motivate NPG by looking at our constraints again: . \\[\\begin{align*} d^T d \\leq \\epsilon \\\\ \\sum_{i,j} \\mathbf{I}_{ij} d_i d_j \\leq \\epsilon \\end{align*}\\] we want to say something like \\(\\| \\partial \\log \\pi_\\theta(a \\mid s) \\|_2^2 \\leq \\epsilon\\) rather than \\(\\| d \\|_2^2 \\leq \\epsilon\\). This is more direct way of talking about small policy changes. We can do this by including the Fisher information matrix into the constraints above: . \\[\\begin{align*} d^T F d \\leq \\epsilon \\\\ \\sum_{i,j} \\frac{\\partial \\log \\pi_\\theta (a \\mid s)}{\\partial \\theta_i} \\frac{\\partial \\log \\pi_\\theta(a \\mid s)}{\\partial \\theta_j} d_i d_j \\leq \\epsilon \\end{align*}\\] where \\(\\mathbf{F}(\\theta)\\) is the Fisher information matrix: . \\[\\begin{equation} \\mathbf{F}(\\theta) = \\mathbb{E}_{\\pi_\\theta (a \\mid s)} \\left[ \\nabla_\\theta \\log \\pi_\\theta(a \\mid s) \\nabla_\\theta \\log \\pi_\\theta(a \\mid s)^T \\right] \\end{equation}\\] which is the covariance of the gradient of the log-likelihood of the policy (expected value of outer product). As \\(\\frac{\\partial \\log \\pi_\\theta (a \\mid s)}{\\partial \\theta_i} \\frac{\\partial \\log \\pi_\\theta(a \\mid s)}{\\partial \\theta_j} d_i d_j\\) has unit where \\(d_i = \\partial \\theta_i\\) and \\(d_j = \\partial \\theta_j\\), we can see that: . \\[\\begin{equation} \\frac{\\partial \\log \\pi_\\theta(a \\mid s) \\partial \\log \\pi_\\theta(a \\mid s) \\partial \\theta_i \\partial \\theta_j} {\\partial \\theta_i \\partial \\theta_j} = (\\partial \\log \\pi_\\theta(a \\mid s))^2 \\end{equation}\\] which makes the constraint pretty much \\(\\sum_{i,j} (\\partial \\log \\pi_\\theta(a \\mid s))^2 \\leq \\epsilon\\). This makes the constraint parametrization independent. Lots of rigorous analysis in information geometry to get this result. We just use this. Turns out Fisher information matrix is the only way to get parametrization independent metric. Now we can write our final optimization problem: . \\[\\begin{equation} \\max_{d} J(\\theta + d) \\\\ \\text{s.t.} \\quad d^T \\mathbf{F}(\\theta) d \\leq \\epsilon \\end{equation}\\] How do we solve this? We do something similar to before where we use first-order approximation: . \\[\\begin{equation*} J(\\theta + d, \\lambda) \\approx J(\\theta) + d^T \\nabla J(\\theta) - \\lambda (\\frac{1}{2} d^T \\mathbf{F}(\\theta) d - \\epsilon) \\end{equation*}\\] then set gradient to zero and solve: . \\[\\begin{align*} 0 &amp;= \\nabla J(\\theta) - \\lambda \\mathbf{F}(\\theta) d \\\\ d^* &amp;= \\frac{1}{\\lambda} \\mathbf{F}(\\theta)^{-1} \\nabla J(\\theta) \\end{align*}\\] Now we just need to tighten the constraint \\(d^T \\mathbf{F}(\\theta) d \\leq \\epsilon\\) just like before using \\(\\beta\\). We do this by plugging \\(d^*\\) into constriant: . \\[\\begin{align*} &amp;\\frac{\\lambda^2}{2} (d^*)^T \\mathbf{F}(\\theta) d^* \\\\ &amp;= \\frac{\\lambda^2}{2} (\\beta \\mathbf{F}(\\theta)^{-1} \\nabla J(\\theta))^T \\mathbf{F}(\\theta) (\\beta \\mathbf{F}(\\theta)^{-1} \\nabla J(\\theta)) \\\\ &amp;= \\frac{\\lambda^2 \\beta^2}{2} \\nabla J(\\theta)^T \\mathbf{F}(\\theta)^{-T} \\mathbf{F}(\\theta) \\mathbf{F}(\\theta)^{-1} \\nabla J(\\theta) \\\\ &amp;= \\frac{\\lambda^2 \\beta^2}{2} \\nabla J(\\theta)^T \\mathbf{F}(\\theta)^{-1} \\nabla J(\\theta) \\leq \\epsilon \\end{align*}\\] we use knowledge that \\(\\mathbf{F}(\\theta)^{-T} = \\mathbf{F}(\\theta)^{-1}\\). Now we can solve for \\(\\beta\\) to find the value that tightens inequality: . \\[\\begin{equation} \\beta^* = \\frac{1}{\\lambda} \\sqrt{\\frac{2\\epsilon}{\\nabla J(\\theta)^T \\mathbf{F}(\\theta)^{-1} \\nabla J(\\theta)}} \\end{equation}\\] same as before, we can plug this into our gradient step:\\(\\theta \\gets \\theta + \\beta^* d^*\\) . Connection to Kullback-Leibler (KL) Divergence The second-order expansion of KL-divergence is Fisher information matrix, thus NPG can be rewritten as KL-divergence: . \\[\\begin{equation} \\max_{d} J(\\theta + d) \\\\ \\text{s.t.} \\quad D_{KL}(\\pi_\\theta(a \\mid s) || \\pi_{\\theta + d}(a \\mid s)) \\leq \\epsilon \\end{equation}\\] Connection to Newton Method NPG is very similar to Newton’s method for second-order optimizaiton: . \\[\\begin{align*} \\theta &amp;\\gets \\theta + \\beta \\mathbf{H}^{-1}(\\theta) \\nabla J(\\theta) \\\\ \\theta &amp;\\gets \\theta + \\beta \\mathbf{F}(\\theta)^{-1} \\nabla J(\\theta) \\end{align*}\\] where the first expression is Newton’s method (\\(\\mathbf{H}(\\theta)\\) is hessian) and second expression is NPG. Since we know that Fisher information matrix is the second-order approximation of KL-divergence, we can see that NPG is Newton’s method of the KL-divergence problem. ",
    "url": "/blog/class-notes/04-Computational-Sensorimotor-Learning/policy-grad.html#362-parametrization-independent-constraints-on-output-space",
    
    "relUrl": "/class-notes/04-Computational-Sensorimotor-Learning/policy-grad.html#362-parametrization-independent-constraints-on-output-space"
  },"312": {
    "doc": "3. Policy Gradients",
    "title": "3.6.3 Monotonic Policy Improvement",
    "content": "While NPG ensures change in policy bounded by \\(\\epsilon\\), it does not guarantee that each policy update will improve policy performance measured by returns. Trust Region Policy Optimization (TRPO) mitigates this by maximizing policy improvement instead of just constraining changes in policy outputs. If policy improvement is guaranteed, then agent will not get stuck in poor local maxima and avoid the bad feedback loop (vicious cycle). We describe the problem as: . \\[\\begin{equation} \\max_\\theta J(\\theta + d) - J(\\theta) \\end{equation}\\] where the difference quantifies policy improvement of \\(\\pi_{\\theta + d}\\) with respect to old policy \\(\\pi_\\theta\\). We can show policy improvement can be rewritten as: . \\[\\begin{equation*} J(\\theta + d) - J(\\theta) = \\mathbb{E}_{\\pi \\sim \\pi_{\\theta + d}} \\left[ \\sum_{t=1}^T \\gamma^{t-1} A^{\\pi_\\theta}(s_t,a_t) \\right] \\end{equation*}\\] with this in mind, we restate optimization as: . \\[\\begin{equation*} \\max_d \\mathbb{E}_{\\tau \\sim \\pi_{\\theta + d}} \\left[ \\sum_{t=1}^T \\gamma^{t-1} A^{\\pi_\\theta}(s_t,a_t) \\right] \\end{equation*}\\] We can break down this equation as follows: . \\[\\begin{align*} \\mathbb{E}_{\\tau \\sim \\pi_{\\theta + d}} \\left[ \\sum_{t} \\gamma^{t-1} A^{\\pi_\\theta}(s_t,a_t) \\right] &amp;= \\sum_{s,a} \\sum_t P(s=s_t,a=a_t \\mid \\pi_{\\theta + d})[\\gamma^{t-1} A^{\\pi_\\theta}(s,a)] \\\\ &amp;= \\sum_t \\sum_{s,a} \\gamma^{t-1} P(s=s_t,a=a_t \\mid \\pi_{\\theta + d}) [A^{\\pi_\\theta}(s,a)] \\\\ &amp;= \\sum_t \\sum_s \\gamma^{t-1} P(s_t=s \\mid \\pi_{\\theta + d}) \\sum_a \\pi_{\\theta + d}(a_t = a \\mid s_t) [ A^{\\pi_\\theta}(s,a) ] \\\\ &amp;= \\sum_s \\rho^\\gamma_{\\pi_{\\theta + d}}(s) \\sum_a \\pi_{\\theta + d} (a \\mid s) [A^{\\pi_\\theta}(s,a)] \\\\ &amp;= \\sum_s \\rho^\\gamma_{\\pi_{\\theta + d}}(s) \\sum_s \\pi_\\theta(a \\mid s) \\frac{\\pi_{\\theta + d}(a \\mid s)}{\\pi_\\theta(a \\mid s)} [ A^{\\pi_\\theta}(s,a) ] \\\\ &amp;= \\mathbb{E}_{s \\sim \\pi_{\\theta + d}, a \\sim \\pi_\\theta} \\left[ \\frac{\\pi_{\\theta + d}(a \\mid s)}{\\pi_\\theta(a \\mid s)} A^{\\pi_\\theta}(s,a) \\right] \\end{align*}\\] where \\(\\rho^\\gamma_{\\pi_{\\theta + d}}(s) = \\sum_t \\gamma^{t-1} P(s=s_t \\mid \\pi_{\\theta + d})\\). We can then rewrite the optimization problem as: . \\[\\begin{equation} \\max_d \\mathbb{E}_{s \\sim \\pi_{\\theta + d}, a \\sim \\pi_\\theta} \\left[ \\frac{\\pi_{\\theta + d}(a \\mid s)}{\\pi_\\theta(a \\mid s)} A^{\\pi_\\theta}(s,a) \\right] \\end{equation}\\] since it requires sampling from the new policy \\(\\pi_{\\theta + d}\\), there is an inefficiency there (need rollouts in environment with new policy). We can approximate the performance of the new policy using state distribution of old policy \\(\\pi_\\theta\\): . \\[\\begin{equation} J(\\theta + d) \\geq \\hat J_\\theta (\\theta + d) - C \\cdot D_{KL}^{\\max}(\\pi_\\theta || \\pi_{\\theta + d}(a \\mid s)) \\end{equation}\\] where \\(C = \\frac{4 \\epsilon \\gamma}{(1-\\gamma)^2}\\), \\(D^{\\max}_{KL} = \\max_s D_{KL}\\), and \\(\\hat J(\\theta + d) = \\mathbb{E}_{s \\sim \\pi_\\theta, a \\sim \\pi_{\\theta + d}} [A^{\\pi_\\theta}(s,a)]\\). KL Divergence : . \\[\\begin{align} D_{KL}(P || Q) &amp;= \\sum_{x \\in \\mathcal{X}} p(x) \\log \\frac{p(x)}{q(x)} \\\\ &amp;= \\mathbb{E}_{x \\sim P} \\left[ \\log\\frac{p(x)}{q(x)} \\right] \\end{align}\\] Using this new approximation, we can rewrite the optimization problem as: . \\[\\begin{equation} \\max_d \\mathbb{E}_{s \\sim \\pi_\\theta, a \\sim_\\theta} \\left[ \\frac{\\pi_{\\theta + d}(a \\mid s)}{\\pi_\\theta (a \\mid s)}A^{\\pi_\\theta}(s,a) \\right] - C D_{KL}^\\max(\\pi_\\theta(a \\mid s) || \\pi_{\\theta + d}(a \\mid s)) \\end{equation}\\] This is TRPO. The approximation we’re using at the end here is only valid if the state distributions from the new policy is similar to the old policy. However, how small of a change we need to make for this to be valid is in practice very small. (especially if we use \\(C\\)’s derived expression). Another issue is that \\(D_{KL}^\\max\\) is typically intractable (I’ve only ever calculated this for gaussians. We don’t want to only have gaussian policies). Instead we can use the expected KL-divergence (which is always estimated through sampling). Then we move the KL-divergence term as a constraint to enable larger step sizes: . \\[\\begin{equation} \\max_d \\mathbb{E}_{s \\sim \\pi_\\theta, a \\sim_\\theta} \\left[ \\frac{\\pi_\\theta(a \\mid s)}{\\pi_\\theta(a \\mid s)} A^{\\pi_\\theta}(s,a) \\right] \\\\ \\text{s.t.} \\quad \\mathbb{E}_s \\left[ D_{KL}(\\pi_\\theta(a \\mid s) || \\pi_{\\theta + d}(a \\mid s)) \\right] \\leq \\epsilon \\end{equation}\\] This looks awfully similar to the NPG problem we had before. Turns out we use the same methodology to solve this problem and get gradient update steps. There is an extra step employed which uses conjugate gradients instead to avoid calculating the matrix inverse of the hessian. What’s the difference? Before, we were using the Fisher information matrix to get the gradient step which doesn’t guarantee performance improvement. Now, we are using the KL-divergence to get the gradient step which “guarantees” performance improvement given small enough steps. With all of these approximations, how do we know that our policy will improve? We will have to use \\(\\beta\\) to make this guaranteed and the constraint is tightened/satisfied. We do the following: . \\[\\beta \\gets \\begin{cases} 0.5 \\beta &amp; \\text{if } D_{KL}(\\pi_\\theta(a\\mid s) || \\pi_{\\theta + d}(a \\mid s)) &lt; \\epsilon \\\\ 0.5 \\beta &amp; \\text{if } \\hat J(\\theta, d_{\\text{new}}) - \\hat J(\\theta, d_{\\text{old}}) &lt; 0 \\\\ \\beta &amp; \\text{otherwise} \\end{cases}\\] At first, \\(\\beta\\) is initialized using NPG’s method. Adaptive Lagrangian Method Line search may be expensive, we can solve the unconstrained lagrangian and use adaptive updates on the lagrange multiplier. \\[\\begin{equation} \\max_d \\mathbb{E}_{s \\sim \\pi_\\theta, a \\sim \\pi_\\theta} \\left[ \\frac{\\pi_{\\theta + d}(a \\mid s)}{\\pi_\\theta (a \\mid s)} A^{\\pi_\\theta}(s,a) - \\lambda D_{KL}(\\pi_\\theta(\\cdot \\mid s) || \\pi_{\\theta + d}(\\cdot \\mid s)) \\right] \\end{equation}\\] these are updates for lagrange multiplier: . \\[\\begin{equation} \\lambda \\gets \\begin{cases} \\frac{\\lambda}{2} &amp; \\delta_{KL} &lt; \\delta_{\\text{target}} / 1.5 \\\\ 2\\lambda &amp; \\delta_{KL} \\geq \\delta_{\\text{target}} \\cdot 1.5 \\end{cases} \\end{equation}\\] where \\(\\delta_{KL} = D_{KL}(\\pi_\\theta(a\\mid s) \\| \\pi_{\\theta + d}(a \\mid s))\\) and \\(\\delta_{\\text{target}}\\) is arbitrary threshold. Clipping Removing the KL-divergence constraint, we can propose to clip the objective function instead of enforcing the constraint in policy updates. This is called Proximal Policy Optimization (PPO). To figure this out, we restart back to the original optimization problem (still doing monotonic policy improvement): . \\[\\begin{equation*} \\mathbb{E}_{s \\sim \\pi_{\\theta + d}, a \\sim \\pi_\\theta} \\left[ \\frac{\\pi_{\\theta + d}(a \\mid s)}{\\pi_\\theta (a \\mid s)} A^{\\pi_\\theta}(s,a) \\right] \\end{equation*}\\] as \\(\\frac{\\pi_{\\theta + d}(a \\mid s)}{\\pi_\\theta (a \\mid s)}\\) weighs the advantage, the idea of PPO is to penalize a new policy \\(\\pi_{\\theta + d}\\) that has either a large or small ratio \\(c_t = \\frac{\\pi_{\\theta + d}(a \\mid s)}{\\pi_\\theta (a \\mid s)}\\). This penalization is realized by clipping \\(c_t\\) within a pre-defined bound: . \\[\\begin{equation} \\mathbb{E}_{s \\sim \\pi_\\theta, a \\sim \\pi_\\theta} \\left[ \\min \\{ \\frac{\\pi_{\\theta+d}(a\\mid s)}{\\pi_\\theta (a \\mid s)}A^{\\pi_\\theta}(s,a), \\text{clip}(\\frac{\\pi_{\\theta+d}(a\\mid s)}{\\pi_\\theta (a \\mid s)}, 1-\\epsilon, 1+\\epsilon) A^{\\pi_\\theta}(s,a) \\} \\right] \\end{equation}\\] so we choose between the clipped and unclipped objective (whichever one is better). This has way less compute and performs better than TRPO. ",
    "url": "/blog/class-notes/04-Computational-Sensorimotor-Learning/policy-grad.html#363-monotonic-policy-improvement",
    
    "relUrl": "/class-notes/04-Computational-Sensorimotor-Learning/policy-grad.html#363-monotonic-policy-improvement"
  },"313": {
    "doc": "3. Policy Gradients",
    "title": "3.7 Practical Considerations of PPO",
    "content": "PPO uses a lot of code-level optimzations to make it work. Without these optimizations, PPO cannot outperform TRPO. This is one of the reasons why PPO has such a wide range of performance on the same tasks across open-sourced implementations. Read this for more details on the practical considerations of PPO. ",
    "url": "/blog/class-notes/04-Computational-Sensorimotor-Learning/policy-grad.html#37-practical-considerations-of-ppo",
    
    "relUrl": "/class-notes/04-Computational-Sensorimotor-Learning/policy-grad.html#37-practical-considerations-of-ppo"
  },"314": {
    "doc": "3. Policy Gradients",
    "title": "3.8 Debugging",
    "content": ". | Visualize policy performance at initialization | Analyze returns at initialization | Increase batch size | Periodically visualize agent behavior | Entropy of actions | . ",
    "url": "/blog/class-notes/04-Computational-Sensorimotor-Learning/policy-grad.html#38-debugging",
    
    "relUrl": "/class-notes/04-Computational-Sensorimotor-Learning/policy-grad.html#38-debugging"
  },"315": {
    "doc": "Point RL",
    "title": "Point RL",
    "content": "We’re currently trying to use PointNet to learn a policy for dense insertion. The idea is to use PointNet to combine features from point clouds and tactile data. The interest stems from the fact that everything we do lies in the 3d world. However, we currently limit ourselves to data that lies in 2d space. Currently, most people use RGB-based policies or visuo-tactile policies which also uses 2d images. There are already works that do 3d point cloud-based policies. However, they don’t utilize the force information that you get through tactile sensors. ",
    "url": "/blog/project-notes/project1/project.html",
    
    "relUrl": "/project-notes/project1/project.html"
  },"316": {
    "doc": "Point RL",
    "title": "Related works",
    "content": "3D-ViTac . 3D-ViTac uses tactile sensors to get point clouds. | It uses PointNet++ as the backbone. | It includes visual point clouds and tactile point clouds. | It uses one-hot encoding to differentiate between the two types of point clouds. | This is object-centric. | . 3D Diffusion Policy . 3D-DP uses scene point clouds with diffusion policy. | It uses PointNet for a baseline. | It takes a scene point cloud. | Removes things like input transform and feature transform from PointNet implementation. | . M3L Paper . M3L . | RL training with 2d data with image + tactile image. | Masked Multi-modal learning | . 3D-RL . 3D-RL some general paper on trying pointcloud-based RL. | Tries PointNet and 3D SparseConvNet. | Suggests processing temporal data with MinkowskiNet (not computationally efficient). | These guys use one-hot encoding to differentiate between frames for object. | . Dense Robotic Packing . Dense Robotic Packing This paper is about dense insertion of objects in a bin. It uses geometrical intuition to figure out where to insert. | TODO: finish reading in full. | . ManipGen . ManipGen is a paper that tries to address scene generation in addition to manipulation. Steerable Scene Generation . Steerable Scene Generation . | TODO: read | . Asymmetric Actor Critic . Asymmetric Actor Critic . | TODO: read | . Badminton Asymmetric Actor Critic . Badminton Asymmetric Actor Critic . | this is more of a systems paper. | use as a reference for engineering. | it’s in science because of how good the system is. | TODO: read | . Unicorn-Hamnet . Unicorn-Hamnet . | automatic generation of robot manipulation scene. | TODO: read | . CORN . CORN . | TODO: read | . Point-BERT . Point-BERT . Point-M2AE . Point-M2AE . Point-MAE . Point-MAE . Point Transformer . Point Transformer . ### . ",
    "url": "/blog/project-notes/project1/project.html#related-works",
    
    "relUrl": "/project-notes/project1/project.html#related-works"
  },"317": {
    "doc": "Python Environments",
    "title": "Pyproject and UV",
    "content": "We set up python project management using pyproject.toml and uv. Go into your project directory. Then run, uv init. $ cd hello-world $ uv init . uv will create the following: . ├── .gitignore ├── .python-version ├── README.md ├── main.py └── pyproject.toml . We can run python scripts using uv run [insert file]. uv creates virtual environment in uv.lock . ",
    "url": "/blog/code-notes/python-env.html#pyproject-and-uv",
    
    "relUrl": "/code-notes/python-env.html#pyproject-and-uv"
  },"318": {
    "doc": "Python Environments",
    "title": "Python Environments",
    "content": " ",
    "url": "/blog/code-notes/python-env.html",
    
    "relUrl": "/code-notes/python-env.html"
  },"319": {
    "doc": "4. Reward Function",
    "title": "4. Reward Function Design",
    "content": ". | Supervised learning we solve optimization from data: | . \\[\\begin{equation*} \\min_f \\mathbb{E} [(y-f(x))^2] \\end{equation*}\\] . | Policy gradient we solve optimization: | . \\[\\begin{equation*} \\max_\\theta \\mathbb{E} [\\sum_t^T \\nabla_\\theta \\log \\pi_\\theta (a \\mid s_t) R(\\tau_{t:T})] \\end{equation*}\\] we can see that \\(R(\\tau_{t:T})\\) is sampled by the policy instead of a fixed dataset. ",
    "url": "/blog/class-notes/04-Computational-Sensorimotor-Learning/reward-function.html#4-reward-function-design",
    
    "relUrl": "/class-notes/04-Computational-Sensorimotor-Learning/reward-function.html#4-reward-function-design"
  },"320": {
    "doc": "4. Reward Function",
    "title": "5.1 Challenges of Reward Function Design",
    "content": "Sparse Reward . \\[\\begin{equation*} r(s,a) = \\begin{cases} 1, &amp; \\mathcal{T}(s' \\mid s,a) = s_g \\\\ 0, &amp; \\text{otherwise} \\end{cases} \\end{equation*}\\] This specifies reaching the goal but is too sparse to optimize for policy gradient. Dense Reward To address reward sparsity, an alternative is to use distance to the goal as reward for encouraging agent to approach goal (MPC-style cost function). \\[\\begin{equation*} r(s,a) = \\begin{cases} -\\| s_g - s' \\|_2 + 1, &amp; s' = s_g, s' = \\mathcal{T}(s' \\mid s,a) \\\\ -\\| s_g - s' \\|_2, &amp; \\text{otherwise} \\end{cases} \\end{equation*}\\] Oracle Reward However, if we’re doing things like obstacle avoidance, we have to take a detour which is not reflected in this reward function. We will call this detour distance the geodesic distance and we will have to add (\\(D_g\\)) that into our reward function: . \\[\\begin{equation*} r(s,a) = \\begin{cases} -D_g(s', s_g) + 1, &amp; s' = s_g, s' = \\mathcal{T}(s' \\mid s,a) \\\\ -D_g(s', s_g), &amp; \\text{otherwise} \\end{cases} \\end{equation*}\\] Getting geodesic distance is hard, and we sometimes need to get enough samples to estimate the geodesic distance. Here’s the summary of 3 reward function design: . | Sparse reward: gives the correct reward but is too sparse to optimize for policy gradient. | Dense reward: easy to optimize but gives wrong objective to optimize. | Oracle reward: ideal reward function which is correct and easy to optimize. | . ",
    "url": "/blog/class-notes/04-Computational-Sensorimotor-Learning/reward-function.html#51-challenges-of-reward-function-design",
    
    "relUrl": "/class-notes/04-Computational-Sensorimotor-Learning/reward-function.html#51-challenges-of-reward-function-design"
  },"321": {
    "doc": "4. Reward Function",
    "title": "5.2 Reward Function Engineering",
    "content": "Achieve goal as soon as possible . \\[\\begin{align*} r(s,a) &amp;= \\begin{cases} 1, &amp; \\mathcal{T}(s' \\mid s,a) = s_g \\\\ -0.1 &amp; \\text{otherwise} \\end{cases} \\\\ r(s,a) &amp;= - \\| a \\|_2 \\end{align*}\\] We punish how many state transitions occur (try to finish as soon as possible) and we also regularize the action to be small. Avoid danger . \\[\\begin{equation*} r(s,a) = \\begin{cases} 1, &amp; \\mathcal{T}(s' \\mid s,a) = s_g \\\\ -0.3 &amp; \\mathcal{T}(s' \\mid s,a) = \\text{Walls} \\\\ -0.1 &amp; \\text{otherwise} \\end{cases} \\end{equation*}\\] we add penalty for hitting walls. this can be considered sparse reward, and we can do a much more dense reward function by adding a distance to the wall (but be careful as the goal is not to be far from the wall, but to avoid it). We can be close to the wall as long as we don’t hit it (maybe it helps to get an optimal policy to be close to the wall). ",
    "url": "/blog/class-notes/04-Computational-Sensorimotor-Learning/reward-function.html#52-reward-function-engineering",
    
    "relUrl": "/class-notes/04-Computational-Sensorimotor-Learning/reward-function.html#52-reward-function-engineering"
  },"322": {
    "doc": "4. Reward Function",
    "title": "5.3 Reward Shaping",
    "content": "\\[\\begin{equation*} r'(s,a) = r(s_t, a_t) + \\phi(s_t) - \\phi(s_{t-1}) \\end{equation*}\\] where \\(\\phi(s)\\) is a potential function. The shapes reward function is unbiased and gives rise to optimal policy invariant to unshaped reward function. This is because \\(\\phi(s') - \\phi(s)\\) cancels out when summing over time. This principle of reward shaping has not been shown to be useful in practice. ",
    "url": "/blog/class-notes/04-Computational-Sensorimotor-Learning/reward-function.html#53-reward-shaping",
    
    "relUrl": "/class-notes/04-Computational-Sensorimotor-Learning/reward-function.html#53-reward-shaping"
  },"323": {
    "doc": "4. Reward Function",
    "title": "4. Reward Function",
    "content": " ",
    "url": "/blog/class-notes/04-Computational-Sensorimotor-Learning/reward-function.html",
    
    "relUrl": "/class-notes/04-Computational-Sensorimotor-Learning/reward-function.html"
  },"324": {
    "doc": "RL Games Coupling",
    "title": "RL Games Coupling",
    "content": "In IsaacGymEnvs train.py, we use Runner, model_builder, env_configurations, and vecenv. Right now it seems vecenv, model_builder, and env_configurations are used for flags/configurations, while `Runner has most of the logic for training and deployment. ",
    "url": "/blog/code-notes/isaac/rl_games_integration.html",
    
    "relUrl": "/code-notes/isaac/rl_games_integration.html"
  },"325": {
    "doc": "RL Games Coupling",
    "title": "RL_GAMES: Runner",
    "content": "Runner is made in rl_games.torch_runner.py. First observation we make is in the __init__ method, . class Runner: def __init__(self, algo_observer=None): self.algo_factory = object_factory.ObjectFactory() self.algo_factory.register_builder('a2c_continuous', lambda **kwargs : a2c_continuous.A2CAgent(**kwargs)) self.algo_factory.register_builder('a2c_discrete', lambda **kwargs : a2c_discrete.DiscreteA2CAgent(**kwargs)) self.algo_factory.register_builder('sac', lambda **kwargs: sac_agent.SACAgent(**kwargs)) self.player_factory = object_factory.ObjectFactory() self.player_factory.register_builder('a2c_continuous', lambda **kwargs : players.PpoPlayerContinuous(**kwargs)) self.player_factory.register_builder('a2c_discrete', lambda **kwargs : players.PpoPlayerDiscrete(**kwargs)) self.player_factory.register_builder('sac', lambda **kwargs : players.SACPlayer(**kwargs)) self.algo_observer = algo_observer if algo_observer else DefaultAlgoObserver() torch.backends.cudnn.benchmark = True . this gives us a clue that there is a separation between “algo” and “player”. Additionally, there were comments in this class that suggests Runner also logs metrics for training and evaluation. Take note that algo_observer is the observer for logging purposes. Next thing Runner does in training script is to run runner.load(rlg_config_dict). def load(self, yaml_config): config = deepcopy(yaml_config) self.default_config = deepcopy(config['params']) self.load_config(params=self.default_config) def load_config(self, params): # NOTE: params is from a config file (.yaml) self.seed = params.get('seed', None) if self.seed is None: self.seed = int(time.time()) ... # there was code here for multi_gpu print(f\"self.seed = {self.seed}\") self.algo_params = params['algo'] self.algo_name = self.algo_params['name'] self.exp_config = None ... # there was code here for seed handling config = params['config'] config['reward_shaper'] = tr_helpers.DefaultRewardsShaper(**config['reward_shaper']) if 'features' not in config: config['features'] = {} config['features']['observer'] = self.algo_observer self.params = params . Finally Runner runs runner.play() which is the main loop for training and evaluation. def run(self, args): if args['train']: self.run_train(args) elif args['play']: self.run_play(args) else: self.run_train(args) # Lets look at run_play def run_play(self, args): player: players.PpoPlayerContinuous = self.create_player() _restore(player, args) # load _override_sigma(player, args) # override parameters in network labelled sigma player.run() def create_player(self): # remember self.player_factory from __init__!. return self.player_factory.create(self.algo_name, params=self.params) class ObjectFactory: def __init__(self): self._builders = {} def register_builder(self, name, builder): self._builders[name] = builder ... def create(self, name, **kwargs): # NOTE: name we use is a2c_continuous builder = self._builders.get(name) if not builder: raise ValueError(name) return builder(**kwargs) # this is players.PpoPlayerContinuous . Since we are initializing the players.PpoPlayerContinuous class, we can look at its __init__ method (which is called in create_player). class PpoPlayerContinuous(BasePlayer): def __init__(self, params): BasePlayer.__init__(self, params) self.network = self.config['network'] self.actions_num = self.action_space.shape[0] self.actions_low = torch.from_numpy(self.action_space.low.copy()).float().to(self.device) self.actions_high = torch.from_numpy(self.action_space.high.copy()).float().to(self.device) self.mask = [False] self.normalize_input = self.config['normalize_input'] self.normalize_value = self.config.get('normalize_value', False) obs_shape = self.obs_shape config = { 'actions_num' : self.actions_num, 'input_shape' : obs_shape, 'num_seqs' : self.num_agents, 'value_size': self.env_info.get('value_size',1), 'normalize_value': self.normalize_value, 'normalize_input': self.normalize_input, } self.model = self.network.build(config) self.model.to(self.device) self.model.eval() self.is_rnn = self.model.is_rnn() . For training, we get our configs from a yaml file like TacSLTaskInsertionPPO_LSTM_dict_AAC.yaml which has all of the stuff rl_games cares about. Based on this __init__ there is no reference of any of the IsaacGymEnvs configs. It’s just from the train yaml file like above. In order to load up the rl_games network and use it, we only need this yaml file. As we have discussed before, TacSLTaskInsertionPPO_LSTM_dict_AAC.yaml is a yaml file we would use to load and build the network in rl_games. Where is this code referenced in IsaacGymEnvs? It is in train.py. It is actually the rlg_config_dict file that we pass. TLDR: Runner is the main class that we want to work with. We should get all the parameters we need for our network from a yaml file like TacSLTaskInsertionPPO_LSTM_dict_AAC.yaml. This yaml file is passed to Runner as rlg_config_dict in train.py. ",
    "url": "/blog/code-notes/isaac/rl_games_integration.html#rl_games-runner",
    
    "relUrl": "/code-notes/isaac/rl_games_integration.html#rl_games-runner"
  },"326": {
    "doc": "RL Games Coupling",
    "title": "RL_GAMES: environment",
    "content": "How does rl_games actually use the environment from IsaacGymEnvs? . In train.py for IsaacGymEnvs, we have the following code: . from rl_games.common import env_configurations, vecenv def create_isaacgym_env(**kwargs): envs = isaacgymenvs.make( cfg.seed, cfg.task_name, cfg.task.env.numEnvs, cfg.sim_device, cfg.rl_device, cfg.graphics_device_id, cfg.headless, cfg.multi_gpu, cfg.capture_video, cfg.force_render, cfg, **kwargs, ) ... # capture_video code here return envs env_configurations.register('rlgpu', { 'vecenv_type': 'RLGPU', 'env_creator': lambda **kwargs: create_isaacgym_env(**kwargs), }) vecenv.register('RLGPU', lambda config_name, num_actors, **kwargs: RLGPUEnv(config_name, num_actors, **kwargs)) . This env_configurations works really like how algo_factory works where it’s a dictionary of environment configurations. This is the same for vecenv. You can consider vecenv and env_configurations as globals that rl_games will use down the line. We actually can see this by looking at the parent class of PpoPlayerContinuous, which is BasePlayer: . class BasePlayer(object): def __init__(self, params): # params is from that .yaml file we mentioned self.config = config = params['config'] self.load_networks(params) self.env_name = self.config['env_name'] self.player_config = self.config.get('player', {}) self.env_config = self.config.get('env_config', {}) self.env_config = self.player_config.get('env_config', self.env_config) self.env_info = self.config.get('env_info') ... # extra configs if self.env_info is None: use_vecenv = self.player_config.get('use_vecenv', False) if use_vecenv: print('[BasePlayer] Creating vecenv: ', self.env_name) self.env = vecenv.create_vec_env( self.env_name, self.config['num_actors'], **self.env_config) self.env_info = self.env.get_env_info() else: print('[BasePlayer] Creating regular env: ', self.env_name) self.env = self.create_env() self.env_info = env_configurations.get_env_info(self.env) else: self.env = config.get('vec_env') # NOTE: vecenv and env_configurations are globals updated # we know how to look up the configs because of the params . We can see self.env is created and we use it during runner.run. TLDR: So if we want to create a new environment, we just have to register it in env_configurations and vecenv like we did above. ",
    "url": "/blog/code-notes/isaac/rl_games_integration.html#rl_games-environment",
    
    "relUrl": "/code-notes/isaac/rl_games_integration.html#rl_games-environment"
  },"327": {
    "doc": "RL Games Coupling",
    "title": "IsaacGymEnvs: Things needed in environment",
    "content": "For IsaacGymEnvs, we create an environment by calling the following . isaacgym_task_map = { \"AllegroHand\": AllegroHand, \"AllegroKuka\": resolve_allegro_kuka, \"AllegroKukaTwoArms\": resolve_allegro_kuka_two_arms, \"AllegroHandManualDR\": AllegroHandDextremeManualDR, \"AllegroHandADR\": AllegroHandDextremeADR, \"Ant\": Ant, \"Anymal\": Anymal, \"AnymalTerrain\": AnymalTerrain, \"BallBalance\": BallBalance, \"Cartpole\": Cartpole, \"FactoryTaskGears\": FactoryTaskGears, \"FactoryTaskInsertion\": FactoryTaskInsertion, \"FactoryTaskNutBoltPick\": FactoryTaskNutBoltPick, \"FactoryTaskNutBoltPlace\": FactoryTaskNutBoltPlace, \"FactoryTaskNutBoltScrew\": FactoryTaskNutBoltScrew, \"IndustRealTaskPegsInsert\": IndustRealTaskPegsInsert, \"IndustRealTaskGearsInsert\": IndustRealTaskGearsInsert, \"FrankaCabinet\": FrankaCabinet, \"FrankaCubeStack\": FrankaCubeStack, \"Humanoid\": Humanoid, \"HumanoidAMP\": HumanoidAMP, \"Ingenuity\": Ingenuity, \"Quadcopter\": Quadcopter, \"ShadowHand\": ShadowHand, \"TacSLTaskInsertion\": TacSLTaskInsertion, \"AmazonTaskInsertion\": AmazonTaskInsertion, \"Trifinger\": Trifinger, } ... env = isaacgym_task_map[task_name]( cfg=task_config, rl_device=_rl_device, sim_device=_sim_device, graphics_device_id=graphics_device_id, headless=headless, virtual_screen_capture=virtual_screen_capture, force_render=force_render, ) . Out of this env needs the following properties: . env.observation_space env.action_space env.agents # 1 env.value_size # 1 env.get_number_of_agents # optional obs = env.reset() obs, rewards, dones, infos = env.step(actions) . ",
    "url": "/blog/code-notes/isaac/rl_games_integration.html#isaacgymenvs-things-needed-in-environment",
    
    "relUrl": "/code-notes/isaac/rl_games_integration.html#isaacgymenvs-things-needed-in-environment"
  },"328": {
    "doc": "Roadmap to RL",
    "title": "Roadmap to Reinforcement Learning for Manipulation",
    "content": " ",
    "url": "/blog/deep-dive-notes/tactile-rl-notes/roadmap-rl.html#roadmap-to-reinforcement-learning-for-manipulation",
    
    "relUrl": "/deep-dive-notes/tactile-rl-notes/roadmap-rl.html#roadmap-to-reinforcement-learning-for-manipulation"
  },"329": {
    "doc": "Roadmap to RL",
    "title": "1. Hard-code Approach",
    "content": "Something we want to break down first is the process of doing manipulation/control: . | Observation: What can the robot sense? | Action: What can the robot do? | Policy: How does the robot generate actions based on observations? | . We need to have a starting point of how to do manipulation before moving to tactile sensors and tactile policies. If we wanted to write up a policy for a task like pick-and-place, where we want to pick up a desired object and place it down in a specific location, we can break it down into the following steps: . | Identify object in the scene and get its pose. | Generate/Execute a trajectory to pick up the object. | Identify the target location and Generate/Execute a trajectory to place the object. | Let go of object and return to home pose. | . This seems very simple, and the observation is fairly simple as well. Using current state-of-the-art object detection algorithms + RGBD cameras, we can easily localize the object and use the manipulator to pick it up and place it down. However, notice that we divided up the policy into phases. This means that as the user, we have to do a lot of engineering and design for a certain manipulation task. VERSATLITY: Something that people in robotics are very interested in is versatility. Can I just minimally give specifications to the robot and it takes in observations and figures out exactly what actions to take? . CAPABILITY: Another thing to consider is whether a robot can perform a difficult task with some versatility. We can sacrifice the generality of the policy if it can perform a specific task well. ",
    "url": "/blog/deep-dive-notes/tactile-rl-notes/roadmap-rl.html#1-hard-code-approach",
    
    "relUrl": "/deep-dive-notes/tactile-rl-notes/roadmap-rl.html#1-hard-code-approach"
  },"330": {
    "doc": "Roadmap to RL",
    "title": "2. Control approach",
    "content": "Control theory has been a long-standing field that robotics has relied on for decades. The basis of control theory is to use the dynamics of the system to generate actions based on observations. As in, we encode the physics of the system into the policy and use that to generate actions. In aerospace and automative, control theory has found a lot of success because of the ability to define a state space (position, velocity) and goal (desired position, desired velocity) and encode dynamics around that. For example, let our system be defined such that the origin \\(g = \\begin{bmatrix} 0 \\\\ ... \\\\ 0 \\end{bmatrix}\\) is the desired state. Then we define a general nonlinear system that takes in state \\(x\\) and action \\(u\\) such that: . \\[\\dot{x} = f(x, u)\\] where \\(f\\) is the dynamics of the system. Our goal is to then stabilize the system around the origin. The traditional way to do this is to write out our policy by hand and plug in the equations into our robot code. The current state-of-the-art approach and most widely used is to generate an optimization problem that finds actions that best minimize a cost function. This field is very deep and has a lot of theory behind it. One of the greatest benefits of control theory is getting guarantees from our systems and leveraging the deep insights from decades of research. \\[\\min_{u_{0:T-1}} \\sum_{t=0}^T C(x_{0:T}, u_{0:T-1}) \\\\ \\text{s.t.} \\quad x_{t+1} = f_\\Delta(x_t, u_t)\\] While control is all great, there are a lot of limitations to applying it into the robotic manipulation setting. ISSUES IN MANIPULATION: . | If we wanted to sweep a pile of dust, how would we concretely define the state space of the dust particles? Without proper definition of state space, we cannot apply control theory or even designate what a goal is. | The dynamics of a system that does manipulation is very difficult to work with: . | Mathematicians work on what they finds interesting problems (things you can solve for and are nice). | Control theory stems from mathematical roots and thus uses lots of assumptions to make the math easier to work with. | The dynamics of a system with contact is very non-smooth in nature and cannot be easily solved using optimization (local methods) or through hand-written policies. | In fact, the optimization problem using the dynamics of this system is NP-hard. | NP problems are problems that are non-deterministic but can be verified in polynomial time. | As a reminder, NP-hard do not have to be NP. They just have to be as hard as an NP problem (NP-complete is reducible to said NP-hard problem). | . | One way of understanding this is that local actions around the state may or may not lead the state closer to a better solution. Non-local actions must be taken to get the optimal solution. | . | Normally the system is a fixed element that we specify. This restricts the versatility of our policy if we vary object shape, object mass, and object material. | . NOTE: People try to accommodate for these issues by integrating deep learning in areas that are hard to work with. For example, we can learn latent sensor dynamics instead of the system dynamics itself or learn a model that is robust to object variation. However, it has been found that these approaches still have non-locality issues and have not found large adoption. As a recap, here are issues with control theory: . | Defining state space | Non-smooth dynamics (non-local nature of manipulation) | Rigidity of the system (fixed object shape, mass, material) | . One way to forego the issues of state space is to do input-output control. Let \\(y = g(x)\\) be the observation of the system. Then we can define a policy \\(u = \\pi(y)\\) that takes in the observation and generates actions. In other words, we use the observations directly to generate actions without needing to define a state space. This is easier said than done. ",
    "url": "/blog/deep-dive-notes/tactile-rl-notes/roadmap-rl.html#2-control-approach",
    
    "relUrl": "/deep-dive-notes/tactile-rl-notes/roadmap-rl.html#2-control-approach"
  },"331": {
    "doc": "Roadmap to RL",
    "title": "3. Learning approach (BC)",
    "content": "Robot learning has been a rising topic in the past decade and has solidified itself as a key area of research. Everyone has hopped on the ML train. This crowd of people are completely separate from the control theory crowd, so their work does not try to solve the issues of control theory. The BC approach takes in a dataset of observations and actions, \\(D = \\{(y_i, u_i)\\}_{i=1}^N\\), and learns a policy \\(\\pi\\) that maps observations to actions. In this dataset, we also assume that the actions are optimal given the observations. This approach foregoes the need for state space or dynamics and instead focuses on learning a mapping from observations to actions. Conceptually, we can see that given an observation (e.g. RGB image of the scene), we can output an action that is optimal for the task using a neural network. There are still some connections to control theory: . | We can actually view a well-trained BC policy as something that tries to stabilize the observations instead of the state space (like input-output control). | Observe that when BC policy sees the “goal” image, it outputs no actions. This is similar to stabilizing the system around the origin. | . | The “goal” observation is implicitly encoded in the dataset instead of explicitly defined. Additionally, this “goal” is not a fixed element and can vary in the same dataset (goal can be noisy). | . NOTE that the connection to control ends with Multi-task BC. Multi-task BC is when we have multiple tasks in the same dataset and the “goal” is specified as a language instruction. Here’s a diagram of a sample Multi-task policy BC neural network architecture: . flowchart LR A(Image) --&gt; D[\\Encoder Head/] B(Robot Joints) --&gt; E[\\Encoder Head/] C(Language) --&gt; F[\\Encoder Head/] D --&gt; G[Concat] E --&gt; G[Concat] F --&gt; G[Concat] G --&gt; H[MLP with ReLU] H --&gt; I[/Decoder Head\\] I --&gt; J[Action] . Note that in here, the language instruction is used to “steer” the actions of a policy to perform a task from the dataset. This instruction is usually hard-coded into the dataset (No GPT robot unfortunately). TRI has shown that without language instruction (all null tokens), the policy still does things with objects. This gives the intuition that language embeddings are not describing goals but rather steering the policy towards a certain action distribution related to the task. RELATION TO MANIPULATION: BC has gained a lot of traction in manipulation because a lot of tasks can just be performed by a human operator and easily replicated by the robot after training. The method is very versatile in terms of not having to really change the code for the policy. However, you do have to collect data for every task you want to perform. This is a big limitation of BC, as human data collection is very expensive and time-consuming. ISSUES/LIMITATIONS: Current works are now trying to scale up BC and obtain a “GPT-moment” for robotics. However, there are still some limitations to BC: . | Language is not precise and can be ambiguous. “Move the red block a little bit” can mean different things to different people. | Is it even possible to collect enough data for every task on every robot? This is highly debated, but the current consensus is that it is not possible to collect enough data for every task on every robot. | Can our models even scale with this approach? Will training a larger model equate to better performance? | Can BC do dexterous manipulation? Most tasks are simple pick-and-place tasks. The real question is whether BC can/should do things like in-hand manipulation, object balancing or pivoting, or cook a wok. | . NOTE on Dexterous Manipulation: I honestly find that there’s a very blurry line on what is dexterous manipulation and what is not. Is cooking a wok really dexterous manipulation? We’re just moving a wok and spatula around. Many “dexterous manipulation” tasks are just pick-and-place or object grasping or moving objects around. It’s honestly becoming a catch-all term for any manipulation task that isn’t pick-and-place. ",
    "url": "/blog/deep-dive-notes/tactile-rl-notes/roadmap-rl.html#3-learning-approach-bc",
    
    "relUrl": "/deep-dive-notes/tactile-rl-notes/roadmap-rl.html#3-learning-approach-bc"
  },"332": {
    "doc": "Roadmap to RL",
    "title": "4. RL approach (Simulation)",
    "content": "Now reinforcement learning is not necessarily the next step up after BC, but it shows an interesting path forward. The jist of RL is the use of simulation to learn a policy. Can we get a robot to freely explore an environment and learn a policy that achieves a specified task? The big issue with RL is the sim2real gap. Can a robot learn a policy in simulation and transfer it to the real world? . If we can close this sim2real gap, then we can leverage the versatility and capabilities of simulation and use RL as a means of leveraging simulation to train strong manipulation policies. Using simulation, we can completely forego the need for human data collection or at least tremendously reduce it. RL OVERVIEW: The current state-of-the-art approach to robotics RL is to use PPO to train a neural network similar to the one used in BC in simulation. We can quickly summarize RL as follows: . \\[\\begin{equation} \\max_{d} J(\\theta + d) - J(\\theta) \\\\ \\text{s.t.} \\quad \\mathbb{E}_s \\left[ D_{KL}(\\pi_\\theta(a \\mid s) || \\pi_{\\theta + d}(a \\mid s)) \\right] \\leq \\epsilon \\end{equation}\\] . | \\(J(\\theta)\\) = \\(\\sum_{i=1}^N \\sum_{t=1}^T \\nabla_\\theta \\log \\pi(a \\mid s_t^t) A(s_t,a_t)\\) | \\(A(s_t,a_t)\\) = \\(Q(s_t,a_t) - V(s_t)\\) | . where we use an Actor-Critic method which contains an actor network and critic network (value function). The actor and critic can be assumed to have almost the same architecture until the very end where the actor outputs actions and the critic outputs a value. The actor generates actions on every rollout and is seen in this equation in \\(a\\) and \\(\\pi\\) and the critic determines \\(V(s_t)\\) which will also determine the Q-function \\(Q(s_t,a_t)\\). We define the reward function based on the task we want to perform. If we just observed \\(\\nabla_\\theta \\log \\pi(a \\mid s_t^t) A(s_t,a_t)\\) and set the advantage function to 1, we can see that this is the gradient of the log-likelihood of the actions taken. In other words, our policy gradient method is already taking gradients to maximize the likelihood of actions taken. This is a common method used in generative modeling to match a distribution. Once we add in the advantage function, we are now weighing which action distributions are better than others. In order for RL to train a good policy, we have to be able to use rewards to guide the policy towards actions that we know will help achieve the task. Without a good reward function, we could likelihood update our network towards a bad action distribution and possibly stay in that bad action distribution without hope of recovering. STATE SPACE?: Based on this already, we can see that RL has a similar problem to control theory. How do you go about defining a reward function if there is no clear notion of state? . | Actually, you can normally simulate piles of objects and liquids in simulation, so you can define a reward function saying if a pile is in a reasonable configuration. | However, this assumes that you can actually simulate at all. | Based on this, simulation already requires you to define some sort of state for these weird objects, so we use that to define a reward function. | . ",
    "url": "/blog/deep-dive-notes/tactile-rl-notes/roadmap-rl.html#4-rl-approach-simulation",
    
    "relUrl": "/deep-dive-notes/tactile-rl-notes/roadmap-rl.html#4-rl-approach-simulation"
  },"333": {
    "doc": "Roadmap to RL",
    "title": "Roadmap to RL",
    "content": " ",
    "url": "/blog/deep-dive-notes/tactile-rl-notes/roadmap-rl.html",
    
    "relUrl": "/deep-dive-notes/tactile-rl-notes/roadmap-rl.html"
  },"334": {
    "doc": "Roadmap to Tactile RL",
    "title": "Roadmap to Tactile",
    "content": "Now that we have some proper motivation on why to do reinforcement learning for manipulation, we should now motivate the use of tactile and its integration with RL. We first define what tactile is. Tactile sensing is loosely spoken about the ability to sense contact and contact forces through a sensor. Normally, we can infer contact location, contact forces, and shear forces. Being able to sense contact location and forces is important for manipulation tasks that involve a lot of forceful reasoning. Picking up objects and placing them is not really a forceful task. Tasks like inserting a key into a lock or screwing a bottle cap on a bottle are forceful tasks. While you could argue that you can do these tasks with vision, you won’t have the same level of precision or robustness. When only doing vision, you really have to r ely on the precision of your robot/controller and have to work in high occlusion (e.g., hand occluding the object). Tactile can help with this by extending the sensing area to the fingers and providing direct contact information to something that is occluded. There is a variety of tactile sensors being developed. There are currently force-torque sensors, tactile skin sensors (force arrays), and high-resolution visuotactile sensors (e.g., Gelsight, DIGIT, etc.). Force-torque sensors: . | Pros: . | Very accurate and reliable force/torque measurements | . | Cons: . | Only measures at a single point. | Can’t get contact location directly. | . | . Visuotactile sensors: . | Pros: . | We can use current vision tech to process tactile images to extract cues. | Very high dimension (definitely not losing out on information). | . | Cons: . | Sim2real is hard. | Requires much more processing power. | . | . The others, I’m not too familiar with. ",
    "url": "/blog/deep-dive-notes/tactile-rl-notes/roadmap-tactile-rl.html#roadmap-to-tactile",
    
    "relUrl": "/deep-dive-notes/tactile-rl-notes/roadmap-tactile-rl.html#roadmap-to-tactile"
  },"335": {
    "doc": "Roadmap to Tactile RL",
    "title": "Tactile BC",
    "content": "There have been works that use tactile sensing for behavior cloning. It is usually just another modality that is processed through MLP or CNN and then fused with other modalities at the embedding level. There have been successes, but there is not much literature on how well this can scale. Everyone has different sensors, so it is hard to collect huge amounts of data to try. ",
    "url": "/blog/deep-dive-notes/tactile-rl-notes/roadmap-tactile-rl.html#tactile-bc",
    
    "relUrl": "/deep-dive-notes/tactile-rl-notes/roadmap-tactile-rl.html#tactile-bc"
  },"336": {
    "doc": "Roadmap to Tactile RL",
    "title": "Tactile RL",
    "content": "Before going into Tactile RL, these are the overarching challenges of RL for manipulation: . | Sample efficiency: RL is notoriously sample inefficient especially for robotics. | Sim2real: How to transfer policies learned in simulation to the real world. | State representation: How to represent the state of the world for manipulation tasks in simulation. | . These are the general questions for Tactile Sensing (namely visuotactile): . | How to simulate visuotactile sensors? | How to sim2real visuotactile sensors? | . For our current interests, we want to sim2real tactile sensors. Previously, we had discussed tactile resolution and how it is very high dimensional. One way of decreasing this is through the use of shear fields. The question then becomes how much resolution do we want. | My question is how relevant this question is to answer… I don’t think it’s relevant at all. | . ",
    "url": "/blog/deep-dive-notes/tactile-rl-notes/roadmap-tactile-rl.html#tactile-rl",
    
    "relUrl": "/deep-dive-notes/tactile-rl-notes/roadmap-tactile-rl.html#tactile-rl"
  },"337": {
    "doc": "Roadmap to Tactile RL",
    "title": "Roadmap to Tactile RL",
    "content": " ",
    "url": "/blog/deep-dive-notes/tactile-rl-notes/roadmap-tactile-rl.html",
    
    "relUrl": "/deep-dive-notes/tactile-rl-notes/roadmap-tactile-rl.html"
  },"338": {
    "doc": "Robomimic",
    "title": "Diffusion Policy Lift Task",
    "content": "Diffusion Policy Codebase . Based on lift_image.yaml and it states that dataset comes from the robomimic dataset . ",
    "url": "/blog/code-notes/benchmark-notes/robomimic.html#diffusion-policy-lift-task",
    
    "relUrl": "/code-notes/benchmark-notes/robomimic.html#diffusion-policy-lift-task"
  },"339": {
    "doc": "Robomimic",
    "title": "Robomimic",
    "content": "Robomimic Codebase . After taking a look at RoboMimic, it seems that understanding this paper is essential for understanding how to do BC on a robot. I would recommend anyone getting into BC to read this paper deeply. Pre-Code Dive . Paper link . Based on README.md 3/11/2025 robomimic datasets move to HuggingFace. IMPORTANT from Robomimic, we see that randomizing object in ADDITION to randomizing robot state (by a little) matters in getting robust performance. IMPORTANT from Robomimic, we see that BC-RNN performs better by a large margin against just BC (concatenate horizon of observations as features). Figure 1: (MG) is machine generated. (PH) is proficient human. (MH) is multi-human. We can see that lift does really well in general. It is perfect for HUMAN demonstrations. However, it is much worse for machine-generated (MG). Either way, we should expect high performance in our task. The observation that (PH) and (MH) does better than (MG) is actually stated in the paper in their C1 remark. These (C#) remarks are claims they wish to validate. (C1) Data from Non-Markovian Decision Process. Human demonstrations can differ substantially from machine-generated demonstrations because humans may not act purely based on a single current observation. External factors (teleoperation device, past actions, history of episode) may all play a role. Prior work [20] has noted substantial benefits from leveraging models that are history-dependent and / or with temporal abstraction to learn from human demonstrations. We investigate various design choices related to such architectures in this study. This would also mean that for (MG) demos, using BC-RNN might not work. (C3) Dependence on Dataset Size. Offline policy learning is sensitive to the state and action space coverage in the dataset, and by extension, the size of the dataset itself. In our study, we investigate how dataset sizes affect policy performance. This analysis is useful to understand the value of adding more data – an important consideration since collecting human demonstrations can be costly. (C4) Mismatch between Training and Evaluation Objectives. Unlike traditional supervised learning, where model selection can be achieved by using the model with the lowest validation loss [21], offline policy learning often suffers from the fact that the training objective is only a surrogate for the true objective of interest (e.g. task success rate), and policy performance can change significantly from epoch to epoch. This makes it difficult to select the best trained model [19, 28, 29]. In our study, we evaluate each policy checkpoint online in the environment in simulation, and report the best policy success rate per training run. We use these ground-truth values to understand the effectiveness of different selection criteria, and confirm that offline policy selection is an important problem, especially in real-world scenarios where large-scale empirical evaluation is difficult. IMPORTANT BIG NOTE (MG) is generated by running a SOTA RL Policy (from 2021) on these tasks. These literally take in one observation horizon and output an action. We should not trust the (MG) results on scripted demos. These figures don’t need much explaning. It just shows how different hyperparameter settings can change the policy’s performance (mostly for the worse here). Now we move into the appendix section of the paper because that is where the juicy pre-code details are. Appendix Info . Policy outputs actions at a rate of 20Hz. Action space is in delta-pose form. Delta-rotation is axis-angle. Lift. Object observations (10-dim) consist of the absolute cube position and cube quaternion (7-dim), and the cube position relative to the robot end effector (3-dim). The cube pose is randomized at the start of each episode with a random z-rotation in a small square region at the center of the table. Simulation Note: All tasks were designed using MuJoCo and the robosuite framework. All of the simulation code is actually from robosuite codebase which wraps MuJoCo. The action horizon was 1 by the way. ",
    "url": "/blog/code-notes/benchmark-notes/robomimic.html",
    
    "relUrl": "/code-notes/benchmark-notes/robomimic.html"
  },"340": {
    "doc": "Robomimic",
    "title": "Robomimic Code Dive",
    "content": "We start in policy_nets.py . Policy Network . Our desired BC network implementation is written in ActorNetwork class. This is the inheritance graph for the class: . flowchart LR A(ActorNetwork) --&gt; B(MIMO_MLP) --&gt; C(Module) --&gt; D(torch.nn.Module) . Here is my notes of the implementation of ActorNetwork where I remove some code and comments for clarity. class ActorNetwork(MIMO_MLP): \"\"\" A basic policy network that predicts actions from observations. Can optionally be goal conditioned on future observations. \"\"\" def __init__(self, obs_shapes, ac_dim, mlp_layer_dims, goal_shapes=None, encoder_kwargs=None): ''' AN NOTE: obs_shapes: OrderedDict(string, OrderedDict(string, tuple(int))) - string: observation key ('obs' or 'goal') - OrderedDict(string, tuple(int)) - string: observation name ('object', 'eef_quat', 'eef_pos') - tuple(int): shape of observation (e.g. (3,), (7,), (10,)) ac_dim: (int) action dim mlp_layer_dims ([int]): specify mlp layers ''' assert isinstance(obs_shapes, OrderedDict) self.obs_shapes = obs_shapes self.ac_dim = ac_dim observation_group_shapes = OrderedDict() observation_group_shapes[\"obs\"] = OrderedDict(self.obs_shapes) self._is_goal_conditioned = False if goal_shapes is not None and len(goal_shapes) &gt; 0: assert isinstance(goal_shapes, OrderedDict) self._is_goal_conditioned = True self.goal_shapes = OrderedDict(goal_shapes) observation_group_shapes[\"goal\"] = OrderedDict(self.goal_shapes) else: self.goal_shapes = OrderedDict() output_shapes = self._get_output_shapes() super(ActorNetwork, self).__init__( input_obs_group_shapes=observation_group_shapes, output_shapes=output_shapes, layer_dims=mlp_layer_dims, encoder_kwargs=encoder_kwargs, ) def _get_output_shapes(self): return OrderedDict(action=(self.ac_dim,)) #AN NOTE: only output is action dim (horizon=1) def forward(self, obs_dict, goal_dict=None): # AN NOTE: actions calls forward from MIMO_MLP actions = super(ActorNetwork, self).forward(obs=obs_dict, goal=goal_dict)[\"action\"] # apply tanh squashing to ensure actions are in [-1, 1] return torch.tanh(actions) #AN NOTE: why does it run tanh here? . For the most part, everything makes sense (and maybe too much is abstracted.) . I think I don’t quite get why torch.tanh(actions) is such a good idea. While it squashes actions down to [-1, 1], it ‘s a nonlinear mapping. I guess if the network can learn anything, the tanh just serves to restrict the action space. Now we move to the implementation of MIMO_MLP . class MIMO_MLP(Module): \"\"\" Extension to MLP to accept multiple observation dictionaries as input and to output dictionaries of tensors. Inputs are specified as a dictionary of observation dictionaries, with each key corresponding to an observation group. This module utilizes @ObservationGroupEncoder to process the multiple input dictionaries and @ObservationDecoder to generate tensor dictionaries. The default behavior for encoding the inputs is to process visual inputs with a learned CNN and concatenating the flat encodings with the other flat inputs. The default behavior for generating outputs is to use a linear layer branch to produce each modality separately (including visual outputs). \"\"\" def __init__( self, input_obs_group_shapes, output_shapes, layer_dims, layer_func=nn.Linear, activation=nn.ReLU, encoder_kwargs=None, ): \"\"\" Args: input_obs_group_shapes (OrderedDict): a dictionary of dictionaries. Each key in this dictionary should specify an observation group, and the value should be an OrderedDict that maps modalities to expected shapes. output_shapes (OrderedDict): a dictionary that maps modality to expected shapes for outputs. layer_dims ([int]): sequence of integers for the MLP hidden layer sizes layer_func: mapping per MLP layer - defaults to Linear activation: non-linearity per MLP layer - defaults to ReLU \"\"\" super(MIMO_MLP, self).__init__() self.input_obs_group_shapes = input_obs_group_shapes self.output_shapes = output_shapes self.nets = nn.ModuleDict() # Encoder for all observation groups. self.nets[\"encoder\"] = ObservationGroupEncoder( observation_group_shapes=input_obs_group_shapes, encoder_kwargs=encoder_kwargs, ) # flat encoder output dimension mlp_input_dim = self.nets[\"encoder\"].output_shape()[0] # intermediate MLP layers self.nets[\"mlp\"] = MLP( input_dim=mlp_input_dim, output_dim=layer_dims[-1], layer_dims=layer_dims[:-1], layer_func=layer_func, activation=activation, output_activation=activation, # make sure non-linearity is applied before decoder ) # decoder for output modalities self.nets[\"decoder\"] = ObservationDecoder( decode_shapes=self.output_shapes, input_feat_dim=layer_dims[-1], ) def output_shape(self, input_shape=None): return { k : list(self.output_shapes[k]) for k in self.output_shapes } def forward(self, **inputs): enc_outputs = self.nets[\"encoder\"](**inputs) mlp_out = self.nets[\"mlp\"](enc_outputs) return self.nets[\"decoder\"](mlp_out) . Based on this code, the BC network looks like this: . flowchart TD A(eef) --&gt; D[\\Encoder Head/] B(gripper) --&gt; E[\\Encoder Head/] C(object) --&gt; F[\\Encoder Head/] D --&gt; G[Concat] E --&gt; G[Concat] F --&gt; G[Concat] G --&gt; H[MLP with ReLU] H --&gt; I[/Decoder Head\\] I --&gt; J[Action] . For the decode head it runs an nn.Linear from the output of the MLP to the action space. Not going to include ObservationDecoder because of my summary. Actualy it could just be concatenation for state-based BC. It only runs encoder head for RGB observations (see create_obs_encoder ). NOTE The difference between this and mine is that I concatenate the observations beforehand (no encoder heads). Training/Sim Details (Action Space) . I think we dug enough into how the ActorNetwork is implemented for us. Now we want to see how it is used in the training loop (and inference time). This is the loss function computed in the BC &lt;/b&gt; class which uses ActorNetwork . def _compute_losses(self, predictions, batch): \"\"\" Internal helper function for BC algo class. Compute losses based on network outputs in @predictions dict, using reference labels in @batch. Args: predictions (dict): dictionary containing network outputs, from @_forward_training batch (dict): dictionary with torch.Tensors sampled from a data loader and filtered by @process_batch_for_training Returns: losses (dict): dictionary of losses computed over the batch \"\"\" losses = OrderedDict() a_target = batch[\"actions\"] actions = predictions[\"actions\"] losses[\"l2_loss\"] = nn.MSELoss()(actions, a_target) losses[\"l1_loss\"] = nn.SmoothL1Loss()(actions, a_target) # cosine direction loss on eef delta position losses[\"cos_loss\"] = LossUtils.cosine_loss(actions[..., :3], a_target[..., :3]) action_losses = [ self.algo_config.loss.l2_weight * losses[\"l2_loss\"], self.algo_config.loss.l1_weight * losses[\"l1_loss\"], self.algo_config.loss.cos_weight * losses[\"cos_loss\"], ] action_loss = sum(action_losses) losses[\"action_loss\"] = action_loss return losses . As we can see, the loss function is as follows: . \\[\\begin{equation} \\mathcal{L} = \\lambda_1 \\mathcal{L}_{l2} + \\lambda_2 \\mathcal{L}_{l1} + \\lambda_3 \\mathcal{L}_{cos} \\end{equation}\\] where \\(\\mathcal{L}_{cos}\\) is cosine similarity loss. This is added on top of the MSE (I am only using this) and L1 loss. If we tried to figure out how the neural network action is used, we won’t find it in this codebase (sadly). It is all abstracted away in a .step() function within run_rollout() which is where the BC class is deployed. The notion of action is just a tensor that is passed around. In order to understand how the action is used, we need to dig into robosuite, to see how it handles its action space! . ",
    "url": "/blog/code-notes/benchmark-notes/robomimic.html#robomimic-code-dive",
    
    "relUrl": "/code-notes/benchmark-notes/robomimic.html#robomimic-code-dive"
  },"341": {
    "doc": "Robomimic",
    "title": "Robosuite Code Dive",
    "content": "Robosuite Codebase . It seems that robosuite has a Lift Task! It is actually a manipulation environment that we can take a look at. NOTE if we want to just use someone else’s environment, keep note that robosuite is the codebase that defines all the environments we are benchmarking against. It is a wrapper around MuJoCo. This is the inheritance graph for Lift class: . flowchart LR A(Lift) --&gt; B(ManipulationEnv) --&gt; C(RobotEnv) --&gt; D(MujocoEnv) . It seems all of the details of how action is handled is within the RobotEnv class. def _pre_action(self, action, policy_step=False): \"\"\" Overrides the superclass method to control the robot(s) within this enviornment using their respective controllers using the passed actions and gripper control. Args: action (np.array): The control to apply to the robot(s). Note that this should be a flat 1D array that encompasses all actions to be distributed to each robot if there are multiple. For each section of the action space assigned to a single robot, the first @self.robots[i].controller.control_dim dimensions should be the desired controller actions and if the robot has a gripper, the next @self.robots[i].gripper.dof dimensions should be actuation controls for the gripper. policy_step (bool): Whether a new policy step (action) is being taken Raises: AssertionError: [Invalid action dimension] \"\"\" # Verify that the action is the correct dimension assert len(action) == self.action_dim, \"environment got invalid action dimension -- expected {}, got {}\".format( self.action_dim, len(action) ) # Update robot joints based on controller actions cutoff = 0 for idx, robot in enumerate(self.robots): robot_action = action[cutoff : cutoff + robot.action_dim] robot.control(robot_action, policy_step=policy_step) cutoff += robot.action_dim . I don’t know what this pre-action is but it seems that it could be overriding the controller???? and just setting the robot position. According to robosuite/environments/base.py line 483 in the definition of _pre_action : . def _pre_action(self, action, policy_step=False): \"\"\" Do any preprocessing before taking an action. Args: action (np.array): Action to execute within the environment policy_step (bool): Whether this current loop is an actual policy step or internal sim update step \"\"\" self.sim.data.ctrl[:] = action . which means that this code does get run and will change the robot! . Note that the robot state change comes from robot.control , so we can investigate that. We find an implementation of control in robosuite/robots/fixed_base_robots.py . def control(self, action, policy_step=False): \"\"\" Actuate the robot with the passed joint velocities and gripper control. Args: action (np.array): The control to apply to the robot. The first @self.robot_model.dof dimensions should be the desired normalized joint velocities and if the robot has a gripper, the next @self.gripper.dof dimensions should be actuation controls for the gripper. :NOTE: Assumes inputted actions are of form: [right_arm_control, right_gripper_control, left_arm_control, left_gripper_control] policy_step (bool): Whether a new policy step (action) is being taken \"\"\" # clip actions into valid range assert len(action) == self.action_dim, \"environment got invalid action dimension -- expected {}, got {}\".format( self.action_dim, len(action) ) self.composite_controller.update_state() if policy_step: self.composite_controller.set_goal(action) applied_action_dict = self.composite_controller.run_controller(self._enabled_parts) for part_name, applied_action in applied_action_dict.items(): applied_action_low = self.sim.model.actuator_ctrlrange[self._ref_actuators_indexes_dict[part_name], 0] applied_action_high = self.sim.model.actuator_ctrlrange[self._ref_actuators_indexes_dict[part_name], 1] applied_action = np.clip(applied_action, applied_action_low, applied_action_high) self.sim.data.ctrl[self._ref_actuators_indexes_dict[part_name]] = applied_action if policy_step: # Update proprioceptive values self.recent_qpos.push(self._joint_positions) self.recent_actions.push(action) self.recent_torques.push(self.torques) for arm in self.arms: controller = self.part_controllers[arm] # Update arm-specific proprioceptive values self.recent_ee_forcetorques[arm].push(np.concatenate((self.ee_force[arm], self.ee_torque[arm]))) self.recent_ee_pose[arm].push(np.concatenate((controller.ref_pos, T.mat2quat(controller.ref_ori_mat)))) self.recent_ee_vel[arm].push(np.concatenate((controller.ref_pos_vel, controller.ref_ori_vel))) # Estimation of eef acceleration (averaged derivative of recent velocities) self.recent_ee_vel_buffer[arm].push(np.concatenate((controller.ref_pos_vel, controller.ref_ori_vel))) diffs = np.vstack( [ self.recent_ee_acc[arm].current, self.control_freq * np.diff(self.recent_ee_vel_buffer[arm].buf, axis=0), ] ) ee_acc = np.array([np.convolve(col, np.ones(10) / 10.0, mode=\"valid\")[0] for col in diffs.transpose()]) self.recent_ee_acc[arm].push(ee_acc) . We see that this actually runs a controller (most likely OSC) and then gives out an “action” which is actually just torques to apply on the robot. So in reality, the _pre_action is the action code. Now, we can take a look at the controller code. This is located in robosuite/controllers/parts/arm/osc.py . def set_goal(self, action): \"\"\" Sets goal based on input @action. If self.impedance_mode is not \"fixed\", then the input will be parsed into the delta values to update the goal position / pose and the kp and/or damping_ratio values to be immediately updated internally before executing the proceeding control loop. Note that @action expected to be in the following format, based on impedance mode! :Mode `'fixed'`: [joint pos command] :Mode `'variable'`: [damping_ratio values, kp values, joint pos command] :Mode `'variable_kp'`: [kp values, joint pos command] Args: action (Iterable): Desired relative joint position goal state \"\"\" # Update state self.update() # Parse action based on the impedance mode, and update kp / kd as necessary if self.impedance_mode == \"variable\": damping_ratio, kp, delta = action[:6], action[6:12], action[12:] self.kp = np.clip(kp, self.kp_min, self.kp_max) self.kd = 2 * np.sqrt(self.kp) * np.clip(damping_ratio, self.damping_ratio_min, self.damping_ratio_max) elif self.impedance_mode == \"variable_kp\": kp, delta = action[:6], action[6:] self.kp = np.clip(kp, self.kp_min, self.kp_max) self.kd = 2 * np.sqrt(self.kp) # critically damped else: # This is case \"fixed\" delta = action # If we're using deltas, interpret actions as such if self.input_type == \"delta\": scaled_delta = self.scale_action(delta) self.goal_pos = self.compute_goal_pos(scaled_delta[0:3]) if self.use_ori is True: self.goal_ori = self.compute_goal_ori(scaled_delta[3:6]) else: self.goal_ori = self.compute_goal_ori(np.zeros(3)) # Else, interpret actions as absolute values elif self.input_type == \"absolute\": self.goal_pos = action[0:3] if self.use_ori is True: self.goal_ori = Rotation.from_rotvec(action[3:6]).as_matrix() else: self.goal_ori = self.compute_goal_ori(np.zeros(3)) else: raise ValueError(f\"Unsupport input_type {self.input_type}\") ... # AN CUTOFF . We can see that when input_type == “delta” , it computes the new rotation matrix given a delta rather than just wrap a rotation instance around the action. If we really want to get specific, we can look at self.compute_goal_ori . def compute_goal_ori(self, delta, goal_update_mode=None): \"\"\" Compute new goal orientation, given a delta to update. Can either update the new goal based on current achieved position or current deisred goal. Updating based on current deisred goal can be useful if we want the robot to adhere with a sequence of target poses as closely as possible, without lagging or overshooting. Args: delta (np.array): Desired relative change in orientation, in axis-angle form [ax, ay, az] goal_update_mode (str): either \"achieved\" (achieved position) or \"desired\" (desired goal) Returns: np.array: updated goal orientation in the controller frame \"\"\" if goal_update_mode is None: goal_update_mode = self._goal_update_mode assert goal_update_mode in [\"achieved\", \"desired\"] if self.goal_ori is None: # if goal is not already set, set it to current orientation (in controller ref frame) if self.input_ref_frame == \"base\": self.goal_ori = self.goal_origin_to_eef_pose()[:3, :3] elif self.input_ref_frame == \"world\": self.goal_ori = self.ref_ori_mat else: raise ValueError # convert axis-angle value to rotation matrix quat_error = T.axisangle2quat(delta) #AN NOTE: this is the moneymaker rotation_mat_error = T.quat2mat(quat_error) if self._goal_update_mode == \"desired\": # update new goal wrt current desired goal goal_ori = np.dot(rotation_mat_error, self.goal_ori) elif self._goal_update_mode == \"achieved\": # update new goal wrt current achieved orientation if self.input_ref_frame == \"base\": curr_goal_ori = self.goal_origin_to_eef_pose()[:3, :3] elif self.input_ref_frame == \"world\": curr_goal_ori = self.ref_ori_mat else: raise ValueError goal_ori = np.dot(rotation_mat_error, curr_goal_ori) #AN NOTE: this is the moneymaker else: raise ValueError # AN REDACTED return goal_ori . We can see in the definition of goal_ori how delta is utilized. Additionally, we know that the delta is angleaxis representation in the instantiation of quat_error where it uses T.axisangle2quat(delta) to convert it from axisangle to a quaternion. This confirms that the action space is in delta-poses and uses axis-angle for rotation representation. Checking out Robomimic’s dataset . Next implementation detail that we need to understand is how the dataset is recorded. ",
    "url": "/blog/code-notes/benchmark-notes/robomimic.html#robosuite-code-dive",
    
    "relUrl": "/code-notes/benchmark-notes/robomimic.html#robosuite-code-dive"
  },"342": {
    "doc": "Robopack",
    "title": "Robopack",
    "content": "Link: Robopack . Yunzhu’s work on dense insertion. We learn the compliance of an object in the dense insertion task by taking multiple demos of us poking the bin objects. We then use this to learn the dynamics of the object and use MPC to insert the object into the bin by deforming the bin objects (pushing away other objects through extrinsic pushing). Extrinsic pushing is when we hold an object and that object pushes another object. ",
    "url": "/blog/project-notes/project1/lit_review/robopack.html",
    
    "relUrl": "/project-notes/project1/lit_review/robopack.html"
  },"343": {
    "doc": "Robopack",
    "title": "Vague Ideas",
    "content": ". | Use recurrent GNN to learn dynamics | Use tactile information to help learn dynamics | Use learned dynamics with sampling-based MPC to solve dense insertion | . ",
    "url": "/blog/project-notes/project1/lit_review/robopack.html#vague-ideas",
    
    "relUrl": "/project-notes/project1/lit_review/robopack.html#vague-ideas"
  },"344": {
    "doc": "Robopack",
    "title": "Specifics",
    "content": ". | What does graph represent?: the nodes of the graph are created by taking in multiple RGBD cameras in the scene and generating an object point cloud (through segmentation) and subsampling them. the edges of the graph are generated in the knn-graph scheme. We track the point clouds over time to ensure consistency foFor computational efficiency, we execute the first K planning steps. While executing the actions, the robot records its tactile readings. After execution, it performs state estimation with the history of observations and re-plans for the next execution. More implementation details on planning can be found in Appendix C. To summarize this section, a diagram of the entire system workflow including training and test-time deployment is available in Figure 10.r the recurrent GNN. We also include end-effector point clouds in the graph to show interaction. | What are tactile observations?: We take the force distribution that can be estimated on the soft-bubble sensors and additionaly take point clouds from soft-bubble (depth sensor). this gives us a tactile signal that we can get embeddings from. | How is compliance modelled?: This is implicitly done through the recurrent GNN and observations. | . In the end, we’re doing a fairly black-box dynamics learning model which uses GNNs. We also have a state-estimator that also uses GNNs. | How can MPC figure out to make space?: A special cost function is created that rewards exploring different starting actions. This is purely done heuristically. With different seeds, you can get different pokes with varying sucess. | . ",
    "url": "/blog/project-notes/project1/lit_review/robopack.html#specifics",
    
    "relUrl": "/project-notes/project1/lit_review/robopack.html#specifics"
  },"345": {
    "doc": "Robopack",
    "title": "Limits",
    "content": "This takes FOREVER! I am pretty sure the dense packing was very hacky because they hard-coded the pokes to be between rows. Although it seems like a decent start since the MPC finds something. ",
    "url": "/blog/project-notes/project1/lit_review/robopack.html#limits",
    
    "relUrl": "/project-notes/project1/lit_review/robopack.html#limits"
  },"346": {
    "doc": "Robopack",
    "title": "Why does GNN work in this case?",
    "content": ". | We get full object point clouds and end-effector point clouds that are all connected via knn-graph scheme. | We get point-wise tactile signals and map those into a latent embedding space. These embeddings are thought of as tactile observations which every node in the graph has as a part of its feature. | . The compliance that we want for dense packing is learned through the data and observation (seeing tracked point clouds not behave like a rigid body). With enough pokes, we can also learn the tactile signals that are involved for poking and inserting successfully. ",
    "url": "/blog/project-notes/project1/lit_review/robopack.html#why-does-gnn-work-in-this-case",
    
    "relUrl": "/project-notes/project1/lit_review/robopack.html#why-does-gnn-work-in-this-case"
  },"347": {
    "doc": "Robopack",
    "title": "Questions related to my project",
    "content": ". | Should we model multiple objects in this scene? | How to model compliance explicitly? | How to integrate extrinsic contact more explicitly and is that even useful? | Why even use RL? | . ",
    "url": "/blog/project-notes/project1/lit_review/robopack.html#questions-related-to-my-project",
    
    "relUrl": "/project-notes/project1/lit_review/robopack.html#questions-related-to-my-project"
  },"348": {
    "doc": "2. Sequential Decisions",
    "title": "2. Sequential Decision Making",
    "content": "Sequential decision making is when an agent must make a horizon or sequence of decisions over time instead of a single decision. ",
    "url": "/blog/class-notes/04-Computational-Sensorimotor-Learning/sequential-decision.html#2-sequential-decision-making",
    
    "relUrl": "/class-notes/04-Computational-Sensorimotor-Learning/sequential-decision.html#2-sequential-decision-making"
  },"349": {
    "doc": "2. Sequential Decisions",
    "title": "2.1 Issues of CB",
    "content": "The thing with CB is that it does not consider the context transitions affected by actions. In control theory, we refer to this as the dynamics of the system. Previously, we just separated the state and action spaces. ",
    "url": "/blog/class-notes/04-Computational-Sensorimotor-Learning/sequential-decision.html#21-issues-of-cb",
    
    "relUrl": "/class-notes/04-Computational-Sensorimotor-Learning/sequential-decision.html#21-issues-of-cb"
  },"350": {
    "doc": "2. Sequential Decisions",
    "title": "2.2 Sequential Decision Process",
    "content": "Note that this is broader than Markov Decision Processes (MDPs) because it does not require the Markov property. | Timestep: \\(t\\) | State: \\(s_t \\in \\mathcal{S}\\) | Action: \\(a_t \\in \\mathcal{A}\\) | Reward: \\(r(s_{1:t}, a_{1:t}) \\in \\mathbb{R}\\) | Transition: \\(\\mathcal{T}(s_{t+1}\\mid s_{1:t,a_{1:t}})\\) non-markov | Policy: \\(\\pi\\) where \\(a_t = \\pi(s_{1:t}, a_{1:t-1})\\) Sometimes we treat \\(\\pi : (\\mathcal{S} \\times \\mathcal{A})^t \\mapsto [0,1]^{\\mid \\mathcal{A} \\mid }\\) | . The objective is to maximize the expected reward over a horizon \\(T\\): . \\[\\begin{equation} \\max_{\\pi} \\mathbb{E}_{s_{1} \\sim P(\\mathcal{S}), a_t \\sim \\pi(s_{1:t}), s_{t+1} \\sim \\mathcal{T}(s' \\mid s_{1:t}, a_{1:t})} [\\sum_{t=1}^T r(s_{1:t}, a_{1:t})] \\end{equation}\\] where \\(P(\\mathcal{S})\\) is the initial state distribution. We refer to sum of rewards as the return. ",
    "url": "/blog/class-notes/04-Computational-Sensorimotor-Learning/sequential-decision.html#22-sequential-decision-process",
    
    "relUrl": "/class-notes/04-Computational-Sensorimotor-Learning/sequential-decision.html#22-sequential-decision-process"
  },"351": {
    "doc": "2. Sequential Decisions",
    "title": "2.3 Finite and Infinite Horizon",
    "content": "We have been thinking about the finite horizon case. For infinite-horizon, it is much trickier. Especially when \\(T \\to \\infty\\) our rewards can diverge. We need to make sure that the rewards are bounded. We can do this by using a discount factor and restate the objective as: . \\[\\begin{equation} \\max_{\\pi} \\mathbb{E} [\\sum_{t=1}^\\infty \\gamma^{t-1} r(s_t, a_t)] \\end{equation}\\] where \\(\\gamma\\) denotes the discount factor. We want it to be between 0 and 1. This is a common trick in reinforcement learning. This is definitely a heuristic on how far we should care about the future. If \\(\\gamma \\to 0^{+}\\), we only care about the immediate reward. If \\(\\gamma \\to 1^{-}\\), we care about all future rewards equally. ",
    "url": "/blog/class-notes/04-Computational-Sensorimotor-Learning/sequential-decision.html#23-finite-and-infinite-horizon",
    
    "relUrl": "/class-notes/04-Computational-Sensorimotor-Learning/sequential-decision.html#23-finite-and-infinite-horizon"
  },"352": {
    "doc": "2. Sequential Decisions",
    "title": "2.4 Trajectory and Episode",
    "content": "Rolling out means to execute a policy in the environment. What you get from rolling out is a rollout or trajectory (rollout for T steps). Process of sampling a single trajectory is an episode . This is a trajectory (episode): . \\[\\begin{equation} \\tau_{1:T}^i := (s_1^i, a_1^i, r_1^i, s_2^i, a_2^i, r_2^i, \\ldots, s_T^i, a_T^i, r_T^i) \\end{equation}\\] where \\(i\\) is the index of the episode. ",
    "url": "/blog/class-notes/04-Computational-Sensorimotor-Learning/sequential-decision.html#24-trajectory-and-episode",
    
    "relUrl": "/class-notes/04-Computational-Sensorimotor-Learning/sequential-decision.html#24-trajectory-and-episode"
  },"353": {
    "doc": "2. Sequential Decisions",
    "title": "2. Sequential Decisions",
    "content": " ",
    "url": "/blog/class-notes/04-Computational-Sensorimotor-Learning/sequential-decision.html",
    
    "relUrl": "/class-notes/04-Computational-Sensorimotor-Learning/sequential-decision.html"
  },"354": {
    "doc": "Isaac Lab Setup",
    "title": "Isaac Sim Install",
    "content": " ",
    "url": "/blog/code-notes/isaaclab/setup.html#isaac-sim-install",
    
    "relUrl": "/code-notes/isaaclab/setup.html#isaac-sim-install"
  },"355": {
    "doc": "Isaac Lab Setup",
    "title": "Pip Install",
    "content": "conda create -n env_isaaclab python=3.10 conda activate env_isaaclab pip install --upgrade pip pip install torch==2.7.0 torchvision==0.22.0 --index-url https://download.pytorch.org/whl/cu128 pip install \"isaacsim[all,extscache]==4.5.0\" --extra-index-url https://pypi.nvidia.com pip install isaaclab[isaacsim,all]==2.1.0 --extra-index-url https://pypi.nvidia.com pip install git+https://github.com/isaac-sim/rl_games.git pip install isaaclab==2.1.0 --extra-index-url https://pypi.nvidia.com . Test if it works with isaacsim. Make sure torch-scatter matches pytorch version . get rsl-rl-lib and skrl . ",
    "url": "/blog/code-notes/isaaclab/setup.html#pip-install",
    
    "relUrl": "/code-notes/isaaclab/setup.html#pip-install"
  },"356": {
    "doc": "Isaac Lab Setup",
    "title": "Isaac Lab Setup",
    "content": " ",
    "url": "/blog/code-notes/isaaclab/setup.html",
    
    "relUrl": "/code-notes/isaaclab/setup.html"
  },"357": {
    "doc": "Isaac Setup",
    "title": "Setting up Isaac",
    "content": "We are starting off from IsaacGymEnvs which uses IsaacGym 4. This IsaacGym only supports Ubuntu 20.04 and Ubuntu 18.04. In order to set this up on my machine, I will have to use Docker. Check out this link for getting into IsaacGym . ",
    "url": "/blog/code-notes/isaac/setup.html#setting-up-isaac",
    
    "relUrl": "/code-notes/isaac/setup.html#setting-up-isaac"
  },"358": {
    "doc": "Isaac Setup",
    "title": "Docker Setup on Desktop",
    "content": "Check out these tutorials: . | Install | PostInstall | . ",
    "url": "/blog/code-notes/isaac/setup.html#docker-setup-on-desktop",
    
    "relUrl": "/code-notes/isaac/setup.html#docker-setup-on-desktop"
  },"359": {
    "doc": "Isaac Setup",
    "title": "Get NVIDIA Container Toolkit",
    "content": "Check this out for setting up an nvidia container for docker. | link | . ",
    "url": "/blog/code-notes/isaac/setup.html#get-nvidia-container-toolkit",
    
    "relUrl": "/code-notes/isaac/setup.html#get-nvidia-container-toolkit"
  },"360": {
    "doc": "Isaac Setup",
    "title": "Setup NGC Account",
    "content": "Need to get an NGC account to download the container from this github repo . | link | . Generate API Key from NGC Account . | link | . Current API Key . | password: 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 | Username: $oauthtoken | . Run this command to login . docker login nvcr.io . and enter the username and password when prompted. ",
    "url": "/blog/code-notes/isaac/setup.html#setup-ngc-account",
    
    "relUrl": "/code-notes/isaac/setup.html#setup-ngc-account"
  },"361": {
    "doc": "Isaac Setup",
    "title": "Setup Isaac Gym",
    "content": "Follow instructions from this link. Before running python scripts, put this in your bashrc file: . export LD_LIBRARY_PATH=/home/andang/anaconda3/envs/rlgpu/lib . Link to Isaac Gym Documentation (also displayed in docs folder when you download Isaac Gym) . ",
    "url": "/blog/code-notes/isaac/setup.html#setup-isaac-gym",
    
    "relUrl": "/code-notes/isaac/setup.html#setup-isaac-gym"
  },"362": {
    "doc": "Isaac Setup",
    "title": "Anaconda Management",
    "content": "Using the tutorial from the isaacgym download, you get an rlgpu environment. However, we will run into issues with running train.py in IsaacGymEnvs. We upgrade our pytorch version (originally 1.8.1) as follows: . python3 -m pip install torch==1.13.1 . ",
    "url": "/blog/code-notes/isaac/setup.html#anaconda-management",
    
    "relUrl": "/code-notes/isaac/setup.html#anaconda-management"
  },"363": {
    "doc": "Isaac Setup",
    "title": "Multi-GPU Setup",
    "content": "So IsaacGymEnvs is really terrible when it comes to working with multiple GPUs. Currently in our setup, it only works on the last GPU in our lambda server. I want to change this. A possible solution was proposed in manipgen. First step is to install vulkan device chooser https://github.com/aejsmith/vkdevicechooser. | git clone https://github.com/aejsmith/vkdevicechooser | cd vkdevicechooser | sudo apt-get install libvulkan-dev vulkan-validationlayers-dev | sudo apt-get install meson | meson builddir --prefix=/usr | ninja -C builddir | sudo meson install -C builddir | . add this to .bashrc file . export CUDA_DEVICE_ORDER=\"PCI_BUS_ID\" alias gpu0=\"export CUDA_VISIBLE_DEVICES=0\" alias gpu1=\"export CUDA_VISIBLE_DEVICES=1\" alias gpu2=\"export CUDA_VISIBLE_DEVICES=2\" alias gpu3=\"export CUDA_VISIBLE_DEVICES=3\" alias vulkan0=\"export ENABLE_DEVICE_CHOOSER_LAYER=1 VULKAN_DEVICE_INDEX=0\" alias vulkan1=\"export ENABLE_DEVICE_CHOOSER_LAYER=1 VULKAN_DEVICE_INDEX=1\" alias vulkan2=\"export ENABLE_DEVICE_CHOOSER_LAYER=1 VULKAN_DEVICE_INDEX=2\" alias vulkan3=\"export ENABLE_DEVICE_CHOOSER_LAYER=1 VULKAN_DEVICE_INDEX=3\" export VK_ICD_FILENAMES=/usr/share/vulkan/icd.d/nvidia_icd.json export DISABLE_LAYER_NV_OPTIMUS_1=1 export DISABLE_LAYER_AMD_SWITCHABLE_GRAPHICS_1=1 . Before running your script, run vulkanX and gpuY and test our what X and Y should be to run on the same gpu. use these flags to choose vulkan device index. device indices can be found by running vulkaninfo. ",
    "url": "/blog/code-notes/isaac/setup.html#multi-gpu-setup",
    
    "relUrl": "/code-notes/isaac/setup.html#multi-gpu-setup"
  },"364": {
    "doc": "Isaac Setup",
    "title": "Jayjun’s !Happiness",
    "content": "Branch overview: . | Main (vitascope) . | 8 camera support | texture support | point cloud support | +2 cameras support for tactile sensor | multiple environemnts semi-support | waypoint following (follow \\(\\text{SE}(3)\\) pose) | . | OG (rl + insertion) . | 3 cameras rgbd | 2 tactile cameras | shear + rgb | rl_games library for (RL training) | . | . Start off with these files in OG Branch: . | Run teacher/run_train_state.sh (to get something running) | distill_expert_to_tactile_rgb_with_delta_ee_to_socket.py | distill_expert_to_tactile_shear_with_delta_ee_to_socket.py | . Dig into this file: . | tacsl_sensors folder | . Vitascope Library . | vitascope/vitascope/sim | getting tactile point clouds | segmentation on normal cameras | . TacSL can’t get good FT from wrist . NOTE: If you want to render, make sure you set force_render to True in your train.py. Here is a sample command to run: . python3 train.py task=Cartpole headless=False force_render=True . ",
    "url": "/blog/code-notes/isaac/setup.html#jayjuns-happiness",
    
    "relUrl": "/code-notes/isaac/setup.html#jayjuns-happiness"
  },"365": {
    "doc": "Isaac Setup",
    "title": "Isaac Setup",
    "content": " ",
    "url": "/blog/code-notes/isaac/setup.html",
    
    "relUrl": "/code-notes/isaac/setup.html"
  },"366": {
    "doc": "Setup FRI",
    "title": "Setup FRI",
    "content": " ",
    "url": "/blog/kuka-notes/setup.html",
    
    "relUrl": "/kuka-notes/setup.html"
  },"367": {
    "doc": "Setup FRI",
    "title": "Hardware Setup",
    "content": "In our current lab setup, we are using a different backend for the Sunrise. It involves running the LCMRobotInterface program. This communicates through the lab network (through a router) to the computer (which runs the main programs). However, FRI tries to cut through the need to work on the lab network completely. It allows for direct communication with the Kuka Sunrise controller over Ethernet. In fact! If you try to setup your network connection on FRI to communicate over the lab network, it will not run at all because the packet rate is too slow. Here is a diagram of the setup I am referring to: . On the control box for the Kuka, here are the relevant ethernet ports I discussed: . TERMINOLOGY: KONI is the ethernet port that FRI uses to communicate over. It stands for Kuka Optional Network Interface. Connect the KONI port to your computer’s ethernet port. That’s all for hardware setup! . ",
    "url": "/blog/kuka-notes/setup.html#hardware-setup",
    
    "relUrl": "/kuka-notes/setup.html#hardware-setup"
  },"368": {
    "doc": "Setup FRI",
    "title": "Computer-side Setup",
    "content": "You should be able to see the network connection on your computer. Now you need to setup your connection! Your network page on the IPv4 tab should look like this: . Now, we just change the “IPv4 Method” to “Manual” and fill in the “Addresses” as follows: . Choose any IP address you want starting with 192.X.X.X. NOTE: When you are doing multiple robots, you need to keep the middle two numbers the same ALSO. That is because the Kuka Sunrise does some network masking and only considers the last number of the IP address when differentiating between robots. TLDR: you have 192.Y.Y.X, if you use multiple robots, you need to keep Y.Y the consistent across all robots. How to test if your setup is correct? You can ping the KONI-side IP address of the Kuka Sunrise controller. $ ping 192.170.10.2 PING 192.170.10.2 (192.170.10.2) 56(84) bytes of data. 64 bytes from 192.170.10.2: icmp_seq=1 ttl=255 time=0.141 ms 64 bytes from 192.170.10.2: icmp_seq=1 ttl=64 time=0.295 ms (DUP!) . ",
    "url": "/blog/kuka-notes/setup.html#computer-side-setup",
    
    "relUrl": "/kuka-notes/setup.html#computer-side-setup"
  },"369": {
    "doc": "Setup FRI",
    "title": "Kuka Sunrise Setup",
    "content": "This has been done before, and people have much better tutorials than I do. Please refer to this github link on how to flash the Kuka Sunrise Java code to the Kuka Sunrise controller. NOTE: Just make sure any of your modifications is on the same project that the lab uses (has LCMRobotInterface program). Without this, you’ll delete configs and code that other members of the lab use! . ",
    "url": "/blog/kuka-notes/setup.html#kuka-sunrise-setup",
    
    "relUrl": "/kuka-notes/setup.html#kuka-sunrise-setup"
  },"370": {
    "doc": "Setup FRI",
    "title": "FRI Computer-side Network Driver",
    "content": "Please refer to this github link. For some reason the original drake-iiwa-driver repo does not work that well. I made some bazel modifications to get it to work. Additionally, I got it to work with two arms. ",
    "url": "/blog/kuka-notes/setup.html#fri-computer-side-network-driver",
    
    "relUrl": "/kuka-notes/setup.html#fri-computer-side-network-driver"
  },"371": {
    "doc": "IsaacGymEnvs Setup",
    "title": "IsaacGymEnvs Setup",
    "content": "#!/bin/bash cd ~ mkdir isaac cd isaac git clone git@github.com:MMintLab/TactileIsaacGymEnvs.git git clone git@github.com:MMintLab/rl_games.git # scp -r -P 2024 [uniqname]:141.212.84.140:/home/[uniqname]/[path_to_isaac]/IsaacGym_Preview_TacSL_Package/ ./ wget https://drive.google.com/file/d/12Sb5IwyP2YGtlmprybgdepWThHhsCmm7/view tar -xf IsaacGym_Preview_TacSL_Package.tar.gz rm IsaacGym_Preview_TacSL_Package.tar.gz mamba create -n isaac python=3.8 source activate isaac # setup IsaacGym cd IsaacGym_Preview_TacSL_Package/isaacgym/python python3 -m pip install -e . cd ../../.. # setup rl_games cd rl_games python3 -m pip install -e . cd .. # setup IsaacGymEnvs cd TactileIsaacGymEnvs python3 -m pip install -e . mkdir isaacgymenvs/runs git checkout main # install all the right packages python3 -m pip install numpy==1.22.0 python3 -m pip install ray gymnasium scikit-image python3 -m pip install torch_geometric python3 -m pip install pyg_lib torch_scatter torch_sparse torch_cluster torch_spline_conv -f https://data.pyg.org/whl/torch-2.4.1+cu121.html python3 -m pip install moviepy rtree pytorch_volumetric urdfpy yourdfpy termcolor open3d opencv-python . ",
    "url": "/blog/code-notes/isaac/setup_isaacgymenvs.html",
    
    "relUrl": "/code-notes/isaac/setup_isaacgymenvs.html"
  },"372": {
    "doc": "Sim2real Tactile RL",
    "title": "Sim2real Tactile RL",
    "content": "Our main focus is on sim2real tactile RL. That is, training RL policies in simulation with tactile sensing and transferring them to the real world. Simulating tactile rgb: . | Taxim | TacSL . | Technically it comes from: tactilesim | . | FOTS | Sim2shear-Real2Sim2Real | . ",
    "url": "/blog/deep-dive-notes/tactile-rl-notes/sim2real-tactile-rl.html",
    
    "relUrl": "/deep-dive-notes/tactile-rl-notes/sim2real-tactile-rl.html"
  },"373": {
    "doc": "Sim2real Tactile RL",
    "title": "FOTS",
    "content": "\\[\\begin{equation} M_c = M_{\\text{ini}} + \\Delta d_d + \\Delta d_s + \\Delta d_t \\end{equation}\\] . | \\(M_c\\): current marker position | \\(M_{\\text{ini}}\\): initial marker position (without object contact) | \\(\\Delta d_d\\): dilate motion displacement | \\(\\Delta d_s\\): shear motion displacement | \\(\\Delta d_t\\): twist motion displacement | . \\[\\begin{align} \\Delta d_d &amp;= \\sum_{i=1}^N \\Delta h_i \\cdot (M - C_i) \\cdot \\exp(-\\lambda_d \\|M - C_i\\|^2) \\end{align}\\] . | \\(M\\): another way of saying \\(M_{\\text{ini}}\\) | \\(\\Delta h_i\\): height of \\(C_i\\) (from depth map). | \\(C_i\\): markers in contact. | \\(N\\): number of \\(C_i\\). | . \\[\\begin{align} \\Delta d_s &amp;= \\min\\{\\Delta s, \\Delta s_{\\max} \\} \\cdot \\exp(-\\lambda_s \\| M - G \\|^2_2) \\\\ \\Delta d_t &amp;= \\min\\{\\Delta \\theta, \\Delta \\theta_{\\max} \\} \\cdot \\exp(-\\lambda_t \\| M - G \\|^2_2) \\end{align}\\] . | \\(G\\): projection point of object coordinate system origin on gel surface along normal direction. | \\(\\Delta s\\): translation distance of G relative to gel surface. | \\(\\Delta \\theta\\): rotation angle of object coordinate system relative to gel surface. | . We calibrate \\(\\lambda_d, \\lambda_s, \\lambda_t\\) for FOTS. ",
    "url": "/blog/deep-dive-notes/tactile-rl-notes/sim2real-tactile-rl.html#fots",
    
    "relUrl": "/deep-dive-notes/tactile-rl-notes/sim2real-tactile-rl.html#fots"
  },"374": {
    "doc": "Sim2real Tactile RL",
    "title": "FOTS Code Implementation",
    "content": "This is the implementation of marker motion in FOTS. def __init__(....): ... # self.W, self.H are dimensions of image self.x = np.arange(0, self.W, 1) self.y = np.arange(0, self.H, 1) self.xx, self.yy = np.meshgrid(self.y, self.x) def _marker_motion(self): xind = (np.random.random(self.N * self.M) * self.W).astype(np.int16) yind = (np.random.random(self.N * self.M) * self.H).astype(np.int32) x = np.arange(23, 320, 29)[:self.N] y = np.arange(15, 240, 26)[:self.M] xind, yind = np.meshgrid(x, y) xind = (xind.reshape([1, -1])[0]).astype(np.int16) yind = (yind.reshape([1, -1])[0]).astype(np.int16) xx_marker, yy_marker = self.xx[xind, yind].reshape([self.M, self.N]), self.yy[xind, yind].reshape([self.M, self.N]) self.xx,self.yy = xx_marker, yy_marker img = self._generate(xx_marker, yy_marker) xx_marker_, yy_marker_ = self._motion_callback(xx_marker, yy_marker) img = self._generate(xx_marker_, yy_marker_) self.contact = [] def _motion_callback(self,xx,yy): for i in range(self.N): for j in range(self.M): r = int(yy[j, i]) c = int(xx[j, i]) if self.mask[r,c] == 1.0: h = self.depth[r,c]*100 # meter to mm self.contact.append([r,c,h]) if not self.contact: xx,yy = self.xx,self.yy xx_,yy_ = self._dilate(self.lamb[0], xx ,yy) if len(self.traj) &gt;= 2: xx_,yy_ = self._shear(int(self.traj[0][0]*meter2pix + 120), int(self.traj[0][1]*meter2pix + 160), self.lamb[1], int((self.traj[-1][0]-self.traj[0][0])*meter2pix), int((self.traj[-1][1]-self.traj[0][1])*meter2pix), xx_, yy_) theta = max(min(self.traj[-1][2]-self.traj[0][2], 50 / 180.0 * math.pi), -50 / 180.0 * math.pi) xx_,yy_ = self._twist(int(self.traj[-1][0]*meter2pix + 120), int(self.traj[-1][1]*meter2pix + 160), self.lamb[2], theta, xx_, yy_) return xx_,yy_ . ",
    "url": "/blog/deep-dive-notes/tactile-rl-notes/sim2real-tactile-rl.html#fots-code-implementation",
    
    "relUrl": "/deep-dive-notes/tactile-rl-notes/sim2real-tactile-rl.html#fots-code-implementation"
  },"375": {
    "doc": "1. Simple Decisions",
    "title": "1. Simple Decision Making",
    "content": " ",
    "url": "/blog/class-notes/04-Computational-Sensorimotor-Learning/simple-decision.html#1-simple-decision-making",
    
    "relUrl": "/class-notes/04-Computational-Sensorimotor-Learning/simple-decision.html#1-simple-decision-making"
  },"376": {
    "doc": "1. Simple Decisions",
    "title": "1.1 Multi-armed Bandit (MAB)",
    "content": "This is a simple reward-based learning problem where you have to choose one out of K possible actions (discrete). The MAB setup assumes that rewards are independent of the state and history of actions (not like how I’m used to in control). Reward depends on current action: . \\[\\begin{equation} r(a_t) = r(a_{1:t}, s_{1:t}) \\end{equation}\\] The goal is to construct a policy that maximizes reward over horizon of T time steps: . \\[\\begin{equation} \\sum_{t=1}^T r_t(a_t) \\end{equation}\\] ",
    "url": "/blog/class-notes/04-Computational-Sensorimotor-Learning/simple-decision.html#11-multi-armed-bandit-mab",
    
    "relUrl": "/class-notes/04-Computational-Sensorimotor-Learning/simple-decision.html#11-multi-armed-bandit-mab"
  },"377": {
    "doc": "1. Simple Decisions",
    "title": "1.1a Webpage example",
    "content": "Say we want to build a recommendation system for landing webpages. Our goal is to build a recommendation system that displays the landing page that maximizes profit. NOTE: We have no priors on which landing page maximizes profit. Assume the following: . | \\(T\\) - Horizon | \\(t\\) - time step or i-th user visiting a page | \\(E\\) - environment (website) | \\(A\\) - agent (recommendation system) | \\(\\mathcal{A} := \\{1,...,K\\}\\) - action space where \\(K\\) is the number of pages and \\(k \\in [1,K]\\) correspond to the action of recommending i-th page. | \\(a_t\\) - action at round t where \\(a_t \\in \\mathcal{A}, \\forall t \\in [1, T]\\) | \\(r_t(a_t) \\in \\mathbb{R}\\) - reward (random variable) which is profit earned by displaying page. | \\(T_k(t)\\) - action count . number of times that action \\(k\\) has been taken within \\(t\\) rounds. | \\(\\mu_k\\) - True mean reward of taking action \\(k\\). This optimal mean reward is defined as \\(\\mu^* = \\max_k \\mu_k\\). | \\(\\hat \\mu_k(t)\\) - empirical mean reward of taking action \\(k\\), estimated by our algorithm over \\(t\\) rounds. | . \\[\\begin{equation} \\hat \\mu_k(t) := \\frac{1}{T_k(t)} \\sum_{t=1}^T \\mathbb{I}\\{a_t = k\\}r(a_t) \\end{equation}\\] where \\(\\mathbb{I}\\) is the indicator function. This means that when \\(k\\) is chosen, the empirical mean reward is the average of the profit of the pages displayed from \\(1\\) to \\(T\\). We formulate maximizing profit as: . \\[\\begin{equation} \\max_{a_1,...,a_T} \\mathbb{E} [\\sum_{t=1}^T r(a_t)] \\end{equation}\\] Exploration and exploitation is the balance between trying new actions (exploration) and using the best action (exploitation). Definition 1.1 (Exploration). We define any action selection \\(a\\) such that: . \\[\\require{cancel} \\begin{align} \\hat \\mu_a(t) &amp;\\cancel{=} \\max_{k} \\hat \\mu_k(t) \\\\ \\hat \\mu_a(t) &amp;\\cancel{=} \\max_{k} \\frac{1}{T_k(t)} \\sum_{t=1}^T \\mathbb{I}\\{a_t = k \\}r(a_t) \\end{align}\\] as exploration. We are taking actions that are not the actions that maximizes the empirical mean reward. NOTE: this is not the same as the true mean reward. Definition 1.2 (Exploitation). We define any action selection \\(a\\) such that: . \\[\\begin{equation} \\hat \\mu_a(t) = \\max_{k} \\hat \\mu_k(t) \\end{equation}\\] as exploitation. We are taking actions that maximize the empirical mean reward. Our goal is to try to align \\(\\hat \\mu_a\\) and \\(\\mu_a\\), \\(\\forall a \\in \\mathcal{A}\\). To do this, we have to get samples of \\(r(a)\\) to improve accuracy of empirical mean reward (exploration). Performance of an algorithm on balancing exploration and exploitation is measured by regret. In plain language, regret means “loss incurred by past actions”. It characterizes gap between optimal expected reward and expected reward obtained by our algorithm. Regret over \\(T\\) time steps is formally denoted as: . \\[\\begin{equation} \\textbf{Regret}(T) = T\\mu^* - \\mathbb{E}[\\sum_{t=1}^T r(a_t)] \\end{equation}\\] where the expectation is taken over multiple episodes of T rounds. We can design MAB to minimize regret: . \\[\\begin{equation} \\min_{a_1,...,a_T} T\\mu^* - \\mathbb{E}[\\sum_{t=1}^T r(a_t)] \\end{equation}\\] Minimizing regret is equivalent to maximizing rewards. Why use regret then? It actually has better theoretical properties to analyze trade-offs between exploration and exploitation. ",
    "url": "/blog/class-notes/04-Computational-Sensorimotor-Learning/simple-decision.html#11a-webpage-example",
    
    "relUrl": "/class-notes/04-Computational-Sensorimotor-Learning/simple-decision.html#11a-webpage-example"
  },"378": {
    "doc": "1. Simple Decisions",
    "title": "1.1.1 Explore-then-commit (ETC)",
    "content": "Let rewards be deterministic. Optimal strategy is try every action space once and then choose action with highest reward for all time steps in the future. In stochastic settings, we try each action multiple times to get a good estimate of the reward. Assume each action is attempted \\(m\\) times. Afterwards, the agent chooses to execute the optimal action with highest empirical mean reward. This is ETC. There is a distinct explore phase and exploit phase. We spend \\(mK\\) time steps on exploration and \\(T - mK\\) time steps on exploitation. def ETC(t,m): if t &lt;= mK: at = (t % K) + 1 else: at = np.argmax( [muhat(mK, a_k) for a_k in action_space] ) return at . This is the ETC policy. Note that muhat is the empirical mean reward that changes after each call of ETC function during the exploration phase. How good is it? The regret of ETC is bounded by: . \\[\\begin{equation} \\textbf{Regret}(T) \\leq m \\sum_{k=1}^K \\Delta_k + (T - mK) \\sum_{k=1}^K \\Delta_k \\text{exp}(-\\frac{m\\Delta_k^2}{4}) \\end{equation}\\] where \\(\\Delta_k = \\mu^* - \\mu_k\\) is the gap between the optimal action and the action \\(k\\). This regret is split into the exploration and exploitation regret. The first term is the regret incurred during exploration and the second term is the regret incurred during exploitation. So now we can see analyze the regret of the algorithm using big-O notation. Typically \\(m := T^{\\frac{2}{3}}\\), thus: . \\[\\begin{equation} \\textbf{Regret}(T) \\leq \\text{O}(KT^{\\frac{2}{3}}) \\end{equation}\\] This is better than worst regret \\(\\text{O}(T)\\) (linear). Worst regret is obtained if agent chooses suboptimal actions or randomly selects actions through T rounds. ",
    "url": "/blog/class-notes/04-Computational-Sensorimotor-Learning/simple-decision.html#111-explore-then-commit-etc",
    
    "relUrl": "/class-notes/04-Computational-Sensorimotor-Learning/simple-decision.html#111-explore-then-commit-etc"
  },"379": {
    "doc": "1. Simple Decisions",
    "title": "1.1.2 Upper Confidence Bound (UCB)",
    "content": "The ETC exploration is “unguided”. We select actions uniformly at random. If we are already certain about expected reward of an action, we should not explore it. This means we have to model uncertainty for each action to take the most promising action for exploration. Upper Confidence Bound (UCB) constructs uncertainty of actions using confidence bounds of mean reward. It employs “optimism in the face of uncertainty” principle. This principle states that we should overestimate the expected reward of actions that we are uncertain about. We want to construct \\(\\hat \\mu_k'\\) such that \\(\\hat \\mu_k' \\geq \\mu_k\\). Finding such \\(\\hat \\mu_k'\\) can be intractable since we do not know \\(\\mu_k\\). If we assume \\(r(a_t)\\) is drawn from a 1-subgaussian distribution, we can construct \\(\\hat \\mu_k\\) probabilistically. Remember that \\(\\mu_k\\) is the mean reward. Thus the following inequality holds: . \\[\\begin{equation} P(\\mu_k \\geq \\hat \\mu_k + \\epsilon) \\leq \\text{exp}(-\\frac{T_k(t)\\epsilon^2}{2}), \\epsilon \\geq 0 \\end{equation}\\] Equivalently, solving for \\(\\epsilon\\) when equality holds, we transform the inequality to: . \\[\\begin{equation} P(\\hat \\mu_k \\geq \\hat \\mu_k + \\sqrt{\\frac{2 \\log \\frac{1}{\\delta}}{T_k(t)}}) \\leq \\delta, \\delta \\in (0,1) \\end{equation}\\] As a result, \\(\\hat \\mu_k\\) can be constructed by: . \\[\\begin{equation} \\hat \\mu_k' = \\hat \\mu_k + \\sqrt{\\frac{2\\log \\frac{1}{\\delta}}{T_k(t)}} \\end{equation}\\] This \\(\\hat \\mu_k'\\) is the upper confidence bound of the empirical mean reward since \\(\\sqrt{\\frac{2\\log \\frac{1}{\\delta}}{T_k(t)}}\\) is a non-negative term. This means that the upper confidence bound holds with at least \\(1 - \\delta\\) probability. As \\(\\delta\\) increases, the term \\(\\sqrt{\\frac{2\\log \\frac{1}{\\delta}}{T_k(t)}}\\) decreases. This means that the upper bound gap decreases as the likelihood of it holding increases. We refer to \\(delta\\) as the confidence level . def UCB(t): at = np.argmax( [muhat(t, a_k) + sqrt(2*log(1/delta)/T_k(t)) for a_k in action_space] ) return at . We need to make sure that setting \\(\\delta\\) ensures that the second term \\(\\sqrt{\\frac{2\\log \\frac{1}{\\delta}}{T_k(t)}}\\) disappears as \\(t\\) increases. This is the regret analysis of UCB when \\(\\delta = \\frac{1}{t^2}\\).: . \\[\\begin{equation} \\textbf{Regret}(T) \\leq 8(\\sum_{k=2}^K \\frac{\\log T}{\\Delta_k}) + (1 + \\frac{\\pi^3}{3})(\\sum_{k=1}^K \\Delta_k) \\end{equation}\\] where without loss of generality, we let \\(\\mu_1 = \\mu^*\\) for simplicity. The big-O notation of this is: . \\[\\begin{equation} \\textbf{Regret}(T) \\leq \\text{O}(\\sqrt{KT\\log T}) \\end{equation}\\] which is much tighter than ETC. ",
    "url": "/blog/class-notes/04-Computational-Sensorimotor-Learning/simple-decision.html#112-upper-confidence-bound-ucb",
    
    "relUrl": "/class-notes/04-Computational-Sensorimotor-Learning/simple-decision.html#112-upper-confidence-bound-ucb"
  },"380": {
    "doc": "1. Simple Decisions",
    "title": "1.2 Contextual Bandit (CB)",
    "content": "Before, we considered a special case where the reward was independent of the state. In CB, we assume that reward is state (or context) dependent. Now the reward is: . \\[\\begin{equation} r(a_t, s_t) = r(a_{1:t}, s_{1:t}) \\end{equation}\\] We consider the next following concepts related to CB which are augmented from MAB: . | Context \\(x_t\\): this is the state where \\(x_t \\in \\mathcal{X}\\) where \\(\\mathcal{X}\\) is the context/state space. | Contextual Reward \\(r(a_t, x_t) \\in \\mathbb{R}\\): this is the reward of taking action \\(a_t\\) in context \\(x_t\\). | Contextual expected regret \\(\\textbf{Regret}(T)\\) expressed as: | . \\[\\begin{equation} \\textbf{Regret}(T) := \\mathbb{E}[\\sum_{t=1}^T \\max_{a \\in \\mathcal{A}} r(x_t, a) - r(x_t, a_t)] \\end{equation}\\] now how do we choose actions in CB? We can use the same principles as MAB. ",
    "url": "/blog/class-notes/04-Computational-Sensorimotor-Learning/simple-decision.html#12-contextual-bandit-cb",
    
    "relUrl": "/class-notes/04-Computational-Sensorimotor-Learning/simple-decision.html#12-contextual-bandit-cb"
  },"381": {
    "doc": "1. Simple Decisions",
    "title": "1.2.1 LinUCB",
    "content": "LinUCB is a linear contextual bandit algorithm. It assumes that the reward is linear in the context. First-pass we can try to make UCB context-dependent: . \\[\\begin{equation} \\hat \\mu_{x,k}(t) := \\frac{1}{T_{x,k}(t)} \\sum_{t=1}^T \\mathbb{I}\\{a_t=k, x_t=x\\}r(x_t,a_t) \\end{equation}\\] How do we obtain \\(T_{x,k}(t)\\) without knowing \\(\\mathcal{X}\\)? If we cannot enumerate through \\(\\mathcal{X}\\), there will be infinite elements for \\(T_{x,k}(t)\\). We need to get around this issue. This is the motivation for LinUCB. The key idea of LinUCB is to estimate context-dependent empirical mean and and confidence interval by linear regression. Let context \\(x \\in \\mathbb{R}^d\\). We use LinUCB implementation with disjoint linear models. Let \\(\\hat \\theta_k \\in \\mathbb{R}^d\\) be the parameter vector for linear regression. \\[\\begin{equation} \\hat \\mu_{\\hat \\theta_k}(x) := \\hat \\theta_k^T x \\end{equation}\\] which is a scalar term. Empirical mean Rewards are assumed to be drawn from a 1-subgaussian distribution. We use least-squares to get the parameter vector: . \\[\\begin{equation} \\hat \\theta_k^* = \\arg \\min_{\\hat \\theta_k} \\mathbb{E} [(\\hat \\mu_{\\hat \\theta_k}(x) - r(x, k))^2] \\end{equation}\\] this approximates the mean of the gaussian distribution. This can be solved in closed form using the following equation: . \\[\\begin{equation} \\hat \\theta_k^* = (\\mathbf{D}_k^T\\mathbf{D}_k + \\lambda \\mathbf{I})^{-1} \\mathbf{D}^T_k \\mathbf{c}_k \\end{equation}\\] This is the closed form solution for linear regression but there is an added term \\(\\lambda \\mathbf{I}\\). This is a regularization term that prevents overfitting (ridge regression). Confidence bound We cannot directly construct a confidence bound because we cannot calculate \\(T_{x,k}(t)\\). A workaround is needed to construct this bound: . \\[\\begin{equation} P(\\mu_{x,k} \\geq \\hat \\mu_{\\hat \\theta_k}(\\mathbf{x}_{t,k}) + \\alpha \\sqrt{\\mathbf{x}_{t,k}^T(\\mathbf{D}_k^T\\mathbf{D}_k + \\lambda \\mathbf{I})^{-1}\\mathbf{x}_{t,k}}) \\leq \\delta, \\delta \\in (0,1) \\end{equation}\\] where \\(\\alpha := 1 + \\sqrt{\\frac{\\log\\frac{2}{\\delta}}{2}}\\). def LinUCB(t, lambda, A, b): x_tk = receive_contexts() d = len(x_tk) theta = [] for k in range(len(action_space)): ak = action_space[k] if ak not in taken_ks: A_k = lambda * np.eye(d) b_k = np.zeros(d) A[k] = A_k b[k] = b_k taken_ks.add(ak) theta_k = np.linalg.inv(A[k]) @ b[k] theta.append(theta_k) alpha = 1 + np.sqrt(2 * np.log(2 / delta) / 2) at = np.argmax( [theta[k] @ x_tk + alpha * sqrt(x_tk.T @ np.linalg.inv(A[k]) @ x_tk) for k in action_space] ) # modify A, b for next calls of LinUCB A[k] = A[k] + np.outer(x_tk, x_tk) b[k] = b[k] + r(x_tk, at) * x_tk return at . ",
    "url": "/blog/class-notes/04-Computational-Sensorimotor-Learning/simple-decision.html#121-linucb",
    
    "relUrl": "/class-notes/04-Computational-Sensorimotor-Learning/simple-decision.html#121-linucb"
  },"382": {
    "doc": "1. Simple Decisions",
    "title": "1. Simple Decisions",
    "content": " ",
    "url": "/blog/class-notes/04-Computational-Sensorimotor-Learning/simple-decision.html",
    
    "relUrl": "/class-notes/04-Computational-Sensorimotor-Learning/simple-decision.html"
  },"383": {
    "doc": "SLURM Interactive",
    "title": "Using SLURM Interactively",
    "content": "Something that would be very cool to achieve is using the cluster like a local machine. One way of doing this is to use salloc to allocate resources and then interact with them on the terminal. We could then run a setup script to load modules, setup the right files, and then run a command like tmux to keep the session alive. Reminder on the role of directories: . | /scratch is for temporary files, intermediate data, IO operations for slurm job. It is purged | /home is for user files or scripts. It is not purged. | /tmp is local files that can be quickly accessed in the node. It is purged after the job is done or process dies. | . ",
    "url": "/blog/code-notes/cluster-notes/slurm-interactive.html#using-slurm-interactively",
    
    "relUrl": "/code-notes/cluster-notes/slurm-interactive.html#using-slurm-interactively"
  },"384": {
    "doc": "SLURM Interactive",
    "title": "GUI Visualization",
    "content": "Visualizing the GUI of a program on a remote server is possible using X11 forwarding. This is similar to specifying -X or -Y when using ssh. Then adding in the display variable to the command. Alternatives include NX (which anydesk uses) or lsh. NOTE: After much digging, it seems that visualizing GUI applications on the Great Lakes cluster is not recommended. We should treat Great Lakes like a prod cluster. Fundamentally, we have to pay for resources on the cluster, and using it for visualization (which is what a GUI does), is wasting resources that could be used for actual compute/training. Any additional notes here can be used for other local lab machines like exxact/lambda. ",
    "url": "/blog/code-notes/cluster-notes/slurm-interactive.html#gui-visualization",
    
    "relUrl": "/code-notes/cluster-notes/slurm-interactive.html#gui-visualization"
  },"385": {
    "doc": "SLURM Interactive",
    "title": "Using anaconda/mamba/miniconda",
    "content": "You’ll notice when using these that conda environments are saved in the /home directory. This will easily start cluttering your home directory. We can think about storing anaconda environments in /scratch or /tmp instead. Using /tmp would containerize the environment to the node, but it would be purged after the job is done. We would have to reinstall packages every time (maybe waste of time). One big benefit is that it is local to the node, so there is no network latency/overhead. ",
    "url": "/blog/code-notes/cluster-notes/slurm-interactive.html#using-anacondamambaminiconda",
    
    "relUrl": "/code-notes/cluster-notes/slurm-interactive.html#using-anacondamambaminiconda"
  },"386": {
    "doc": "SLURM Interactive",
    "title": "SLURM Interactive",
    "content": " ",
    "url": "/blog/code-notes/cluster-notes/slurm-interactive.html",
    
    "relUrl": "/code-notes/cluster-notes/slurm-interactive.html"
  },"387": {
    "doc": "SLURM",
    "title": "Some",
    "content": " ",
    "url": "/blog/code-notes/cluster-notes/slurm.html#some",
    
    "relUrl": "/code-notes/cluster-notes/slurm.html#some"
  },"388": {
    "doc": "SLURM",
    "title": "Some commands",
    "content": "sbatch [sbatch_script].sh squeue -u [uniqname] # List all jobs of the user submitted scancel [job_id] # Cancel a job scontrol show job -dd [job_id] # detailed squeue scontrol show nodes scontrol show [node] scontrol hold [job_id] # Hold a job scontrol release [job_id] # Release a job scontrol write batch_script [job_id] # view job batch script sinfo # cluster status salloc [args] # start interactive jobsa salloc [args] --x11 # X11 forwarding sacct -j [job_num] --format jobid,jobname,NTasks,nodelist,CPUTime,RegMem,Elapsed # monitor or review a job's resource usage\\ sacctmgr show assoc user=$USER # view job batch script sacctmgr show assoc account=[account] # view users with access to account sacctmgr show assoc user [uniqname] # view default submission account and wckey my_account_usage -A [account] my_job_statistics -j [job_id] . ",
    "url": "/blog/code-notes/cluster-notes/slurm.html#some-commands",
    
    "relUrl": "/code-notes/cluster-notes/slurm.html#some-commands"
  },"389": {
    "doc": "SLURM",
    "title": "Partition options",
    "content": "Partition options: . | Debug: run jobs quickly for debugging . | max jobs per user: 1 | max walltime: 4 hours | max processor per job: 8 | max memory per job: 40gb | higher scheduling priority | . | standard: . | max walltime: 14 days | default partition | . | standard-oc: get additional software you can only use on-campus . | max walltime: 14 days | . | gpu: Allows use of Tesla V100 GPUs . | max walltime: 14 days | . | spgpu: Allows use of A40 GPUs . | max walltime: 14 days | . | largemem: allows use of compute node with 1.5TB of RAM . | max walltime: 14 days | . | . ",
    "url": "/blog/code-notes/cluster-notes/slurm.html#partition-options",
    
    "relUrl": "/code-notes/cluster-notes/slurm.html#partition-options"
  },"390": {
    "doc": "SLURM",
    "title": "Batch job: Options for requesting resources",
    "content": "| Option | SLURM Command (#SBATCH) | Example | . | Job name | --job-name=&lt;name&gt; | --job-name=asdf | . | Account | --account=&lt;account&gt; | --account=test This account is from the provider account | . | Queue | --partition=&lt;queue&gt; | --partition=standard We can choose “standard”, “gpu” (GPU jobs only), largemem (large memory), viz, debug, standard-oc (oc=on-campus software) | . | Wall time limit | --time=&lt;dd-hh:mm:ss&gt; | --time=10:00 (10 minutes) If you want if you got the write wall time, you can try on debug partition | . | Node count | --nodes=&lt;count&gt; | --nodes=3 | . | Process count per node | --ntasks-per-node=&lt;count&gt; | --ntasks-per-node=1 invoke ntasks on each node | . | Core count (per process) | --cpus-per-task=&lt;cores&gt; | --cpus-per-task=1 Without specifying, it defaults to 1 processor per task | . | Memory limit | --mem=&lt;limit&gt; (Memory per node in GiB, MiB, KiB) | --mem=12000m (12 GiB roughly) | . | Minimum memory per processor | --mem-per-cpu=&lt;memory&gt; | --mem-per-cpu=1000m (1 GiB per CPU) | . | Request GPUs | --gres=gpu:&lt;count&gt; --gpus=[type:]&lt;number&gt; | --gres=gpu:2 gpus=2 | . | Process count per GPU | --ntasks-per-gpu=&lt;count&gt; Must be used with --ntasks or --gres=gpu: | --ntasks-per-gpu=1 --gres=gpu:4 2 tasks per GPU times 4 GPUS = 8 tasks total | . ",
    "url": "/blog/code-notes/cluster-notes/slurm.html#batch-job-options-for-requesting-resources",
    
    "relUrl": "/code-notes/cluster-notes/slurm.html#batch-job-options-for-requesting-resources"
  },"391": {
    "doc": "SLURM",
    "title": "Batch job: Environment variables and logs",
    "content": "| Option | SLURM Command (#SBATCH) | Example | . | Copy environment | --export=ALL | --export=ALL This copies all environment variables to the job | . | Set Env variable | --export=&lt;variable=value,var2=val2&gt; | --export=EDITOR=/bin/vim | . | Standard output file | --output=&lt;file path&gt; (path must exist) | --output=/home/%u/%x-%j.log NOTE: %u is the user, %x is the job name, %j is the job ID | . | Standard error file | --error=&lt;file path&gt; (path must exist) | --error=/home/%u/%x-%j.err | . ",
    "url": "/blog/code-notes/cluster-notes/slurm.html#batch-job-environment-variables-and-logs",
    
    "relUrl": "/code-notes/cluster-notes/slurm.html#batch-job-environment-variables-and-logs"
  },"392": {
    "doc": "SLURM",
    "title": "Batch job: job control",
    "content": "| Option | SLURM Command (#SBATCH) | Example | . | Job array | --array=&lt;array indices&gt; | --array=0-15 | . | Job dependency | --dependency=after:jobID[:jobID...] | --dependency=after:1234[:1233] | . | Email address | --mail-user=&lt;email&gt; | --mail-user=uniqname@umich.edu | . | Defer job til time | --begin=&lt;date/time&gt; | --begin=2020-12-25T12:30:00 | . ",
    "url": "/blog/code-notes/cluster-notes/slurm.html#batch-job-job-control",
    
    "relUrl": "/code-notes/cluster-notes/slurm.html#batch-job-job-control"
  },"393": {
    "doc": "SLURM",
    "title": "SLURM",
    "content": " ",
    "url": "/blog/code-notes/cluster-notes/slurm.html",
    
    "relUrl": "/code-notes/cluster-notes/slurm.html"
  },"394": {
    "doc": "Space Workshop",
    "title": "Space Workshop (5/19/2025)",
    "content": " ",
    "url": "/blog/conference-notes/icra-2025/workshop/space.html#space-workshop-5192025",
    
    "relUrl": "/conference-notes/icra-2025/workshop/space.html#space-workshop-5192025"
  },"395": {
    "doc": "Space Workshop",
    "title": "Nima Fazeli",
    "content": "Context: Crows is more of a materials crowd. They’re much more inclined towards model-based approach to space manipulation. Nima presenting the usual stuff. Tries to convince a crowd to adopt tactile technology. I think the marketing is to add-on tactile into the system. For a space crowd, they may be concerned about the compute requirements of the work he’s presenting (Miquel’s work). This is especially the case when the spacecraft is naturally resource-constrained. Another thing is that on a spacecraft, we’re using a floating-base. Can we keep the current assumptions from tabletop manipulation (quasi-static, )? . ",
    "url": "/blog/conference-notes/icra-2025/workshop/space.html#nima-fazeli",
    
    "relUrl": "/conference-notes/icra-2025/workshop/space.html#nima-fazeli"
  },"396": {
    "doc": "Space Workshop",
    "title": "Space Workshop",
    "content": " ",
    "url": "/blog/conference-notes/icra-2025/workshop/space.html",
    
    "relUrl": "/conference-notes/icra-2025/workshop/space.html"
  },"397": {
    "doc": "Spatial Transformer Networks",
    "title": "Spatial Transformer Networks",
    "content": "paper link . ",
    "url": "/blog/deep-dive-notes/debug-point-ml/spatial-transformer.html",
    
    "relUrl": "/deep-dive-notes/debug-point-ml/spatial-transformer.html"
  },"398": {
    "doc": "TacSL",
    "title": "TacSL",
    "content": "Equations they use for simulating shear fields is: . For each point on the elastomer, we calculate the contact force acting on it due to object penetration. \\(\\begin{align} \\mathbf{f}_n &amp;= (-k_n + k_d \\dot d) d \\cdot \\mathbf{n} \\\\ \\mathbf{f}_t &amp;= -\\frac{\\mathbf{v}_t}{\\|\\mathbf{v}_t\\|} \\min\\{k_t \\| \\mathbf{v}_t\\|, \\mu \\|\\mathbf{f}_n\\|\\} \\end{align}\\) . | \\(k_n, k_d\\) are spring/damping constants. | \\(d\\): penetration depth of the point (gotten from object-elastomer sdf calculation). | \\(\\mathbf{v}_t\\): tangential contact velocity. | they set \\(k_d = 0\\) | for \\(-\\frac{\\mathbf{v}_t}{\\|\\mathbf{v}_t\\|}\\), they clamp the minimum of the norm to avoid NaN. | . ",
    "url": "/blog/deep-dive-notes/tactile-rl-notes/tacsl.html",
    
    "relUrl": "/deep-dive-notes/tactile-rl-notes/tacsl.html"
  },"399": {
    "doc": "Extrinsic Contact Control",
    "title": "Tactile-driven Non-prehensile Control via Extrinsic Contact Mode Control",
    "content": "Link . This is a more explicit paper that uses extrinsic contact for control. We grasp on object using a bubble gripper and use it to pivot/push another object. It’s a fairly straightforward setup and the controller is solve using a Quadratic Program (QP). It assumes quasi-static for simplicity. Measurements: . | \\(\\mathbf{w}_{\\text{ext}}\\) - this is the Force-Torque (FT) sensor measurement if we subtract the weight of the end-effector below the FT sensor. | \\(\\mathbf{x}_{\\text{ee}}\\) - this comes from proprioception (joints of Kuka Iiwa). | \\(\\mathbf{x}_{\\text{go}}\\) - we can get this from apriltag. | \\(\\mathbf{x}_{\\text{eo}}\\) - we can get this from apriltag. | . Givens: . | \\(m_{\\text{go}}\\) - we measure | \\(m_{\\text{eo}}\\) - we measure | \\(\\mu_i\\) - we calibrate to get | . We recover contact points (object frame) using Contact Particle Filter. This also gives us the contact jacobian. This allows us to map the contact forces from end-effector to object and object to ground to world frame. Using this, we can solve for the contact forces on the extrinsic contact points to perform our tasks. Then, SEED (another force-controller using springs-dampers) is used to enforce those contacts forces. We model the optimization problem for contact forces as follows: . \\[\\begin{align*} \\min_{\\mathbf{f}_{c,1},..,\\mathbf{f}_{c,N}} &amp;\\sum_{i=1}^N \\frac{1}{2} \\mathbf{f}_{c,i}^T \\mathbf{U} \\mathbf{f}_{c,i} \\\\ s.t. \\quad &amp;\\mathbf{\\tau}_{g,eo} + \\sum_{i=1}^N \\mathbf{J}_{c,i}^T \\mathbf{f}_{c,i} = 0, \\\\ &amp;\\mathbf{w}_{ext} + \\mathbf{\\tau}_{g,go} + \\sum_{j \\in \\text{obj}} \\mathbf{J}^T_{c,go,j}\\mathbf{f}_{c,j} = 0 \\\\ &amp;0 \\leq \\mu_i f_{c,i,n} + f_{c,i,t} \\perp \\dot s_{c,i}^+ \\geq 0 \\quad i = 1,...,N \\\\ &amp;0 \\leq \\mu_i f_{c,i,n} - f_{c,i,t} \\perp \\dot s_{c,i}^- \\geq 0 \\quad i = 1,...,N \\end{align*}\\] We can further simplify this problem by only considering the grasped object and the extrinsic object. \\[\\begin{align*} \\min_{\\mathbf{f}_{c,1},..,\\mathbf{f}_{c,N}} &amp;\\sum_{i=1}^N \\frac{1}{2} \\mathbf{f}_{c,i}^T \\mathbf{U} \\mathbf{f}_{c,i} \\\\ s.t. \\quad &amp;\\mathbf{w}_{ext} + \\mathbf{\\tau}_{g,go} + \\sum_{j \\in \\text{obj}} \\mathbf{J}^T_{c,go,j}\\mathbf{f}_{c,j} = 0 \\\\ &amp;0 \\leq \\mu_i f_{c,i,n} + f_{c,i,t} \\perp \\dot s_{c,i}^+ \\geq 0 \\quad i = 1,...,N \\\\ &amp;0 \\leq \\mu_i f_{c,i,n} - f_{c,i,t} \\perp \\dot s_{c,i}^- \\geq 0 \\quad i = 1,...,N \\end{align*}\\] For contact-mode control, we already set the complementarity constraints, so they just become linear inequality constraints. Thus, this becomes a QP. | For pushing, we want the extrinsic object to ground contact to be sliding. | For pivoting, we want the extrinsic object to ground contact to be sticking (and only one is in contact). | . In order for seed to enforce these contacts, we actually model a nonlinear optimization problem. SEED allows us to convert end-effector poses and grasped object poses to an external wrench. The QP allows us to convert the external wrench to contact forces. We use all of this to stabilize the relative end-effector to object trajectory and contact modes. ",
    "url": "/blog/project-notes/project1/lit_review/tactile-control-extrinsic.html#tactile-driven-non-prehensile-control-via-extrinsic-contact-mode-control",
    
    "relUrl": "/project-notes/project1/lit_review/tactile-control-extrinsic.html#tactile-driven-non-prehensile-control-via-extrinsic-contact-mode-control"
  },"400": {
    "doc": "Extrinsic Contact Control",
    "title": "Questions:",
    "content": ". | It seems like we can easily extend this problem to multiple objects. Can we? | This is a very strict regime. As in, we have to set the friction parameters, contact locations, and find object pose. Can we soften this regime to be much more friendly? | . ",
    "url": "/blog/project-notes/project1/lit_review/tactile-control-extrinsic.html#questions",
    
    "relUrl": "/project-notes/project1/lit_review/tactile-control-extrinsic.html#questions"
  },"401": {
    "doc": "Extrinsic Contact Control",
    "title": "Extension to multi-object (not in paper)",
    "content": "So the current implementation goes like this: . | end-effector grasps object | object interacts with extrinsic object | extrinsic object interacts with ground | . | Given target poses, get external wrench from end-effector to grasped object. | Use external wrench to get contact forces from extrinsic object to ground. | . To extend to multiple objects, it’s fairly simple . | Given target poses, get external wrench 1 from end-effector to object 1. | Use external wrench 1 (should be sticking) contact forces from object 2 to object 3. These contact forces will be external wrenches if object 2 is in contact with object 3. | Use external wrenches from object 2 to object 3 to get contact forces from object 3 and object 4. … And so on and so forth | . This is a simple extension and the problem is still a QP. It’s fairly tough to solve though (could be infeasible) and could be very sensitive as the number of objects increase. ",
    "url": "/blog/project-notes/project1/lit_review/tactile-control-extrinsic.html#extension-to-multi-object-not-in-paper",
    
    "relUrl": "/project-notes/project1/lit_review/tactile-control-extrinsic.html#extension-to-multi-object-not-in-paper"
  },"402": {
    "doc": "Extrinsic Contact Control",
    "title": "Extrinsic Contact Control",
    "content": " ",
    "url": "/blog/project-notes/project1/lit_review/tactile-control-extrinsic.html",
    
    "relUrl": "/project-notes/project1/lit_review/tactile-control-extrinsic.html"
  },"403": {
    "doc": "Taught Moves",
    "title": "Taught Moves",
    "content": " ",
    "url": "/blog/judo-notes/taught.html",
    
    "relUrl": "/judo-notes/taught.html"
  },"404": {
    "doc": "Taught Moves",
    "title": "Quick List",
    "content": ". | Forward throws . | O Goshi | Ippon Seoi Nage | Uki Goshi | Koshi Guruma | Uchi Mata (Hip) | Harai Goshi | Tai Otoshi | . | Ashi Waza (Foot stuff) . | O Soto Gari | Deashi Harai | Sasae Tsurikomi Ashi | Kouchi Gari | Ouchi Gari | . | Pins . | Kesa Gatame | Kata Gatame | Yoko Shio Gatame | Uki Gatame | Kami Shiho Gatame | . | Sacrifice Throws . | Kouchi Makikomi | Tomoe Nage | Sumi Gaeshi | . | Non-competitive Throws . | ",
    "url": "/blog/judo-notes/taught.html#quick-list",
    
    "relUrl": "/judo-notes/taught.html#quick-list"
  },"405": {
    "doc": "Taught Moves",
    "title": "Sukui Nage (single leg)",
    "content": "| . | . ",
    "url": "/blog/judo-notes/taught.html#sukui-nage-single-leg",
    
    "relUrl": "/judo-notes/taught.html#sukui-nage-single-leg"
  },"406": {
    "doc": "Taught Moves",
    "title": "Forward throws:",
    "content": "O Goshi: . | Hip throw | Grips: One hand on sleeve, one hand on belt from other side. | Fairly easy transfer from wrestling. | Kuzushi: Pull on sleeve and belt. | . Ippon Seoi Nage: . | One arm shoulder throw. | Grips: One hand on sleeve, one hand on collar. | Kuzushi: Pull on sleeve and collar. Turn and uppercut armpit. | Variations: . | One hand on sleeve (closest to throwing arm). | Morote Seoi Nage: Throw without uppercut, use collar arm as a lever. | . | . Uki Goshi: . | Floating hip throw. | . Koshi Guruma: . | Hip wheel throw. | Much like head throw in wrestling. | . Uchi Mata (Hip): . | Inner thigh throw. | Grips: One hand on sleeve, one hand on collar. | Kuzushi: Pull on sleeve and collar. | You have step in deep (squat lower). | Then the throw is lifting the leg (slight mount on the hip) and sweeping the leg out. | . Harai Goshi: . | Hip sweep throw. | Grips: One hand on sleeve, one hand collar. | Kuzushi: Pull on sleeve and collar. | You have to step in deep (squat lower). | Similar to uchi mata in terms of penetration step. | Reap with the front leg. | Compared to Osoto-Gari in terms of ripping the throw. | . ",
    "url": "/blog/judo-notes/taught.html#forward-throws",
    
    "relUrl": "/judo-notes/taught.html#forward-throws"
  },"407": {
    "doc": "Taught Moves",
    "title": "Ashi Waza (Foot stuff):",
    "content": "O Soto Gari: . | Major outer reap. | . Deashi Harai: . | Major foot sweep. | . nice to learn ashi guruma and kosoto gari. Sasae tsurikomi ashi: . | Major foot block. | Spinny boi kinda like the muay thai clinch throw. | Spin oneself to move opponent and block the leading foot. | Grips: One hand on sleeve, one hand on collar. | . Kouchi gari: . | Small inner reap. | . Ouchi gari: . | Big inner reap. | . I suck at kouchi gari and ouchi gari. I also struggle with getting deashi harai to work. ",
    "url": "/blog/judo-notes/taught.html#ashi-waza-foot-stuff",
    
    "relUrl": "/judo-notes/taught.html#ashi-waza-foot-stuff"
  },"408": {
    "doc": "Taught Moves",
    "title": "Pins:",
    "content": "Kesa Gatame: . | Scarf hold. | . Kata Gatame: . | Head arm choke but it’s a pin. | . Yoko Shio Gatame: . | Side control from bjj. | . Uki Gatame: . | The kenny special | Knee on belly pin. | . Kami Shiho Gatame: . | North-south pin. | . NOTE: not going to focus on sacrifice throws since I think it’s low percentage. ",
    "url": "/blog/judo-notes/taught.html#pins",
    
    "relUrl": "/judo-notes/taught.html#pins"
  },"409": {
    "doc": "Taxim",
    "title": "Taxim",
    "content": "Taxim uses a reflection function inspired by photometric stereo to model tactile rgb rendering. Photometric stereo linear reflection function is defined by: . \\[\\begin{align} I = \\sum_l \\alpha_l \\mathbf{n} \\end{align}\\] where \\(I\\) is the intensity (we can have \\(I_r, I_g, I_b\\) for all rgb values). However, we can note that this is linear and the lighting is not necessarily linear in the environment, so we go for a more nonlinear function: . We are given a background image \\(I_0\\) (image of gel without contact). We want to find the image while in contact \\(I_1\\). we do this by finding the change in the image \\(\\Delta I\\). \\(\\begin{align} I_1 &amp;= I_0 + \\Delta I \\\\ \\Delta I &amp;= (\\mathbf{w}_n)^T \\begin{bmatrix} x^2 &amp; y^2 &amp; xy &amp; x &amp; y &amp; 1 \\end{bmatrix}^T \\end{align}\\) . | \\(\\Delta I\\): intensity at location (x,y) in pixel coordinates. | We’ll have \\(I_r, I_g, I_b\\) (3 sets of weights). | \\(\\mathbf{w}_n\\): weights for light source \\(l\\) and normal direction \\(n\\). | normal direction \\(n\\) is discretized into 125x125 bins (binning by direction in spherical coordinates). | . where \\(\\mathbf{w}^l_n\\) is a list of weights for a 2nd degree polynomial. Note that the subscript \\(n\\) denotes the fact that the weights are looked up based on the normal direction. ",
    "url": "/blog/deep-dive-notes/tactile-rl-notes/taxim.html",
    
    "relUrl": "/deep-dive-notes/tactile-rl-notes/taxim.html"
  },"410": {
    "doc": "Temporal GNNs",
    "title": "Temporal GNNs",
    "content": " ",
    "url": "/blog/graph-notes/temporal-gnn.html",
    
    "relUrl": "/graph-notes/temporal-gnn.html"
  },"411": {
    "doc": "Temporal GNNs",
    "title": "Motivation",
    "content": "At the time (era of GCN, GAT, MPNN, GraphSAGE, …etc), GNNs have mostly been ran on static graphs. As in, these graphs do not change over time. However, lots of interesting data is dynamic, so we want to extend our algorithms to work on dynamic graphs. These kinds of graphs change over time (not just features, sometimes nodes/edges are added/deleted). Above image shows a time-series dynamic graph. ",
    "url": "/blog/graph-notes/temporal-gnn.html#motivation",
    
    "relUrl": "/graph-notes/temporal-gnn.html#motivation"
  },"412": {
    "doc": "Temporal GNNs",
    "title": "Temporal Graph Networks (TGN)",
    "content": "This was developed at twitter to take in a graph interaction network over a time-series and predict interaction network in the future step. TGN encoder overview. The goal of this diagram is to predict the future interaction between nodes 2 and 4. TGN computes embeddings for nodes 2 and 4 and then concatenates them and feeds it to MLP decoder. Components of a TGN: . | Memory | Message Function | Memory Updater | Embedding | . ",
    "url": "/blog/graph-notes/temporal-gnn.html#temporal-graph-networks-tgn",
    
    "relUrl": "/graph-notes/temporal-gnn.html#temporal-graph-networks-tgn"
  },"413": {
    "doc": "Temporal GNNs",
    "title": "Memory",
    "content": "This is analogous to the RNN hidden state. If a new node is added, we add a state of vectors to our data matrix. ",
    "url": "/blog/graph-notes/temporal-gnn.html#memory",
    
    "relUrl": "/graph-notes/temporal-gnn.html#memory"
  },"414": {
    "doc": "Temporal GNNs",
    "title": "Message Function",
    "content": "Main mechanism for updating memory. Given interaction between nodes i and j at time t, message function computes two messages (one for i and one for j), which are used to update memory. Analogous to messages in MPNN. The message is a function fo the memory of nodes i and j at time \\(t^-\\) preceding interaction, the interaction time t, and the edge features. \\[\\begin{align} \\mathbf{m}_i(t) = \\text{msg}(\\mathbf{s}_i(t^-), \\mathbf{s}_j(t^-), t, \\mathbf{e}_{ij}(t)) \\\\ \\mathbf{m}_j(t) = \\text{msg}(\\mathbf{s}_j(t^-), \\mathbf{s}_i(t^-), t, \\mathbf{e}_{ij}(t)) \\end{align}\\] . ",
    "url": "/blog/graph-notes/temporal-gnn.html#message-function",
    
    "relUrl": "/graph-notes/temporal-gnn.html#message-function"
  },"415": {
    "doc": "Temporal GNNs",
    "title": "Memory Updater",
    "content": "The memory updater updates memory with new messages. This is implemented as an RNN. \\[\\begin{align} \\mathbf{s}_i(t) = \\text{mem}(\\mathbf{m}_i(t), \\mathbf{s}_i(t^-)) \\end{align}\\] Given that this memory is a vector, we could try to use this as our node embedding. In practice, this can be stale as we have to wait until the next interaction to update the memory. ",
    "url": "/blog/graph-notes/temporal-gnn.html#memory-updater",
    
    "relUrl": "/graph-notes/temporal-gnn.html#memory-updater"
  },"416": {
    "doc": "Temporal GNNs",
    "title": "Embedding",
    "content": "A solution is to look at node neighbors. The embedding module computes temporal embedding of a node by performing graph aggregation over the spatiotemporal neighbors of that node. Even if a node has been inactive for a while, it likely has some neighbors that were active. ",
    "url": "/blog/graph-notes/temporal-gnn.html#embedding",
    
    "relUrl": "/graph-notes/temporal-gnn.html#embedding"
  },"417": {
    "doc": "Temporal GNNs",
    "title": "Overall Compute Architecture",
    "content": ". ",
    "url": "/blog/graph-notes/temporal-gnn.html#overall-compute-architecture",
    
    "relUrl": "/graph-notes/temporal-gnn.html#overall-compute-architecture"
  },"418": {
    "doc": "To Learn",
    "title": "Things I want to Learn",
    "content": " ",
    "url": "/blog/judo-notes/tolearn.html#things-i-want-to-learn",
    
    "relUrl": "/judo-notes/tolearn.html#things-i-want-to-learn"
  },"419": {
    "doc": "To Learn",
    "title": "Grip Fighting",
    "content": ". | Grip Fighting Workshop . | Cross Collar grip RvR break: . | 7:07-8:48 | Let go of sleeve grip and put arm over the cross collar grip arm (punch their chest). | Turn towards the “punch” and pull back on my own leading arm (right). | Once they let go of the throw, use my own leading arm to push their arm out and feed it to my other hand (strong wrist grip). | EXTRA MOVE: do a snap down on the wrist grip. | Get higher collar grip using snap down. | . | . | Deal with double collar stiff arm RvR: . | 9:23-12:27 | right arm goes underneath, left arm goes over. | we double collar them back. | we circle on our lead leg (to the right) and then rotate them to their left. | RULE OF THUMB: never circle on lead leg unless we know they are playing defensive (posture back and stiff arm). | . | When opponent reaches with their sleeve arm, you can’t go past initial point of contact. | 12:30-15:20 | You have to grab their wrist. | Go straight for their sleeve arm (using your arm from other side). | Double arm on sleeve. | If they take your back (it’s your movement), you can just turn and throw them. | With double grip, tai otoshi sounds useful. | . | Control sleeve arm and they’ll never throw (RvR) . | 15:20-18:50 | Defense: Stiff arm way down (elbow down). | Offense: If your arm is above your chest, you can do maki-komi. | . | Counter Georgian Grip (RvR) . | 19:20-24:00 | Moves with Georgian: . | Uchi Mata | Osoto Gari | Habo Relis? | Ouchi Gari | Harai Goshi | . | Don’t stay in georgian grip for too long. | Grip their belt and keep their hips away. | You can also try ouchi gari. | Grab belt using your sleeve arm. | Pull on belt, and push on lapel. | Then backstep and do ouchi gari. | Doesn’t matter if you can’t hit ouchi, it’s about defense since georgian is dangerous. | . | Sumi Gaeshi (sacrifice) with Georgian. (not ready) | . | One-handed collar grip (RvR): . | 26:00-27:15 | Just turn throws, one-handed osoto, …etc. | Kouchi Makikomi | . | Use body to generate power (kuzushi): . | 28:00-30:00 | Use body to whip them toward you. | Sell the movement with the chest. | . | What if they two hands on your right hand (RvR): . | 30:30-33:00 | Use hips and move to the side. | Get your doubled hand above your chest (same intuition as before). | Grab their right hand and maybe go for tai otoshi or kouchi gari. | If you do kouchi gari, you can follow up with snapping arm down like before and throw other hand high. | . | Ippon Seoi Nage Grip fighting (cross collar): . | 33:30-34:30 | Cross collar grip (if they don’t resist, throw). | If they try counter with stiff arm, catch before it lands and go 2-on-1. | If 2-on-1 no resist, snap down or get better position. | If they grip your lead arm, you can use your arm to move over their 2-on-1 arm to break off the grip. | Once their grip breaks, it’s an OPENING. Grab high right after. | . | . | RvL counter to cross collar grip: . | 34:45-35:12 | Post the stiff arm. | Grab their cross collar grip arm with your other hand. | Draw bow and extend their stiff arm. | . | How to counter O Goshi Grip (RvL): . | 35:30-37:30 | If they get belt grip, frame their belt grip arm with same side arm. | Tighten the gi jacket by creating distance squeezing your arms together. | . | How to counter high collar (neck) grip and/or/xor wrist grip (RvR): . | 37:45-40:00 | With high collar grip, you can treat like a wrestling collar tie and snap them down (make life suck). | Option 1: Grab left collar (opposite) and shimmy over the collar tie frame. | Option 2: Dip slide back and then forward. | as you slide forward, get your collar grip deeper. | win wrist grip. | turn back to face them, since we have deeper collar grip, their neck grip breaks. | use wrist grip arm to grab their broken neck grip. | then snap down the arm…. | . | . | Angles and Clubbing . | 54:00-56:00 | cross collar grip, block lapel arm. | if they come from underneath grip, you throw it out with your elbow. | hit him with a hook (that lands on the neck/back). | . | Break sleeve grip (RvR): . | 60:00-63:00 | It’s about what happens after a grip is broken. | C-grip grab their wrist. Then use other hand to pull the sleeve back (due to limits of the jacket). | On (LvR) you can actually do sumi-gaeshi from this position. | . | Double lapell grip (RvR): . | 64:00-68:00 | Russian Tie wrestling but in judo (belt grip). | Counters is uchi-mata (or if too low, te-waza). | Baseball bat like breaking of the grip. | . | Break sleeves: . | 77:00-78:30 | just watch the clip. | . | Harai Makikomi . | 78:45-80:00 | . | Overall tips: . | stay on the left. | control the left. | . | . | . ",
    "url": "/blog/judo-notes/tolearn.html#grip-fighting",
    
    "relUrl": "/judo-notes/tolearn.html#grip-fighting"
  },"420": {
    "doc": "To Learn",
    "title": "Breaking down moves",
    "content": ". | Harai Goshi and Senkaku Travis Stevens . | Kuzushi, Tsukure, Kake | Kuzushi (off balance): Pull on sleeve and collar and turn to one foot. | Tsukure (entry): Step in deep (squat lower) and turn. | Kake (throw): reap the leg and push/pull with arms. | Senkaku from turtle. | . | Harai Goshi Shintaro . | Squat deeper to create elevation during throw. | Set up combo with ouchi gari to harai goshi. | In IJF they do Uchi-Mata to Harai Goshi. | Better Explanation of Harai Goshi . | Explanation of the lift (it’s small not like O goshi). | . | 2 Harai Goshis . | Classic hip loading | Rotate over the leg. | . | . | . ",
    "url": "/blog/judo-notes/tolearn.html#breaking-down-moves",
    
    "relUrl": "/judo-notes/tolearn.html#breaking-down-moves"
  },"421": {
    "doc": "To Learn",
    "title": "Moves to Learn",
    "content": ". | Kosoto Gari | Tai Otoshi | . ",
    "url": "/blog/judo-notes/tolearn.html#moves-to-learn",
    
    "relUrl": "/judo-notes/tolearn.html#moves-to-learn"
  },"422": {
    "doc": "To Learn",
    "title": "Followups / Combinations",
    "content": ". | Combo 1: . | Sasai Tsurikomi Ashi . | Prevent opponent from taking a large step forward. | . | Kouchi Gari . | Pull on them to get their non-sasaid leg to step forward. | then run kouchi on the non-sasaid leg. | . | Deashi Harai . | run deashi on the kouchi’d leg. (non-sasaid leg). | . | . | Combo 2: . | Sasai Tsurikomi Ashi | Deashi Harai | NOTE: attack the trailing leg (not attacked from sasai). | . | Combo 3: . | Ouchi Gari | Deashi Harai | O Soto Gari . | NOTE: Ouchi Gari is a good follow up to Deashi Harai. | . | . | Combo 4: . | Deashi Harai / Ouchi Gari / Kouchi Gari | Follow up with Ippon Seoi Nage. | . | Grip + Foot Combo . | extend the opponent’s arm that grabs your lapel. | put weight on the extended arm. | then run deashi harai on their lead foot (closest to you RvR). | NOTE: if you can’t hit this, run a kosoto gari on the back foot. | . | Deashi Moves . | Step near (outside of) leg you want to sweep. | Pull | Deashi Harai | . | Kouchi Gari . | Pull on opponent in 50/50: . | If they don’t step forward, run turn throw (ippon seoi nage or o toshi) | If they do step forward, run kouchi gari on the leg they stepped forward with. | . | Circle then deashi | Kosoto Gari NOTE: you lose power if you don’t have a straight up posture when doing a foot sweep. | . | Osoto Depth . | Osoto Response for Stiff Arm 1 . | Move on the side | . | Adjust grips (uke’s lapel grabbing arm down, uke’s label goes up). | then just osoto gari. | . | If we throw osoto and they resist (combo) . | Don’t reap (you should feel their resistance through your attempted kuzushi). | We hit sasae. | . | Infinity Loop (osoto &lt;-&gt; Sasae) | RvL Osoto (turn shoulder and frame well). | in RvR, if they step the leg you want to reap away, just like RvL where you keep stepping forward (very important to keep the tension from hand position). | If they counter with osoto gari, push arm you’re holding way back for a pec fly. | Then you can reach back and do a mat drop (tani otoshi). | . | . ",
    "url": "/blog/judo-notes/tolearn.html#followups--combinations",
    
    "relUrl": "/judo-notes/tolearn.html#followups--combinations"
  },"423": {
    "doc": "To Learn",
    "title": "Footwork",
    "content": ". | Footwork . | If you have dominant hand grips, create movement. | If you don’t have dominant hand grips, don’t do it (it’s opponents movement). | Hand Grip Tip: . | Slowly move up the lapel to the collar (I’m pretty good at that). | Slowly regrip the sleeve to get it tighter. | Pull on lapel, push on sleeve. | Or slowly move down the arm to the wrist. | . | Huge tip: once you get really good grip and are pushing / pulling. | You can actually reverse the direction of push/pull to get opponent on back foot. | After that, do kouchi gari. | . | Good hand grips allow you to float to your setups. | . | . ",
    "url": "/blog/judo-notes/tolearn.html#footwork",
    
    "relUrl": "/judo-notes/tolearn.html#footwork"
  },"424": {
    "doc": "To Learn",
    "title": "To Learn",
    "content": " ",
    "url": "/blog/judo-notes/tolearn.html",
    
    "relUrl": "/judo-notes/tolearn.html"
  },"425": {
    "doc": "GDL 2: Topology",
    "title": "GDL 2: Topology (super short)",
    "content": " ",
    "url": "/blog/prereq-notes/topology.html#gdl-2-topology-super-short",
    
    "relUrl": "/prereq-notes/topology.html#gdl-2-topology-super-short"
  },"426": {
    "doc": "GDL 2: Topology",
    "title": "Metric Spaces",
    "content": "Metric spaces are sets \\(X\\) equipped with a distance \\(d(x,y)\\) between points \\(x,y \\in X\\). We can specify what is close/nearby in a quantifiable way. ",
    "url": "/blog/prereq-notes/topology.html#metric-spaces",
    
    "relUrl": "/prereq-notes/topology.html#metric-spaces"
  },"427": {
    "doc": "GDL 2: Topology",
    "title": "Topological Spaces",
    "content": "Topological spaces offer us a qualitative and more abstract way to think about what is “local” or “close”. A topological space is a set \\(X\\) together with a collection \\(T\\) of subsets of \\(X\\) called the open sets of \\(X\\) and satisfying the following properties: . | The empty set and \\(X\\) belong to \\(T\\). | Any finite intersection and arbitrary union of open sets in \\(T\\) is open. | . Open sets can be thought of as a neighborhood of the points they contain. ",
    "url": "/blog/prereq-notes/topology.html#topological-spaces",
    
    "relUrl": "/prereq-notes/topology.html#topological-spaces"
  },"428": {
    "doc": "GDL 2: Topology",
    "title": "Euclidian Topology",
    "content": "Is topology given by open sets induced by Euclidian Metric. More generally, for any metric space \\((X,d)\\), the open balls generate a topology on \\(X\\) called topology generated by \\(d\\). Continuity . A function \\(f: X \\to Y\\) between topological spaces is continuous if for every open subset \\(U \\subset Y\\) its preimage \\(f^{-1}(U)\\) is open in \\(X\\). It is like \\(\\epsilon-\\delta\\) continuity but without the need for a metric. Homeomorphism . A homeomorphism \\(\\phi: X \\to Y\\) between topological spaces is a bijective map such that both \\(\\phi\\) and \\(\\phi^{-1}\\) are continuous. Cube is homeomorphic to sphere. Donut is homeomorphic to mug. Let \\(C = \\{ (x,y,z) \\mid \\max (\\mid x \\mid, \\mid y \\mid, \\mid z \\mid) = 1 \\}\\) be the cube. Define \\(\\phi: C \\to S^2\\) with \\(\\phi(x,y,z) = \\frac{(x,y,z)}{\\sqrt{x^2+y^2+z^2}}\\). The inverse is \\(\\phi^{-1}(x,y,z) = \\frac{(x,y,z)} {\\max(\\mid x \\mid, \\mid y \\mid, \\mid z \\mid)}\\). Smoothness is not a topological property! . Hausdorff Spaces . Hausdorff Spaces is a physical space that respects the intuition that we can “separate” any two points. Hausdorff spaces are spaces where any two points can be “housed off”. The more formal definition is: A topological space \\(X\\) is said to be a Hausdorff space if given any points \\(p_1, p_2 \\in X\\), there exists neighborhoods \\(U_1\\) of \\(p_1\\) and \\(U_2\\) of \\(p_2\\) such that \\(U_1 \\cap U_2 = \\emptyset\\). Basis of Topological Space . In linear algebra, we always want to see how our linear map will affect the basis of our vector space. This is the same idea in topology. A collection \\(\\mathcal{B}\\) of open sets of \\(X\\) is called a basis for the topology of \\(X\\) if every open subset is the union of some collection of elements in \\(B\\). Second-countable Spaces . A topological space is second-countable if it has a countable base. This means that the topology can be generated by a countable collection of open sets. First-countable spaces are those that have a countable local base at each point. This means that for every point in the space, there exists a countable collection of open sets such that any open set containing the point contains at least one of the sets in the collection. Manifolds . An n-dimensional topological manifold is a second-countable Hausdorff space such that every point of \\(M\\) has a neighborhood that is homeomorphic to an open subset of \\(\\mathbb{R}^n\\). ",
    "url": "/blog/prereq-notes/topology.html#euclidian-topology",
    
    "relUrl": "/prereq-notes/topology.html#euclidian-topology"
  },"429": {
    "doc": "GDL 2: Topology",
    "title": "GDL 2: Topology",
    "content": " ",
    "url": "/blog/prereq-notes/topology.html",
    
    "relUrl": "/prereq-notes/topology.html"
  },"430": {
    "doc": "Variational Mechanics (1.2 - 1.3)",
    "title": "Variational Mechanics (1.2 - 1.3)",
    "content": " ",
    "url": "/blog/class-notes/00-Geometric-Mechanics/Textbook/variational-mech.html",
    
    "relUrl": "/class-notes/00-Geometric-Mechanics/Textbook/variational-mech.html"
  },"431": {
    "doc": "Vector Fields (2.1)",
    "title": "Vector Fields (2.1)",
    "content": " ",
    "url": "/blog/class-notes/00-Geometric-Mechanics/Textbook/vector-fields.html",
    
    "relUrl": "/class-notes/00-Geometric-Mechanics/Textbook/vector-fields.html"
  },"432": {
    "doc": "GNN WL-test",
    "title": "Weisfeiler-Lehman Test",
    "content": "The Weisfeiler-Lehman Test (WL-test) is a test that you can conduct to see if two graphs are isomorphic. ",
    "url": "/blog/graph-notes/wl-test-gnn.html#weisfeiler-lehman-test",
    
    "relUrl": "/graph-notes/wl-test-gnn.html#weisfeiler-lehman-test"
  },"433": {
    "doc": "GNN WL-test",
    "title": "Isomorphism Quick Description",
    "content": "First lets define what isomorphism is. Generally, an isomorphism is a mapping between two spaces such that the structure between the 2 spaces are preserved. This is kind of vague, but I have seen examples of these before. Examples of Isomorphisms: . | Homeomorphism (isomorphism of topological spaces) | Diffeomorphism (isomorphism of spaces with differentiable structure) | Symplectomorphism (isomorphism of symplectic manifolds, preserves energy) | . One super specific example that I know of: \\(\\mathrm{SO(3)} / \\mathrm{SO(2)} \\cong \\mathrm{S}^2\\) In other words, the quotient space between \\(SO(3)\\) and \\(SO(2)\\) is isomorphic to \\(S^2\\). ",
    "url": "/blog/graph-notes/wl-test-gnn.html#isomorphism-quick-description",
    
    "relUrl": "/graph-notes/wl-test-gnn.html#isomorphism-quick-description"
  },"434": {
    "doc": "GNN WL-test",
    "title": "Graph Isomorphism",
    "content": "Let \\(\\mathcal{G}_1 = (\\mathcal{V}_1, \\mathcal{E}_1)\\) be the first graph and \\(\\mathcal{G}_2 = (\\mathcal{V}_2, \\mathcal{E}_2)\\) be the second graph. An isomorphism between these graphs, \\(\\mathcal{G}_1 \\simeq \\mathcal{G}_2\\) is when there exists a mapping \\(f\\) that is isomorphic meaning: . \\[\\begin{equation}\\label{graph-isomorphic} f: V_1 \\to V_2 \\end{equation}\\] where \\(f\\) is bijective. Additionally, \\(f\\) does not affect the edge set between the graphs thus preserving the connectivity between the graphs. Then we can interpret isomorphism as two graphs that have the same connectivity. Diving deeper into the notes, you can take a look at this page . In terms of Graph Neural Networks (GNNs), graph isomorphisms are important because running the GNN layer over isomorphic graphs will return the same results. GNNs will actually still provide similar outputs even if the graphs are not isomorphic. It has been found that GNNs are at most as expressive as the WL-1 test. This is an example of a pair of graphs that are not isomorphic but fail the WL-1 test. This means the GNN output will be the same. ",
    "url": "/blog/graph-notes/wl-test-gnn.html#graph-isomorphism",
    
    "relUrl": "/graph-notes/wl-test-gnn.html#graph-isomorphism"
  },"435": {
    "doc": "GNN WL-test",
    "title": "How WL-1 test works",
    "content": "It’s a very simple algorithm. | \\(\\lambda_0\\) is the initial color. Assign this color to each vertex \\(\\lambda_0(v)\\) where \\(\\lambda_0\\) is the label of vertex \\(v\\). Just make sure you know that there is a difference between a color and a label (for us it’ll be a 2-tuple). | \\[\\forall v \\in \\mathcal{V}, \\lambda_{i+1} = (\\lambda_i(v) | \\{\\{\\lambda_i(w) | w \\in \\mathcal{N}(v)\\}\\})\\] | repeat step-2 over and over again until color distribution converges (stays the same). | run on both graphs. if the histogram is the same, then 1-WL test considers these graphs “isomorphic” meaning GNN will give same result. | . This is a figure showcasing the WL-1 test / color algorithm . There does exist higher-order WL-tests and you can design GNNs to support these, but it might not be as important. Apparently, the WL-1 test works for most graphs. ",
    "url": "/blog/graph-notes/wl-test-gnn.html#how-wl-1-test-works",
    
    "relUrl": "/graph-notes/wl-test-gnn.html#how-wl-1-test-works"
  },"436": {
    "doc": "GNN WL-test",
    "title": "GNN WL-test",
    "content": " ",
    "url": "/blog/graph-notes/wl-test-gnn.html",
    
    "relUrl": "/graph-notes/wl-test-gnn.html"
  }
}
