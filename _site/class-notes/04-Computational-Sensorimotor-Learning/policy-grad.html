<!DOCTYPE html> <html lang="en-US"> <head> <meta charset="UTF-8"> <meta http-equiv="X-UA-Compatible" content="IE=Edge"> <link rel="stylesheet" href="/blog/assets/css/just-the-docs-default.css"> <link rel="stylesheet" href="/blog/assets/css/just-the-docs-head-nav.css" id="jtd-head-nav-stylesheet"> <style id="jtd-nav-activation"> .site-nav > ul.nav-list:first-child > li > a, .site-nav > ul.nav-list:first-child > li > ul > li > a, .site-nav > ul.nav-list:first-child > li > ul > li > ul > li:not(:nth-child(3)) > a, .site-nav > ul.nav-list:first-child > li > ul > li > ul > li > ul > li a { background-image: none; } .site-nav > ul.nav-list:not(:first-child) a, .site-nav li.external a { background-image: none; } .site-nav > ul.nav-list:first-child > li:nth-child(3) > ul > li:nth-child(5) > ul > li:nth-child(3) > a { font-weight: 600; text-decoration: none; }.site-nav > ul.nav-list:first-child > li:nth-child(3) > button svg, .site-nav > ul.nav-list:first-child > li:nth-child(3) > ul > li:nth-child(5) > button svg, .site-nav > ul.nav-list:first-child > li:nth-child(3) > ul > li:nth-child(5) > ul > li:nth-child(3) > button svg { transform: rotate(-90deg); }.site-nav > ul.nav-list:first-child > li.nav-list-item:nth-child(3) > ul.nav-list, .site-nav > ul.nav-list:first-child > li.nav-list-item:nth-child(3) > ul.nav-list > li.nav-list-item:nth-child(5) > ul.nav-list, .site-nav > ul.nav-list:first-child > li.nav-list-item:nth-child(3) > ul.nav-list > li.nav-list-item:nth-child(5) > ul.nav-list > li.nav-list-item:nth-child(3) > ul.nav-list { display: block; } </style> <script src="/blog/assets/js/vendor/lunr.min.js"></script> <script src="/blog/assets/js/just-the-docs.js"></script> <meta name="viewport" content="width=device-width, initial-scale=1"> <!-- Begin Jekyll SEO tag v2.8.0 --> <title>Policy Gradients | Notes</title> <meta name="generator" content="Jekyll v4.2.2" /> <meta property="og:title" content="Policy Gradients" /> <meta property="og:locale" content="en_US" /> <meta name="description" content="Just messing around right now" /> <meta property="og:description" content="Just messing around right now" /> <link rel="canonical" href="http://localhost:4000/blog/class-notes/04-Computational-Sensorimotor-Learning/policy-grad.html" /> <meta property="og:url" content="http://localhost:4000/blog/class-notes/04-Computational-Sensorimotor-Learning/policy-grad.html" /> <meta property="og:site_name" content="Notes" /> <meta property="og:type" content="website" /> <meta name="twitter:card" content="summary" /> <meta property="twitter:title" content="Policy Gradients" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"WebPage","description":"Just messing around right now","headline":"Policy Gradients","url":"http://localhost:4000/blog/class-notes/04-Computational-Sensorimotor-Learning/policy-grad.html"}</script> <!-- End Jekyll SEO tag --> <link rel="icon" href="data:,"> <script type="text/x-mathjax-config"> MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: "AMS" } } }); </script> <script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS_CHTML"> </script> </head> <body> <a class="skip-to-main" href="#main-content">Skip to main content</a> <svg xmlns="http://www.w3.org/2000/svg" class="d-none"> <symbol id="svg-link" viewBox="0 0 24 24"> <title>Link</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-link"> <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path> </svg> </symbol> <symbol id="svg-menu" viewBox="0 0 24 24"> <title>Menu</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-menu"> <line x1="3" y1="12" x2="21" y2="12"></line><line x1="3" y1="6" x2="21" y2="6"></line><line x1="3" y1="18" x2="21" y2="18"></line> </svg> </symbol> <symbol id="svg-arrow-right" viewBox="0 0 24 24"> <title>Expand</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-chevron-right"> <polyline points="9 18 15 12 9 6"></polyline> </svg> </symbol> <!-- Feather. MIT License: https://github.com/feathericons/feather/blob/master/LICENSE --> <symbol id="svg-external-link" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-external-link"> <title id="svg-external-link-title">(external link)</title> <path d="M18 13v6a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2V8a2 2 0 0 1 2-2h6"></path><polyline points="15 3 21 3 21 9"></polyline><line x1="10" y1="14" x2="21" y2="3"></line> </symbol> <symbol id="svg-doc" viewBox="0 0 24 24"> <title>Document</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-file"> <path d="M13 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V9z"></path><polyline points="13 2 13 9 20 9"></polyline> </svg> </symbol> <symbol id="svg-search" viewBox="0 0 24 24"> <title>Search</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-search"> <circle cx="11" cy="11" r="8"></circle><line x1="21" y1="21" x2="16.65" y2="16.65"></line> </svg> </symbol> <!-- Bootstrap Icons. MIT License: https://github.com/twbs/icons/blob/main/LICENSE.md --> <symbol id="svg-copy" viewBox="0 0 16 16"> <title>Copy</title> <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-clipboard" viewBox="0 0 16 16"> <path d="M4 1.5H3a2 2 0 0 0-2 2V14a2 2 0 0 0 2 2h10a2 2 0 0 0 2-2V3.5a2 2 0 0 0-2-2h-1v1h1a1 1 0 0 1 1 1V14a1 1 0 0 1-1 1H3a1 1 0 0 1-1-1V3.5a1 1 0 0 1 1-1h1v-1z"/> <path d="M9.5 1a.5.5 0 0 1 .5.5v1a.5.5 0 0 1-.5.5h-3a.5.5 0 0 1-.5-.5v-1a.5.5 0 0 1 .5-.5h3zm-3-1A1.5 1.5 0 0 0 5 1.5v1A1.5 1.5 0 0 0 6.5 4h3A1.5 1.5 0 0 0 11 2.5v-1A1.5 1.5 0 0 0 9.5 0h-3z"/> </svg> </symbol> <symbol id="svg-copied" viewBox="0 0 16 16"> <title>Copied</title> <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-clipboard-check-fill" viewBox="0 0 16 16"> <path d="M6.5 0A1.5 1.5 0 0 0 5 1.5v1A1.5 1.5 0 0 0 6.5 4h3A1.5 1.5 0 0 0 11 2.5v-1A1.5 1.5 0 0 0 9.5 0h-3Zm3 1a.5.5 0 0 1 .5.5v1a.5.5 0 0 1-.5.5h-3a.5.5 0 0 1-.5-.5v-1a.5.5 0 0 1 .5-.5h3Z"/> <path d="M4 1.5H3a2 2 0 0 0-2 2V14a2 2 0 0 0 2 2h10a2 2 0 0 0 2-2V3.5a2 2 0 0 0-2-2h-1v1A2.5 2.5 0 0 1 9.5 5h-3A2.5 2.5 0 0 1 4 2.5v-1Zm6.854 7.354-3 3a.5.5 0 0 1-.708 0l-1.5-1.5a.5.5 0 0 1 .708-.708L7.5 10.793l2.646-2.647a.5.5 0 0 1 .708.708Z"/> </svg> </symbol> </svg> <div class="side-bar"> <div class="site-header" role="banner"> <a href="/blog/" class="site-title lh-tight"> Notes </a> <button id="menu-button" class="site-button btn-reset" aria-label="Toggle menu" aria-pressed="false"> <svg viewBox="0 0 24 24" class="icon" aria-hidden="true"><use xlink:href="#svg-menu"></use></svg> </button> </div> <nav aria-label="Main" id="site-nav" class="site-nav"> <ul class="nav-list"><li class="nav-list-item"><button class="nav-list-expander btn-reset" aria-label="toggle items in Code Notes category" aria-pressed="false"> <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg> </button><a href="/blog/code-notes/" class="nav-list-link">Code Notes</a><ul class="nav-list"><li class="nav-list-item"><a href="/blog/code-notes/how-to-pdb.html" class="nav-list-link">How-to PDB</a></li><li class="nav-list-item"><a href="/blog/code-notes/how-to-tmux.html" class="nav-list-link">How-to Tmux</a></li><li class="nav-list-item"><a href="/blog/code-notes/distributed-dl.html" class="nav-list-link">Distributed Notes</a></li><li class="nav-list-item"><a href="/blog/code-notes/image-augmentation.html" class="nav-list-link">Image Augmentation (Torchvision)</a></li><li class="nav-list-item"><button class="nav-list-expander btn-reset" aria-label="toggle items in Benchmarks category" aria-pressed="false"> <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg> </button><a href="/blog/code-notes/benchmark-notes/" class="nav-list-link">Benchmarks</a><ul class="nav-list"><li class="nav-list-item"><button class="nav-list-expander btn-reset" aria-label="toggle items in Robocasa category" aria-pressed="false"> <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg> </button><a href="/blog/code-notes/benchmark-notes/robocasa/" class="nav-list-link">Robocasa</a><ul class="nav-list"><li class="nav-list-item"><a href="/blog/code-notes/benchmark-notes/robocasa/paper.html" class="nav-list-link">Paper</a></li></ul></li><li class="nav-list-item"><button class="nav-list-expander btn-reset" aria-label="toggle items in Libero category" aria-pressed="false"> <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg> </button><a href="/blog/code-notes/benchmark-notes/libero/" class="nav-list-link">Libero</a><ul class="nav-list"><li class="nav-list-item"><a href="/blog/code-notes/benchmark-notes/libero/paper.html" class="nav-list-link">Paper</a></li><li class="nav-list-item"><a href="/blog/code-notes/benchmark-notes/libero/code.html" class="nav-list-link">Code</a></li></ul></li><li class="nav-list-item"><a href="/blog/code-notes/benchmark-notes/robomimic.html" class="nav-list-link">Robomimic</a></li></ul></li><li class="nav-list-item"><a href="/blog/code-notes/python-env.html" class="nav-list-link">Python Environments</a></li><li class="nav-list-item"><button class="nav-list-expander btn-reset" aria-label="toggle items in GROOT category" aria-pressed="false"> <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg> </button><a href="/blog/code-notes/groot/" class="nav-list-link">GROOT</a><ul class="nav-list"><li class="nav-list-item"><a href="/blog/code-notes/groot/code.html" class="nav-list-link">Code</a></li></ul></li><li class="nav-list-item"><button class="nav-list-expander btn-reset" aria-label="toggle items in Isaac category" aria-pressed="false"> <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg> </button><a href="/blog/code-notes/isaac/" class="nav-list-link">Isaac</a><ul class="nav-list"><li class="nav-list-item"><a href="/blog/code-notes/isaac/setup.html" class="nav-list-link">Isaac Setup</a></li><li class="nav-list-item"><a href="/blog/code-notes/isaac/doc_notes.html" class="nav-list-link">Isaac Gym Doc Notes</a></li><li class="nav-list-item"><a href="/blog/code-notes/isaac/gymenv-obs.html" class="nav-list-link">IsaacGymEnv Obs Notes</a></li><li class="nav-list-item"><a href="/blog/code-notes/isaac/gymenv-cam.html" class="nav-list-link">IsaacGymEnv Camera Notes</a></li><li class="nav-list-item"><a href="/blog/code-notes/isaac/gymenv-network.html" class="nav-list-link">IsaacGymEnv Network Notes</a></li><li class="nav-list-item"><a href="/blog/code-notes/isaac/gymenv-tacsl.html" class="nav-list-link">IsaacGymEnv Tacsl Notes</a></li><li class="nav-list-item"><a href="/blog/code-notes/isaac/gymenv-assets.html" class="nav-list-link">IsaacGymEnv Assets Notes</a></li><li class="nav-list-item"><a href="/blog/code-notes/isaac/gymenv-env.html" class="nav-list-link">IsaacGymEnv Env Notes</a></li><li class="nav-list-item"><a href="/blog/code-notes/isaac/gymenv-task.html" class="nav-list-link">IsaacGymEnv Task Notes</a></li><li class="nav-list-item"><a href="/blog/code-notes/isaac/rl_games_integration.html" class="nav-list-link">RL Games Coupling</a></li><li class="nav-list-item"><a href="/blog/code-notes/isaac/setup_isaacgymenvs.html" class="nav-list-link">IsaacGymEnvs Setup</a></li></ul></li><li class="nav-list-item"><button class="nav-list-expander btn-reset" aria-label="toggle items in Cluster Notes category" aria-pressed="false"> <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg> </button><a href="/blog/code-notes/cluster-notes/" class="nav-list-link">Cluster Notes</a><ul class="nav-list"><li class="nav-list-item"><a href="/blog/code-notes/cluster-notes/great-lakes.html" class="nav-list-link">Great Lakes Cluster</a></li><li class="nav-list-item"><a href="/blog/code-notes/cluster-notes/slurm-interactive.html" class="nav-list-link">SLURM Interactive</a></li><li class="nav-list-item"><a href="/blog/code-notes/cluster-notes/slurm.html" class="nav-list-link">SLURM</a></li></ul></li><li class="nav-list-item"><button class="nav-list-expander btn-reset" aria-label="toggle items in Isaac Lab category" aria-pressed="false"> <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg> </button><a href="/blog/code-notes/isaaclab/" class="nav-list-link">Isaac Lab</a><ul class="nav-list"><li class="nav-list-item"><a href="/blog/code-notes/isaaclab/setup.html" class="nav-list-link">Isaac Lab Setup</a></li><li class="nav-list-item"><a href="/blog/code-notes/isaaclab/isaacsim-start.html" class="nav-list-link">Isaac Sim Python Start</a></li><li class="nav-list-item"><a href="/blog/code-notes/isaaclab/isaacsim-hello-world.html" class="nav-list-link">IsaacSim Core (Hello World)</a></li></ul></li></ul></li><li class="nav-list-item"><button class="nav-list-expander btn-reset" aria-label="toggle items in Conference Notes category" aria-pressed="false"> <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg> </button><a href="/blog/conference-notes/" class="nav-list-link">Conference Notes</a><ul class="nav-list"><li class="nav-list-item"><button class="nav-list-expander btn-reset" aria-label="toggle items in ICRA 2025 category" aria-pressed="false"> <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg> </button><a href="/blog/conference-notes/icra-2025/" class="nav-list-link">ICRA 2025</a><ul class="nav-list"><li class="nav-list-item"><a href="/blog/conference-notes/icra-2025/workshop/beyond-pick-place.html" class="nav-list-link">Beyond Pick and Place Workshop</a></li><li class="nav-list-item"><a href="/blog/conference-notes/icra-2025/workshop/contact-rich.html" class="nav-list-link">Contact Rich Workshop</a></li><li class="nav-list-item"><a href="/blog/conference-notes/icra-2025/workshop/handy.html" class="nav-list-link">Handy Workshop</a></li><li class="nav-list-item"><a href="/blog/conference-notes/icra-2025/homework.html" class="nav-list-link">Homework</a></li><li class="nav-list-item"><a href="/blog/conference-notes/icra-2025/keynote-manipulation.html" class="nav-list-link">Manipulation Keynote</a></li><li class="nav-list-item"><a href="/blog/conference-notes/icra-2025/keynote-opt.html" class="nav-list-link">Optimization Control Keynote</a></li><li class="nav-list-item"><a href="/blog/conference-notes/icra-2025/workshop/neuro-symbolic.html" class="nav-list-link">Neuro-Symbolic Workshop</a></li><li class="nav-list-item"><a href="/blog/conference-notes/icra-2025/workshop/space.html" class="nav-list-link">Space Workshop</a></li></ul></li></ul></li><li class="nav-list-item"><button class="nav-list-expander btn-reset" aria-label="toggle items in Class Notes category" aria-pressed="false"> <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg> </button><a href="/blog/class-notes/" class="nav-list-link">Class Notes</a><ul class="nav-list"><li class="nav-list-item"><button class="nav-list-expander btn-reset" aria-label="toggle items in MATH 658: Geometric Mechanics category" aria-pressed="false"> <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg> </button><a href="/blog/class-notes/00-Geometric-Mechanics/" class="nav-list-link">MATH 658: Geometric Mechanics</a><ul class="nav-list"><li class="nav-list-item"><button class="nav-list-expander btn-reset" aria-label="toggle items in 658-Lectures category" aria-pressed="false"> <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg> </button><a href="/blog/class-notes/00-Geometric-Mechanics/Lectures/" class="nav-list-link">658-Lectures</a><ul class="nav-list"><li class="nav-list-item"><a href="/blog/class-notes/00-Geometric-Mechanics/Lectures/08-27.html" class="nav-list-link">August 27</a></li><li class="nav-list-item"><a href="/blog/class-notes/00-Geometric-Mechanics/Lectures/09-03.html" class="nav-list-link">September 3</a></li></ul></li><li class="nav-list-item"><button class="nav-list-expander btn-reset" aria-label="toggle items in Textbook category" aria-pressed="false"> <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg> </button><a href="/blog/class-notes/00-Geometric-Mechanics/Textbook/" class="nav-list-link">Textbook</a><ul class="nav-list"><li class="nav-list-item"><a href="/blog/class-notes/00-Geometric-Mechanics/Textbook/diff-manifold.html" class="nav-list-link">Differentiable Manifold (2.2)</a></li><li class="nav-list-item"><a href="/blog/class-notes/00-Geometric-Mechanics/Textbook/variational-mech.html" class="nav-list-link">Variational Mechanics (1.2 - 1.3)</a></li><li class="nav-list-item"><a href="/blog/class-notes/00-Geometric-Mechanics/Textbook/vector-fields.html" class="nav-list-link">Vector Fields (2.1)</a></li></ul></li></ul></li><li class="nav-list-item"><a href="/blog/class-notes/01-Math-of-Bio-Networks/" class="nav-list-link">MATH 540: Mathematics for Biological Networks</a></li><li class="nav-list-item"><a href="/blog/class-notes/02-Online-Learning-Control/" class="nav-list-link">AEROSP 740: Online Learning for Control</a></li><li class="nav-list-item"><a href="/blog/class-notes/03-CVOPT-Control/" class="nav-list-link">ECE 598: Convex Optimization Methods in Control</a></li><li class="nav-list-item"><button class="nav-list-expander btn-reset" aria-label="toggle items in Sensorimotor Learning category" aria-pressed="false"> <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg> </button><a href="/blog/class-notes/04-Computational-Sensorimotor-Learning/" class="nav-list-link">Sensorimotor Learning</a><ul class="nav-list"><li class="nav-list-item"><a href="/blog/class-notes/04-Computational-Sensorimotor-Learning/simple-decision.html" class="nav-list-link">1. Simple Decisions</a></li><li class="nav-list-item"><a href="/blog/class-notes/04-Computational-Sensorimotor-Learning/sequential-decision.html" class="nav-list-link">2. Sequential Decisions</a></li><li class="nav-list-item"><a href="/blog/class-notes/04-Computational-Sensorimotor-Learning/policy-grad.html" class="nav-list-link">3. Policy Gradients</a></li><li class="nav-list-item"><a href="/blog/class-notes/04-Computational-Sensorimotor-Learning/reward-function.html" class="nav-list-link">4. Reward Function</a></li><li class="nav-list-item"><a href="/blog/class-notes/04-Computational-Sensorimotor-Learning/learning-from-demo.html" class="nav-list-link">5. Learning Demos</a></li></ul></li><li class="nav-list-item"><button class="nav-list-expander btn-reset" aria-label="toggle items in Unsupervised Learning category" aria-pressed="false"> <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg> </button><a href="/blog/class-notes/05-Unsupervised-Learning/" class="nav-list-link">Unsupervised Learning</a><ul class="nav-list"><li class="nav-list-item"><a href="/blog/class-notes/05-Unsupervised-Learning/1_autoregressive.html" class="nav-list-link">1. Autoregressive</a></li><li class="nav-list-item"><a href="/blog/class-notes/05-Unsupervised-Learning/2_flow.html" class="nav-list-link">2. Flow</a></li><li class="nav-list-item"><a href="/blog/class-notes/05-Unsupervised-Learning/3_lvm.html" class="nav-list-link">3. Latent Variable Models</a></li><li class="nav-list-item"><a href="/blog/class-notes/05-Unsupervised-Learning/3a_lvm.html" class="nav-list-link">3a. LVM Papers</a></li><li class="nav-list-item"><a href="/blog/class-notes/05-Unsupervised-Learning/4_implicit_sample.html" class="nav-list-link">4. Implicit Models</a></li><li class="nav-list-item"><a href="/blog/class-notes/05-Unsupervised-Learning/5_diffusion.html" class="nav-list-link">5. Diffusion</a></li><li class="nav-list-item"><a href="/blog/class-notes/05-Unsupervised-Learning/5a_diffusion.html" class="nav-list-link">5a. Diffusion Papers</a></li><li class="nav-list-item"><a href="/blog/class-notes/05-Unsupervised-Learning/6_self_supervised.html" class="nav-list-link">6. Self-Supervised Learning</a></li></ul></li><li class="nav-list-item"><a href="/blog/class-notes/06-Flow-Matching/" class="nav-list-link">Flow Matching and Diffusion</a></li><li class="nav-list-item"><a href="/blog/class-notes/07-Deep-RL/" class="nav-list-link">Deep Reinforcement Learning</a></li><li class="nav-list-item"><a href="/blog/class-notes/08-Deep-Multitask-Learning/" class="nav-list-link">Deep Multi-Task and Meta Learning</a></li><li class="nav-list-item"><a href="/blog/class-notes/09-Algebraic-Techniques-Optimization/" class="nav-list-link">Algebraic Techniques for Optimization</a></li></ul></li><li class="nav-list-item"><button class="nav-list-expander btn-reset" aria-label="toggle items in Deep Dive Ideas category" aria-pressed="false"> <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg> </button><a href="/blog/deep-dive-notes/" class="nav-list-link">Deep Dive Ideas</a><ul class="nav-list"><li class="nav-list-item"><button class="nav-list-expander btn-reset" aria-label="toggle items in Debug Point ML category" aria-pressed="false"> <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg> </button><a href="/blog/deep-dive-notes/debug-point-ml/" class="nav-list-link">Debug Point ML</a><ul class="nav-list"><li class="nav-list-item"><a href="/blog/deep-dive-notes/debug-point-ml/pointnet.html" class="nav-list-link">PointNet</a></li><li class="nav-list-item"><a href="/blog/deep-dive-notes/debug-point-ml/spatial-transformer.html" class="nav-list-link">Spatial Transformer Networks</a></li></ul></li><li class="nav-list-item"><a href="/blog/deep-dive-notes/limit-surface/" class="nav-list-link">Limit Surface Planner</a></li><li class="nav-list-item"><button class="nav-list-expander btn-reset" aria-label="toggle items in Tactile RL Notes category" aria-pressed="false"> <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg> </button><a href="/blog/deep-dive-notes/tactile-rl-notes/" class="nav-list-link">Tactile RL Notes</a><ul class="nav-list"><li class="nav-list-item"><a href="/blog/deep-dive-notes/tactile-rl-notes/fots.html" class="nav-list-link">FOTS</a></li><li class="nav-list-item"><a href="/blog/deep-dive-notes/tactile-rl-notes/roadmap-rl.html" class="nav-list-link">Roadmap to RL</a></li><li class="nav-list-item"><a href="/blog/deep-dive-notes/tactile-rl-notes/roadmap-tactile-rl.html" class="nav-list-link">Roadmap to Tactile RL</a></li><li class="nav-list-item"><a href="/blog/deep-dive-notes/tactile-rl-notes/sim2real-tactile-rl.html" class="nav-list-link">Sim2real Tactile RL</a></li><li class="nav-list-item"><a href="/blog/deep-dive-notes/tactile-rl-notes/tacsl.html" class="nav-list-link">TacSL</a></li><li class="nav-list-item"><a href="/blog/deep-dive-notes/tactile-rl-notes/taxim.html" class="nav-list-link">Taxim</a></li></ul></li></ul></li><li class="nav-list-item"><a href="/blog/paper-notes/" class="nav-list-link">Deep Dive Papers</a></li><li class="nav-list-item"><button class="nav-list-expander btn-reset" aria-label="toggle items in Judo Notes category" aria-pressed="false"> <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg> </button><a href="/blog/judo-notes/" class="nav-list-link">Judo Notes</a><ul class="nav-list"><li class="nav-list-item"><a href="/blog/judo-notes/taught.html" class="nav-list-link">Taught Moves</a></li><li class="nav-list-item"><a href="/blog/judo-notes/tolearn.html" class="nav-list-link">To Learn</a></li></ul></li><li class="nav-list-item"><button class="nav-list-expander btn-reset" aria-label="toggle items in Kuka FRI Notes category" aria-pressed="false"> <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg> </button><a href="/blog/kuka-notes/" class="nav-list-link">Kuka FRI Notes</a><ul class="nav-list"><li class="nav-list-item"><a href="/blog/kuka-notes/code.html" class="nav-list-link">Code</a></li><li class="nav-list-item"><a href="/blog/kuka-notes/setup.html" class="nav-list-link">Setup FRI</a></li></ul></li><li class="nav-list-item"><a href="/blog/ml-git-gud-notes/" class="nav-list-link">ML (Git Gud) Resources</a></li><li class="nav-list-item"><button class="nav-list-expander btn-reset" aria-label="toggle items in PhD Project Notes category" aria-pressed="false"> <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg> </button><a href="/blog/project-notes/" class="nav-list-link">PhD Project Notes</a><ul class="nav-list"><li class="nav-list-item"><a href="/blog/project-notes/hand-notes.html" class="nav-list-link">Hand Notes</a></li><li class="nav-list-item"><button class="nav-list-expander btn-reset" aria-label="toggle items in Graph Insertion category" aria-pressed="false"> <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg> </button><a href="/blog/project-notes/project1/" class="nav-list-link">Graph Insertion</a><ul class="nav-list"><li class="nav-list-item"><button class="nav-list-expander btn-reset" aria-label="toggle items in Lit Review (Graph Insertion) category" aria-pressed="false"> <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg> </button><a href="/blog/project-notes/project1/lit_review/" class="nav-list-link">Lit Review (Graph Insertion)</a><ul class="nav-list"><li class="nav-list-item"><a href="/blog/project-notes/project1/lit_review/robopack.html" class="nav-list-link">Robopack</a></li><li class="nav-list-item"><a href="/blog/project-notes/project1/lit_review/tactile-control-extrinsic.html" class="nav-list-link">Extrinsic Contact Control</a></li></ul></li><li class="nav-list-item"><a href="/blog/project-notes/project1/math.html" class="nav-list-link">Math Notes</a></li><li class="nav-list-item"><a href="/blog/project-notes/project1/project.html" class="nav-list-link">Point RL</a></li></ul></li></ul></li><li class="nav-list-item"><button class="nav-list-expander btn-reset" aria-label="toggle items in Prerequisites category" aria-pressed="false"> <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg> </button><a href="/blog/prereq-notes/" class="nav-list-link">Prerequisites</a><ul class="nav-list"><li class="nav-list-item"><a href="/blog/prereq-notes/calc-variation.html" class="nav-list-link">Calculus of Variations - Chill</a></li><li class="nav-list-item"><a href="/blog/prereq-notes/calc-variation2.html" class="nav-list-link">Calculus of Variations - Serious</a></li><li class="nav-list-item"><a href="/blog/prereq-notes/groups.html" class="nav-list-link">GDL 1: Groups</a></li><li class="nav-list-item"><a href="/blog/prereq-notes/topology.html" class="nav-list-link">GDL 2: Topology</a></li><li class="nav-list-item"><a href="/blog/prereq-notes/manifolds.html" class="nav-list-link">GDL 3: Manifolds</a></li></ul></li><li class="nav-list-item"><button class="nav-list-expander btn-reset" aria-label="toggle items in Quick Notes on Graphs category" aria-pressed="false"> <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg> </button><a href="/blog/graph-notes/" class="nav-list-link">Quick Notes on Graphs</a><ul class="nav-list"><li class="nav-list-item"><a href="/blog/graph-notes/advective-diff.html" class="nav-list-link">Advective Diffusion</a></li><li class="nav-list-item"><a href="/blog/graph-notes/beyond-wl.html" class="nav-list-link">Beyond WL-test</a></li><li class="nav-list-item"><a href="/blog/graph-notes/wl-test-gnn.html" class="nav-list-link">GNN WL-test</a></li><li class="nav-list-item"><a href="/blog/graph-notes/gnn-grad-flow.html" class="nav-list-link">GNN as Gradient Flow</a></li><li class="nav-list-item"><a href="/blog/graph-notes/gnn-pde.html" class="nav-list-link">GNNs as PDEs</a></li><li class="nav-list-item"><a href="/blog/graph-notes/graph-rewiring.html" class="nav-list-link">Graph Rewiring</a></li><li class="nav-list-item"><a href="/blog/graph-notes/latent-gnn.html" class="nav-list-link">Latent GNNs</a></li><li class="nav-list-item"><a href="/blog/graph-notes/neural-sheaf-diffusion.html" class="nav-list-link">Neural Sheaf Diffusion</a></li><li class="nav-list-item"><a href="/blog/graph-notes/deep-gnn.html" class="nav-list-link">Scaling GNNs</a></li><li class="nav-list-item"><a href="/blog/graph-notes/temporal-gnn.html" class="nav-list-link">Temporal GNNs</a></li></ul></li></ul> </nav> <footer class="site-footer"> This site uses <a href="https://github.com/just-the-docs/just-the-docs">Just the Docs</a>, a documentation theme for Jekyll. </footer> </div> <div class="main" id="top"> <div id="main-header" class="main-header"> <div class="search" role="search"> <div class="search-input-wrap"> <input type="text" id="search-input" class="search-input" tabindex="0" placeholder="Search Notes" aria-label="Search Notes" autocomplete="off"> <label for="search-input" class="search-label"><svg viewBox="0 0 24 24" class="search-icon"><use xlink:href="#svg-search"></use></svg></label> </div> <div id="search-results" class="search-results"></div> </div> </div> <div class="main-content-wrap"> <nav aria-label="Breadcrumb" class="breadcrumb-nav"> <ol class="breadcrumb-nav-list"> <li class="breadcrumb-nav-list-item"><a href="/blog/class-notes/">Class Notes</a></li> <li class="breadcrumb-nav-list-item"><a href="/blog/class-notes/04-Computational-Sensorimotor-Learning/">Sensorimotor Learning</a></li> <li class="breadcrumb-nav-list-item"><span>3. Policy Gradients</span></li> </ol> </nav> <div id="main-content" class="main-content"> <main> <h1 id="3-policy-gradients"> <a href="#3-policy-gradients" class="anchor-heading" aria-labelledby="3-policy-gradients"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 3. Policy Gradients </h1> <p>Policy gradients are used to solve SDM problems. We take the gradient of the sum of rewards with respect to policy parameters. Using gradient ascent, we can find the optimal policy.</p> <h2 id="31-derivation-and-properties-of-policy-gradient"> <a href="#31-derivation-and-properties-of-policy-gradient" class="anchor-heading" aria-labelledby="31-derivation-and-properties-of-policy-gradient"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 3.1 Derivation and Properties of Policy Gradient </h2> \[\begin{equation} \max_{\theta} J(\theta) = \mathbb{E}_{\pi_{\theta}} \left[ R^{\gamma}(\tau) \right] \end{equation}\] <p>where \(R^{\gamma}(\tau)\) denotes the discounted return of a trajectory \(\tau\) and \(\pi_{\theta}\) denotes policy parametrized by \(\theta\).</p> <p>Let \(p_{\theta}(\tau)\) be probability of trajectory \(\tau\) being sampled. We express gradient of expected return \(\nabla_{\theta} J(\theta)\) as:</p> \[\begin{align*} \nabla_{\theta} J(\theta) &amp;= \nabla_{\theta} \mathbb{E}_{\pi_{\theta}} \left[ R^{\gamma}(\tau) \right] \\ &amp;= \nabla_{\theta} \int_{\tau} p_{\theta}(\tau) R^{\gamma}(\tau) d\tau \\ &amp;= \int_{\tau} \nabla_{\theta} p_{\theta}(\tau) R^{\gamma}(\tau) d\tau \\ &amp;= \int_{\tau} p_{\theta}(\tau) \frac{\nabla_{\theta} p_{\theta}(\tau)}{p_{\theta}(\tau)} R^{\gamma}(\tau) d\tau \\ &amp;= \int_{\tau} p_{\theta}(\tau) \nabla_{\theta} \log p_{\theta}(\tau) R^{\gamma}(\tau) d\tau \\ &amp;= \mathbb{E}_{\pi_{\theta}} \left[ \nabla_{\theta} \log (p_\theta (\tau)) R^\gamma (\tau) \right] \end{align*}\] <p>using the log-derivative trick, we can move the gradient inside the expectation. Now the calculation of the gradient is reduced to calculating the gradient of the log-likelihood of the trajectory. Gradient ascent can be calculated as follows \(\theta \gets \theta + \alpha \nabla_{\theta} J(\theta)\).</p> <p><b> Remark 3.1 </b> (Continuous action spaces). Policy gradients make no assumption about action spaces. We can use any choice of probabilistic distribution for \(\pi\) and sample \(a_t\) from \(\pi\).</p> <p><b> Remark 3.2 </b> (Model-free). We can rewrite \(\nabla_{\theta} J(\theta)\) as:</p> \[\begin{equation} \mathbb{E}_{\pi_{\theta}} \left[ \nabla_{\theta} \log (p_\theta (\tau)) R^\gamma (\tau) \right] = \mathbb{E}_{\pi_{\theta}} \left[ \nabla_{\theta} \sum_{t=1}^T \log (\pi_\theta (a_t \mid s_{1:t}, a_{1:t}))R^\gamma(\tau_{1:T}) \right] \end{equation}\] <p>We can see that this is independent of the state transition model \(P\). We say that this is model-free. This means that we estimate policy gradients by sampling trajectories from an environment.</p> <p><b> Remark 3.3 </b> (No Markov Assumption). Markov assumption is described as follows:</p> \[\begin{equation} \pi(a_t \mid s_{1:t}, a_{1:t}) = \pi(a_t \mid s_t) \end{equation}\] <p>a lot of algorithms assume that the policy is Markovian. However, this is not necessary in this case. We can make our policy gradient algorithm work with Markovian policies if we want to.</p> <h2 id="32-reinforce"> <a href="#32-reinforce" class="anchor-heading" aria-labelledby="32-reinforce"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 3.2 REINFORCE </h2> <p>The above policy gradient derivation is also known as REINFORCE. However, we just described the update step. These are the exact steps of the algorithm:</p> <ol> <li>Roll out trajectory \(\tau^i\) using policy \(\pi_{\theta}\).</li> <li>Compute gradient of log-likelihood of all actions \(g^i = \nabla_\theta \log \pi_\theta (\cdot \mid \cdot)\).</li> <li>Weigh gradient by corresponding returns \(g^i R^\gamma (\tau^i)\).</li> <li>Update policy parameters \(\theta \gets \theta + \alpha g^i R^\gamma (\tau^i)\).</li> </ol> <p>We can rollout multiple trajectories and average the gradients. This is called the Monte Carlo estimate of the gradient.</p> <p>If we assume markov property, we can rewrite the steps as:</p> <ol> <li>Collect \(N\) trajectories \(\{ \tau_{1:T}^i \}_{i=1}^N\) with policy \(\pi_{\theta}\).</li> <li>Compute gradient \(\nabla_\theta J(\theta) = \frac{1}{N} \sum_{i=1}^N \sum_{t=1}^T \nabla_\theta \log \pi (a \mid s^i_t) R^\gamma (\tau^i_{1:t})\).</li> <li>update policy parameters \(\theta \gets \theta + \alpha \nabla_\theta J(\theta)\).</li> </ol> <h2 id="33-credit-assignment"> <a href="#33-credit-assignment" class="anchor-heading" aria-labelledby="33-credit-assignment"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 3.3 Credit Assignment </h2> <p>The idea of weighing the gradient of the policy by the return is called credit assignment. Unfortunately, credit assignment is not easy because of the high variance in sampled trajectory returns \(R^\gamma (\tau)\). The reason why this is the case is that it is challenging to distinguish contribution of eacha ction to the observed return. This is one of the issues that come with model-free approaches. Normally in model-based approaches, the gradient of the transition model can allow us to figure out the contribution of each action to the observed return.</p> <p>The issue still stands and it only gets worse as trajectory length increases.</p> <p><b> Discount </b> We can use discount factor to reduce the variance of the return. A lower discount factor anneals the variance of the return since the total magnitude of the return is reduced. However, too small of a discount can lead to suboptimal policies.</p> <p><b> Baseline </b> We can introduce a baseline to increase probability of trajectories with above-average returns and decrease that with below-average returns (baseline is average return). We increase probabilities of trajectories with “advantage”. Advantage quantifies gap between return of trajectory \(R^\gamma (\tau)\) to the average trajectory return \(b(s_t) = \mathbb{E}_{\pi_{\theta}} \left[ R^\gamma (\tau) \right]\). With notion of advantage, we re-write policy gradient as:</p> \[\begin{equation} \sum_{i=1}^N \sum_{t=1}^T \nabla_\theta \log \pi (a \mid s_t^i) (R^\gamma (\tau^i_{1:t}) - b(s_t^i)) \end{equation}\] <p>where \(R^\gamma (\tau^i_{1:t}) - b(s_t^i)\) is the advantage of the action \(a_t\).</p> <p>A fundamental reason that baselines reduce variance is because of the following inequality:</p> \[\begin{equation} \text{Var} \left[ \nabla_\theta \log(p_\theta(\tau))(R^\gamma(\tau) - b(\tau)) \right] \leq \text{Var} \left[ \nabla_\theta \log(p_\theta(\tau))R^\gamma(\tau) \right] \end{equation}\] <p>which only holds when \(R^\gamma(\tau)\) and \(b(\tau)\) are correlated. If we assume both of these are random variables, then the variance of the difference of two random variables is described as:</p> \[\begin{equation} \text{Var} \left[ X - Y \right] = \text{Var} \left[ X \right] + \text{Var} \left[ Y \right] - 2\text{Cov} \left[ X, Y \right] \end{equation}\] <p>if \(2\text{Cov} \left[ X, Y \right] &gt; \text{Var} \left[ Y \right]\) (i.e. X and Y correlate), then \(\text{Var} \left[ X - Y \right] &lt; \text{Var} \left[ X \right]\). This is because the term subtracts from the variance of \(X\) more than the contribution of the variance of \(Y\).</p> <p>Here are some common choices of baselines:</p> <ul> <li><strong>Weighted average</strong>: \(b(s_t) = \frac{\mathbb{E} \left[ \| \nabla_\theta \log \pi_\theta(a \mid s_t) \|_2^2 R(\tau_{1:T}) \right]}{ \| \mathbb{E} \left[ \nabla_\theta \log \pi_\theta (a \mid s_t) \|_2^2 \right] }\)</li> <li><strong>Value function</strong>: \(V^\pi (s_t) = \mathbb{E}_{\pi_\theta} \left[ R^\gamma (\tau_{1:T}) \right]\)</li> </ul> <p>Calculating the value function is a bit tricky. We can parametrize the value function with another model (separate from policy model) \(V^\pi_\psi\). We then minimize the expected magnitude of advantage with respect to \(\psi\):</p> \[\begin{equation} \min_\psi \mathbb{E} \left[ \| R^\gamma(\tau_{t:T}) - V^\pi_\psi (s_t) \|_2^2 \right] \end{equation}\] <p>where \(R^\gamma(\tau_{t:T})\) is the monte carlo estimation of expected return of a trajectory starting from \(s_t\).</p> <p><b> Sample size </b> Another way to reduce variance is to increase number of trajectories sampled \(N\). However, we know that this is not always possible and can be too expensive, especially considering that we are sampling \(N\) trajectories for each gradient update.</p> <h2 id="34-actor-critic"> <a href="#34-actor-critic" class="anchor-heading" aria-labelledby="34-actor-critic"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 3.4 Actor-Critic </h2> <p>Actor-critic method uses the value function to replace MC estimates of trajectory returns \(R^\gamma(\tau)\) and reduce variance of the policy gradient. This section goes over the <b> Advantage Actor-Critic </b> method and the <b> Generalized Advantage Estimation </b> (GAE) method which balances bias-variance trade-off between MC estimation and value predictions.</p> <h2 id="341-advantage-actor-critic-a2c"> <a href="#341-advantage-actor-critic-a2c" class="anchor-heading" aria-labelledby="341-advantage-actor-critic-a2c"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 3.4.1 Advantage Actor-Critic (A2C) </h2> <p>A2C smooths estimates of traj return by averaging which reduces variance of policy gradient. The reason to smooth is because it is unlikely to obtain samples of trajectory returns from exactly the same state. As a result, we won’t get sufficient samples of trajectory returns starting from the same state.</p> <p>Taking average trajectory returns from similar states will reduce the variance of estimating the trajectory return starting from state \(s\).</p> <p>To smooth estimates of \(R^\gamma(\tau)\), A2C replaces the MC estimates of trajectory returns \(R^\gamma(\tau)\) starting from state \(s_t\) with:</p> \[\begin{equation} R^\gamma(\tau_{t:T}) = Q^\pi(s_t,a_t) = \mathbb{E} [ \sum_{i=t}^T \gamma^{i-t} r(s_i, a_i) \mid s_t, a_t ] \end{equation}\] <p>where \(Q\) is the expected return of taking action \(a_t\) in state \(s_t\). \(\mathbb{E}[\cdot \mid s_t, a_t]\) means \(s_t\) and \(a_t\) are fixed. \(Q\) can be obtained from the value function baseline \(V^\pi\) by constructing \(Q^\pi\) in a recursive way:</p> \[\begin{align*} Q^\pi(s_t,a_t) &amp;= \mathbb{E} \left[ \sum_{i=t}^T \gamma^{i-t} r(s_i,a_i) \mid s_t, a_t \right] \\ &amp;= \mathbb{E} \left[ r(s_t, a_t) + \gamma V^\pi(s_{t+1}) \right] \\ &amp;\approx \mathbb{E} \left[ r(s_t,a_t) + \gamma V^\pi_\psi (s_{t+1}) \right] \end{align*}\] <p>this is assuming \(V^\pi \approx V^\pi_\psi\).</p> <p>Now we can write objective of policy gradient:</p> \[\begin{align*} &amp;\sum_{i=1}^N \sum_{t=1}^T \nabla_\theta \log \pi(a \mid s_t^t) (R^\gamma (\tau_{t:T}) - V^\pi_\psi(s_t)) \\ &amp;= \sum_{i=1}^N \sum_{t=1}^T \nabla_\theta \log \pi(a \mid s_t^t) (r(s_t,a_t) + \gamma V^\pi_\psi (s_{t+1}) - V^\pi_\psi (s_t)) \\ &amp;= \sum_{i=1}^N \sum_{t=1}^T \nabla_\theta \log \pi(a \mid s_t^t) A(s_t,a_t) \end{align*}\] <p>where \(A(s_t,a_t) = Q^\pi(s_t,a_t) - V^\pi_\psi(s_t)\) is the advantage.</p> <p><b> Reminder </b> all of this is for calculating the policy gradient if you haven’t been following along.</p> <p>Using this calculation in our algorithm is called <b> Advantage Actor-Critic </b> (A2C).</p> <p>The MC estimates of \(R^\gamma(\tau_{t:T})\) has high randomness, while the randomness of \(r(s_t,a_t) + \gamma V^\pi_\psi(s_{t+1})\) only comes from \(r(s_t,a_t)\) since the value function is deterministic. This eliminates variance but introduces bias since \(V^\pi_\psi\) is not the true value function and has underlying error.</p> <p>Formally, we can write out the bias-variance trade-off. Consider an unbiased estimate \(h\):</p> \[\begin{equation*} \text{Bias}(h) = \mathbb{E} [ X - h(X) ] = \mathbb{E} [ X ] - \mathbb{E} [ h(X) ] = 0 \end{equation*}\] <p>where \(X\) is the random variable and \(h(X)\) is the estimate to \(X\). In this case, \(X\) is \(R^\gamma(\tau_{t:T})\) and \(h(X)\) is estimate of \(R^\gamma(\tau_{t:T})\). Let’s check out if MC estimation is unbiased estimate:</p> \[\begin{equation*} \mathbb{E} \left[ \frac{1}{N} \sum_{i=1}^N R^\gamma (\tau_{t:T}^i) \right] = \frac{1}{N} \sum_{i=1}^N \mathbb{E} [ R^\gamma (\tau_{t:T}^i)] = \frac{1}{N} \cdot N \mathbb{E} [ R^\gamma(\tau^i_{t:T})] = \mathbb{E} [ R^\gamma(\tau_{t:T})] \end{equation*}\] <p>The MC estimation method above is unbiased.</p> <p>Now let’s check out our A2C method:</p> \[\require{cancel} \begin{equation*} \mathbb{E} \left[ \frac{1}{N} \sum_{i=1}^N (r(s_t^i) + \gamma V^\pi_\psi) (s^i_{t+1}) \right] = \frac{1}{N} \sum_{i=1}^N \mathbb{E} [ r(s_t^i, a_t^i)] + \gamma \frac{1}{N} \mathbb{E} [ V^\pi_\psi (s^i_{t+1})] \cancel{=} \mathbb{E} [ R^\gamma (\tau_{t:T})] \end{equation*}\] <p>as we can see, there is no way A2C is unbiased.</p> <h2 id="342-generalized-advantage-estimation-gae"> <a href="#342-generalized-advantage-estimation-gae" class="anchor-heading" aria-labelledby="342-generalized-advantage-estimation-gae"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 3.4.2 Generalized Advantage Estimation (GAE) </h2> <p>GAE balances bias and variance by mixing MC and approximated estimation. Consider \(r(s_t,a_t) + \gamma V^\pi_\psi (s_{t+1})\) which is a biased estimate. One way to suppress variance in advantage estimation is incorporating more steps of rewards sampled from invronment. This is what we mean if we incorporate one more step:</p> \[\begin{equation*} A^1(s_t,a_t) = r(s_t,a_t) + \gamma r(s_{t+1}, a_{t+1}) + \gamma^2 V^\pi_\psi (s_{t+2}) - V^\pi_\psi (s_t) \end{equation*}\] <p>We can generalize this to \(n\) steps:</p> \[\begin{equation*} A^n(s_t,a_t) = \sum_{i=0}^{n} \gamma^{i-1} r(s_{t+i}, a_{t+i}) + \gamma^{n+1} V^\pi_\psi (s_{t+n+1}) - V^\pi_\psi (s_t) \end{equation*}\] <p>How to choose \(n\)? Instead of picking, GAE uses exponential weighted average to mix all \(A^i\) with a parameter \(\lambda\) which gives us:</p> \[\begin{equation} A^{\text{GAE}(\gamma, \lambda)}(s_t, a_t) = \sum_{i=1}^T (\gamma \lambda)^i A^i(s_t, a_t) \end{equation}\] <p>where \(\lambda\) controls bias and variance. If \(\lambda = 0\), then we only use 1-step advantage estimation. If \(\lambda = 1\), then we are doing MC estimation.</p> <h2 id="35-exploration-and-data-diversity"> <a href="#35-exploration-and-data-diversity" class="anchor-heading" aria-labelledby="35-exploration-and-data-diversity"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 3.5 Exploration and Data Diversity </h2> <p><b> Asynchronous sampling </b> A3C (Asynchronous Advantage Actor-Critic) is a variant of A2C that is asynchronous. It collects multiple trajectories in parallel using copies of the same policy. A3C consists of one master and multiple workers. This allows us to increase sample size.</p> <p>Each worker fetches master’s policy parameters asynchronously, computes policygradient using collected trajectories and sends gradient back to master thread. So what is the correctness of accumulating these gradients? Still leads to satisfactory performance.</p> <p><b> Entropy Regularization </b> Regularize policy gradients with entropy to increase diversity of actions executed by agent. Regularized objective is:</p> \[\begin{align} \sum_{i=1}^N \sum_{t=1}^T \nabla_\theta \log \pi(a \mid s_t^i) A(s_t, a_t) + \mathcal{H}(\pi(a \mid s_t)) \\ \mathcal{H}(\pi(a \mid s_t)) = -\sum_{a \in \mathcal{A}} \pi(a \mid s_t) \log \pi(a \mid s_t) \end{align}\] <p>\(\mathcal{H}\) is the entropy and we just take the entropy of the policy. This term encourages exploration by making parameters want to increase the entropy of the policy.</p> <h2 id="36-conservative-policy-optimization"> <a href="#36-conservative-policy-optimization" class="anchor-heading" aria-labelledby="36-conservative-policy-optimization"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 3.6 Conservative Policy Optimization </h2> <p>Now lets think about our policy gradient problem. Since our data collection comes from the same policy, a bad policy may collect bad data. It’s a bad feedback loop where we may not be able to get a good policy. One way to get around this is to ensure our updates are conservative. We do this to make sure that we don’t get into this bad feedback loop. This conservative update correlates to finding a conservative gradient step.</p> <h2 id="361-parameter-space-constraint"> <a href="#361-parameter-space-constraint" class="anchor-heading" aria-labelledby="361-parameter-space-constraint"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 3.6.1 Parameter Space Constraint </h2> <p>One way to do this is to re-formulate our optimization problem:</p> \[\begin{equation} \max_{\theta} J(\theta) \\ \text{s.t.} \quad \| \theta \|_2^2 \leq \epsilon \end{equation}\] <p>We can re-cast this problem as:</p> \[\begin{equation} \max_{d} J(\theta + d) \\ \text{s.t.} \quad \| d \|_2^2 \leq \epsilon \end{equation}\] <p>we can get the Lagrangian of this problem:</p> \[\begin{equation} \max_{d} J(\theta + d) - \lambda \| d \|_2^2 \end{equation}\] <p>Since \(J(\theta + d)\) is unknown, we can use a first-order approximation:</p> \[\begin{equation*} J(\theta + d) \approx J(\theta) + d^T \nabla J(\theta) + O(d^2) \end{equation*}\] <p>Using this approximation, we can write the Lagrangian as:</p> \[\begin{equation} \max_{d} J(\theta) + d^T \nabla J(\theta) - \lambda \| d \|_2^2 \end{equation}\] <p>if we set the gradient with respect to \(d\) to zero, we get:</p> \[\begin{align*} 0 &amp;= \nabla J(\theta) - 2\lambda d \\ d &amp;= \frac{1}{2\lambda} \nabla J(\theta) \end{align*}\] <p>which is just vanilla gradient ascent but we have a constraint on the step size or learning rate. Additionally, we want to make sure we follow this:</p> \[\begin{equation*} \| \beta d^* \|_2^2 \leq \epsilon \end{equation*}\] <p>which implies \(\beta = \sqrt{\frac{\epsilon}{\| d^* \|_2^2}}\). And thus our gradient step is:</p> \[\begin{equation} \theta \gets \theta + \beta d^* = \theta + \sqrt{\frac{\epsilon}{\| d^* \|_2^2}} d^* \end{equation}\] <p>Another thing to note is how to pick \(\epsilon\). We need to pick it such that the policy change is within a desired range. One thing to note is that the parameter difference depends on the current policy parametrizations and is not fixed.</p> <h2 id="362-parametrization-independent-constraints-on-output-space"> <a href="#362-parametrization-independent-constraints-on-output-space" class="anchor-heading" aria-labelledby="362-parametrization-independent-constraints-on-output-space"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 3.6.2 Parametrization-independent Constraints on Output Space </h2> <p>This is where <b> Natural Policy Gradient (NPG) </b> comes in. We motivate NPG by looking at our constraints again:</p> \[\begin{align*} d^T d \leq \epsilon \\ \sum_{i,j} \mathbf{I}_{ij} d_i d_j \leq \epsilon \end{align*}\] <p>we want to say something like \(\| \partial \log \pi_\theta(a \mid s) \|_2^2 \leq \epsilon\) rather than \(\| d \|_2^2 \leq \epsilon\). This is more direct way of talking about small policy changes. We can do this by including the Fisher information matrix into the constraints above:</p> \[\begin{align*} d^T F d \leq \epsilon \\ \sum_{i,j} \frac{\partial \log \pi_\theta (a \mid s)}{\partial \theta_i} \frac{\partial \log \pi_\theta(a \mid s)}{\partial \theta_j} d_i d_j \leq \epsilon \end{align*}\] <p>where \(\mathbf{F}(\theta)\) is the Fisher information matrix:</p> \[\begin{equation} \mathbf{F}(\theta) = \mathbb{E}_{\pi_\theta (a \mid s)} \left[ \nabla_\theta \log \pi_\theta(a \mid s) \nabla_\theta \log \pi_\theta(a \mid s)^T \right] \end{equation}\] <p>which is the covariance of the gradient of the log-likelihood of the policy (expected value of outer product).</p> <p>As \(\frac{\partial \log \pi_\theta (a \mid s)}{\partial \theta_i} \frac{\partial \log \pi_\theta(a \mid s)}{\partial \theta_j} d_i d_j\) has unit where \(d_i = \partial \theta_i\) and \(d_j = \partial \theta_j\), we can see that:</p> \[\begin{equation} \frac{\partial \log \pi_\theta(a \mid s) \partial \log \pi_\theta(a \mid s) \partial \theta_i \partial \theta_j} {\partial \theta_i \partial \theta_j} = (\partial \log \pi_\theta(a \mid s))^2 \end{equation}\] <p>which makes the constraint pretty much \(\sum_{i,j} (\partial \log \pi_\theta(a \mid s))^2 \leq \epsilon\). This makes the constraint parametrization independent. Lots of rigorous analysis in information geometry to get this result. We just use this. Turns out Fisher information matrix is the only way to get parametrization independent metric.</p> <p>Now we can write our final optimization problem:</p> \[\begin{equation} \max_{d} J(\theta + d) \\ \text{s.t.} \quad d^T \mathbf{F}(\theta) d \leq \epsilon \end{equation}\] <p>How do we solve this? We do something similar to before where we use first-order approximation:</p> \[\begin{equation*} J(\theta + d, \lambda) \approx J(\theta) + d^T \nabla J(\theta) - \lambda (\frac{1}{2} d^T \mathbf{F}(\theta) d - \epsilon) \end{equation*}\] <p>then set gradient to zero and solve:</p> \[\begin{align*} 0 &amp;= \nabla J(\theta) - \lambda \mathbf{F}(\theta) d \\ d^* &amp;= \frac{1}{\lambda} \mathbf{F}(\theta)^{-1} \nabla J(\theta) \end{align*}\] <p>Now we just need to tighten the constraint \(d^T \mathbf{F}(\theta) d \leq \epsilon\) just like before using \(\beta\). We do this by plugging \(d^*\) into constriant:</p> \[\begin{align*} &amp;\frac{\lambda^2}{2} (d^*)^T \mathbf{F}(\theta) d^* \\ &amp;= \frac{\lambda^2}{2} (\beta \mathbf{F}(\theta)^{-1} \nabla J(\theta))^T \mathbf{F}(\theta) (\beta \mathbf{F}(\theta)^{-1} \nabla J(\theta)) \\ &amp;= \frac{\lambda^2 \beta^2}{2} \nabla J(\theta)^T \mathbf{F}(\theta)^{-T} \mathbf{F}(\theta) \mathbf{F}(\theta)^{-1} \nabla J(\theta) \\ &amp;= \frac{\lambda^2 \beta^2}{2} \nabla J(\theta)^T \mathbf{F}(\theta)^{-1} \nabla J(\theta) \leq \epsilon \end{align*}\] <p>we use knowledge that \(\mathbf{F}(\theta)^{-T} = \mathbf{F}(\theta)^{-1}\). Now we can solve for \(\beta\) to find the value that tightens inequality:</p> \[\begin{equation} \beta^* = \frac{1}{\lambda} \sqrt{\frac{2\epsilon}{\nabla J(\theta)^T \mathbf{F}(\theta)^{-1} \nabla J(\theta)}} \end{equation}\] <p>same as before, we can plug this into our gradient step:\(\theta \gets \theta + \beta^* d^*\)</p> <p><b> Connection to Kullback-Leibler (KL) Divergence </b> The second-order expansion of KL-divergence is Fisher information matrix, thus NPG can be rewritten as KL-divergence:</p> \[\begin{equation} \max_{d} J(\theta + d) \\ \text{s.t.} \quad D_{KL}(\pi_\theta(a \mid s) || \pi_{\theta + d}(a \mid s)) \leq \epsilon \end{equation}\] <p><b> Connection to Newton Method </b> NPG is very similar to Newton’s method for second-order optimizaiton:</p> \[\begin{align*} \theta &amp;\gets \theta + \beta \mathbf{H}^{-1}(\theta) \nabla J(\theta) \\ \theta &amp;\gets \theta + \beta \mathbf{F}(\theta)^{-1} \nabla J(\theta) \end{align*}\] <p>where the first expression is Newton’s method (\(\mathbf{H}(\theta)\) is hessian) and second expression is NPG. Since we know that Fisher information matrix is the second-order approximation of KL-divergence, we can see that NPG is Newton’s method of the KL-divergence problem.</p> <h2 id="363-monotonic-policy-improvement"> <a href="#363-monotonic-policy-improvement" class="anchor-heading" aria-labelledby="363-monotonic-policy-improvement"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 3.6.3 Monotonic Policy Improvement </h2> <p>While NPG ensures change in policy bounded by \(\epsilon\), it does not guarantee that each policy update will improve policy performance measured by returns. <b> Trust Region Policy Optimization </b> (TRPO) mitigates this by maximizing policy improvement instead of just constraining changes in policy outputs. If policy improvement is guaranteed, then agent will not get stuck in poor local maxima and avoid the bad feedback loop (vicious cycle).</p> <p>We describe the problem as:</p> \[\begin{equation} \max_\theta J(\theta + d) - J(\theta) \end{equation}\] <p>where the difference quantifies policy improvement of \(\pi_{\theta + d}\) with respect to old policy \(\pi_\theta\). We can show policy improvement can be rewritten as:</p> \[\begin{equation*} J(\theta + d) - J(\theta) = \mathbb{E}_{\pi \sim \pi_{\theta + d}} \left[ \sum_{t=1}^T \gamma^{t-1} A^{\pi_\theta}(s_t,a_t) \right] \end{equation*}\] <p>with this in mind, we restate optimization as:</p> \[\begin{equation*} \max_d \mathbb{E}_{\tau \sim \pi_{\theta + d}} \left[ \sum_{t=1}^T \gamma^{t-1} A^{\pi_\theta}(s_t,a_t) \right] \end{equation*}\] <p>We can break down this equation as follows:</p> \[\begin{align*} \mathbb{E}_{\tau \sim \pi_{\theta + d}} \left[ \sum_{t} \gamma^{t-1} A^{\pi_\theta}(s_t,a_t) \right] &amp;= \sum_{s,a} \sum_t P(s=s_t,a=a_t \mid \pi_{\theta + d})[\gamma^{t-1} A^{\pi_\theta}(s,a)] \\ &amp;= \sum_t \sum_{s,a} \gamma^{t-1} P(s=s_t,a=a_t \mid \pi_{\theta + d}) [A^{\pi_\theta}(s,a)] \\ &amp;= \sum_t \sum_s \gamma^{t-1} P(s_t=s \mid \pi_{\theta + d}) \sum_a \pi_{\theta + d}(a_t = a \mid s_t) [ A^{\pi_\theta}(s,a) ] \\ &amp;= \sum_s \rho^\gamma_{\pi_{\theta + d}}(s) \sum_a \pi_{\theta + d} (a \mid s) [A^{\pi_\theta}(s,a)] \\ &amp;= \sum_s \rho^\gamma_{\pi_{\theta + d}}(s) \sum_s \pi_\theta(a \mid s) \frac{\pi_{\theta + d}(a \mid s)}{\pi_\theta(a \mid s)} [ A^{\pi_\theta}(s,a) ] \\ &amp;= \mathbb{E}_{s \sim \pi_{\theta + d}, a \sim \pi_\theta} \left[ \frac{\pi_{\theta + d}(a \mid s)}{\pi_\theta(a \mid s)} A^{\pi_\theta}(s,a) \right] \end{align*}\] <p>where \(\rho^\gamma_{\pi_{\theta + d}}(s) = \sum_t \gamma^{t-1} P(s=s_t \mid \pi_{\theta + d})\).</p> <p>We can then rewrite the optimization problem as:</p> \[\begin{equation} \max_d \mathbb{E}_{s \sim \pi_{\theta + d}, a \sim \pi_\theta} \left[ \frac{\pi_{\theta + d}(a \mid s)}{\pi_\theta(a \mid s)} A^{\pi_\theta}(s,a) \right] \end{equation}\] <p>since it requires sampling from the new policy \(\pi_{\theta + d}\), there is an inefficiency there (need rollouts in environment with new policy). We can approximate the performance of the new policy using state distribution of old policy \(\pi_\theta\):</p> \[\begin{equation} J(\theta + d) \geq \hat J_\theta (\theta + d) - C \cdot D_{KL}^{\max}(\pi_\theta || \pi_{\theta + d}(a \mid s)) \end{equation}\] <p>where \(C = \frac{4 \epsilon \gamma}{(1-\gamma)^2}\), \(D^{\max}_{KL} = \max_s D_{KL}\), and \(\hat J(\theta + d) = \mathbb{E}_{s \sim \pi_\theta, a \sim \pi_{\theta + d}} [A^{\pi_\theta}(s,a)]\).</p> <p><b> KL Divergence </b>:</p> \[\begin{align} D_{KL}(P || Q) &amp;= \sum_{x \in \mathcal{X}} p(x) \log \frac{p(x)}{q(x)} \\ &amp;= \mathbb{E}_{x \sim P} \left[ \log\frac{p(x)}{q(x)} \right] \end{align}\] <p>Using this new approximation, we can rewrite the optimization problem as:</p> \[\begin{equation} \max_d \mathbb{E}_{s \sim \pi_\theta, a \sim_\theta} \left[ \frac{\pi_{\theta + d}(a \mid s)}{\pi_\theta (a \mid s)}A^{\pi_\theta}(s,a) \right] - C D_{KL}^\max(\pi_\theta(a \mid s) || \pi_{\theta + d}(a \mid s)) \end{equation}\] <p>This is TRPO. The approximation we’re using at the end here is only valid if the state distributions from the new policy is similar to the old policy. However, how small of a change we need to make for this to be valid is in practice very small. (especially if we use \(C\)’s derived expression).</p> <p>Another issue is that \(D_{KL}^\max\) is typically intractable (I’ve only ever calculated this for gaussians. We don’t want to only have gaussian policies). Instead we can use the expected KL-divergence (which is always estimated through sampling). Then we move the KL-divergence term as a constraint to enable larger step sizes:</p> \[\begin{equation} \max_d \mathbb{E}_{s \sim \pi_\theta, a \sim_\theta} \left[ \frac{\pi_\theta(a \mid s)}{\pi_\theta(a \mid s)} A^{\pi_\theta}(s,a) \right] \\ \text{s.t.} \quad \mathbb{E}_s \left[ D_{KL}(\pi_\theta(a \mid s) || \pi_{\theta + d}(a \mid s)) \right] \leq \epsilon \end{equation}\] <p>This looks awfully similar to the NPG problem we had before. Turns out we use the same methodology to solve this problem and get gradient update steps. There is an extra step employed which uses conjugate gradients instead to avoid calculating the matrix inverse of the hessian. What’s the difference? Before, we were using the Fisher information matrix to get the gradient step which doesn’t guarantee performance improvement. Now, we are using the KL-divergence to get the gradient step which “guarantees” performance improvement given small enough steps.</p> <p>With all of these approximations, how do we know that our policy will improve? We will have to use \(\beta\) to make this guaranteed and the constraint is tightened/satisfied. We do the following:</p> \[\beta \gets \begin{cases} 0.5 \beta &amp; \text{if } D_{KL}(\pi_\theta(a\mid s) || \pi_{\theta + d}(a \mid s)) &lt; \epsilon \\ 0.5 \beta &amp; \text{if } \hat J(\theta, d_{\text{new}}) - \hat J(\theta, d_{\text{old}}) &lt; 0 \\ \beta &amp; \text{otherwise} \end{cases}\] <p>At first, \(\beta\) is initialized using NPG’s method.</p> <p><b> Adaptive Lagrangian Method </b> Line search may be expensive, we can solve the unconstrained lagrangian and use adaptive updates on the lagrange multiplier.</p> \[\begin{equation} \max_d \mathbb{E}_{s \sim \pi_\theta, a \sim \pi_\theta} \left[ \frac{\pi_{\theta + d}(a \mid s)}{\pi_\theta (a \mid s)} A^{\pi_\theta}(s,a) - \lambda D_{KL}(\pi_\theta(\cdot \mid s) || \pi_{\theta + d}(\cdot \mid s)) \right] \end{equation}\] <p>these are updates for lagrange multiplier:</p> \[\begin{equation} \lambda \gets \begin{cases} \frac{\lambda}{2} &amp; \delta_{KL} &lt; \delta_{\text{target}} / 1.5 \\ 2\lambda &amp; \delta_{KL} \geq \delta_{\text{target}} \cdot 1.5 \end{cases} \end{equation}\] <p>where \(\delta_{KL} = D_{KL}(\pi_\theta(a\mid s) \| \pi_{\theta + d}(a \mid s))\) and \(\delta_{\text{target}}\) is arbitrary threshold.</p> <p><b> Clipping </b> Removing the KL-divergence constraint, we can propose to clip the objective function instead of enforcing the constraint in policy updates. This is called Proximal Policy Optimization (PPO). To figure this out, we restart back to the original optimization problem (still doing monotonic policy improvement):</p> \[\begin{equation*} \mathbb{E}_{s \sim \pi_{\theta + d}, a \sim \pi_\theta} \left[ \frac{\pi_{\theta + d}(a \mid s)}{\pi_\theta (a \mid s)} A^{\pi_\theta}(s,a) \right] \end{equation*}\] <p>as \(\frac{\pi_{\theta + d}(a \mid s)}{\pi_\theta (a \mid s)}\) weighs the advantage, the idea of PPO is to penalize a new policy \(\pi_{\theta + d}\) that has either a large or small ratio \(c_t = \frac{\pi_{\theta + d}(a \mid s)}{\pi_\theta (a \mid s)}\). This penalization is realized by clipping \(c_t\) within a pre-defined bound:</p> \[\begin{equation} \mathbb{E}_{s \sim \pi_\theta, a \sim \pi_\theta} \left[ \min \{ \frac{\pi_{\theta+d}(a\mid s)}{\pi_\theta (a \mid s)}A^{\pi_\theta}(s,a), \text{clip}(\frac{\pi_{\theta+d}(a\mid s)}{\pi_\theta (a \mid s)}, 1-\epsilon, 1+\epsilon) A^{\pi_\theta}(s,a) \} \right] \end{equation}\] <p>so we choose between the clipped and unclipped objective (whichever one is better). This has way less compute and performs better than TRPO.</p> <h2 id="37-practical-considerations-of-ppo"> <a href="#37-practical-considerations-of-ppo" class="anchor-heading" aria-labelledby="37-practical-considerations-of-ppo"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 3.7 Practical Considerations of PPO </h2> <p>PPO uses a lot of code-level optimzations to make it work. Without these optimizations, PPO cannot outperform TRPO. This is one of the reasons why PPO has such a wide range of performance on the same tasks across open-sourced implementations.</p> <p>Read <a href="https://arxiv.org/pdf/2005.12729">this</a> for more details on the practical considerations of PPO.</p> <h2 id="38-debugging"> <a href="#38-debugging" class="anchor-heading" aria-labelledby="38-debugging"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 3.8 Debugging </h2> <ul> <li>Visualize policy performance at initialization</li> <li>Analyze returns at initialization</li> <li>Increase batch size</li> <li>Periodically visualize agent behavior</li> <li>Entropy of actions</li> </ul> </main> <hr> <footer> <p class="text-small text-grey-dk-100 mb-0">By An Dang.</p> <div class="d-flex mt-2"> <p class="text-small text-grey-dk-000 mb-0"> <a href="https://github.com/dangthanhan507/dangthanhan507.github.io/blob/master/class-notes/04-Computational-Sensorimotor-Learning/policy-grad.md" id="edit-this-page">Edit this page on GitHub.</a> </p> </div> </footer> </div> </div> <div class="search-overlay"></div> </div> <script type="module"> import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.6.0/dist/mermaid.esm.min.mjs'; var config = {} ; mermaid.initialize(config); mermaid.run({ querySelector: '.language-mermaid', }); </script> </body> </html>
