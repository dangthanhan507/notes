---
layout: default
title: 5. Learning Demos
parent: Sensorimotor Learning
nav_order: 5
mathjax: true
tags: 
  - latex
  - math
# math: katex
---

# 5. Learning Demos

This method is fairly straightforward, we provide expert demonstrations of the task and supervise learn the policy. Goal is to learn a state-conditioned policy: $$\pi : \mathcal{S} \mapsto \mathcal{A}$$. We are provided with trajectories/demonstrations $$\{ \tau_{1:T}^i \}_{i=1}^N $$.

<b> Assumption: </b> We assume these policies are optimal or near-optimal. 

- **Training time**: Stage of training policy $$\pi_\theta$$ using demos.
- **Testing time**: We fix parameters and execute predicted actions.
- **Expert Policy** $$\pi_d$$: Expert policy that achieves optimal performance on given task.
- **Demonstration dataset** $$\mathcal{D} = \{ \tau^i_{1:T} \}_{i=1}^N$$: constructed from rolling trajectories from the expert policy $$\pi_d$$. 

Typically the way we train in simulation is to run a few epochs, then test the policy.

## 5.1 Behavior Cloning

One way to learn policy is to match distributions of trajectories generated by $$\pi_\theta$$ and $$\pi_d$$. This is known as <b>Behavior Cloning </b> and can be formulated by minimizing KL-divergence between the two distributions.

$$
\begin{equation}
\min_\theta L(\theta) = D_{KL}(P_D || P_\theta)
\end{equation}
$$

where $$P_D$$ denote distributions of trajectories of $$\pi_d$$ and $$\pi_D$$, respectively. To see connection of gradient $$L(\theta)$$ and $$\pi_\theta$$, we define probability of trajectory as $$P(\tau)$$:

$$
\begin{align*}
P(\tau) &= P(s_1, a_1, ..., s_T) \\
&= P(s_1) P(a_1 \mid s_1) ... P(a_{T-1} \mid s_1, a_1, ..., s_{T-1}) P(s_T \mid s_1, a_1, ..., a_{T-1}) \\
&= P(s_1) \prod_{t=1}^T P(s_t \mid s_1, a_1, ..., a_{t-1})P(a_t \mid s_1, a_1, ..., s_t) \\
&= P(s_1) \prod_{t=1}^T P(s_t \mid s_{t-1}, a_{t-1}) P(a_t \mid s_t) \quad \text{(Assume Markov)} \\
&= \rho(s_1) \prod_{t=1}^T \mathcal{T}(s_t \mid s_{t-1}, a_{t-1})\pi(a_t \mid s_t)
\end{align*}
$$

How to calculate gradient of $$L(\theta)$$?:

$$
\begin{align*}
\nabla_\theta L(\theta) &= \nabla_\theta D_{KL}(P_D || P_\theta) \\
&= \nabla_\theta (\sum_\tau P_D(s_1, a_1, ..., s_T) \log \frac{P_D(s_1,a_1,...,s_T)}{P_\theta(s_1,a_1,...,s_T)}) \\
&= \nabla_\theta (\sum_\tau P_D(s_1, a_1, ..., s_T) (\log P_D(s_1, a_1, ..., s_T) - \log P_\theta(s_1, a_1, ..., s_T))) \\
&= -\mathbb{E}_{\tau \sim D} \left[ \nabla_\theta \log P_\theta(s_1,a_1,...,s_T) \right] \\
&= -\mathbb{E}_{\tau \sim D} \left[ \sum_{t=1}^T \nabla_\theta \log \pi_\theta (a_t \mid s_t) \right] \\
&= -\mathbb{E}_{s,a \sim D} \left[ \nabla_\theta \log \pi_\theta(a \mid s) \right]
\end{align*}
$$

which makes sense to what we're doing.

Thus, minimzing $$L(\theta)$$ is Maximum Likelihood Estimation (MLE) of the policy $$\pi_\theta$$:

$$
\begin{equation}
\theta^* = \arg \max_\theta \mathbb{E}_{s,a \sim \mathcal{D}} \left[ \log \pi_\theta (\hat a = a \mid s) \right]
\end{equation}
$$

One big difference between this and other supervised learning algorithms is the "drift" issue. If $$\pi_\theta$$ makes a slightly incorrect step away from $$\pi_d$$ it'll be slightly different which may or may not be in distribution, and this kind of error will just compound itself as time goes on. The timeseries aspect of this supervised learning perspective gives rise to the <b> co-variate shift </b>. 

One way to address co-variate shift is <b> DAgger (Dataset Aggregation) </b>. This performs the training process iteratively where DAgger will take data collected in eval time and re-run with the expert policy to get more data. This will reduce the covariate-shift as we are seeing the small errors and addressing them in the form of new data from the expert policy. 

There are still other issues with BC:
- **Tedious Experts**: DAgger requires access to expert to create new data.
- **Multimodality**: Distribution of expert actions might have multiple "peaks". We have to address which peak to choose for our policy.
- **Causal Confusion**: Agent might learn "spurious" features. We might learn weird features and correlate them to a certain action instead of a feature that we want it to correlate to a certain action. For example, a car might brake because of the brake indicator in the car instead of visually seeing pedestrians on the road. That's because brake indicator goes on every time we brake really hard. 
- **Copy cat problem**: BC might predict next action by previous actions instead of reacting to its sensing modalities.
- **Suboptimal Demonstrations**: Our demonstrations may not be from a task expert which may train a policy to be sub-optimal at best.